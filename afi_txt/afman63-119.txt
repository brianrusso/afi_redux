 BY ORDER OF THE  SECRETARY OF THE AIR FORCE AIR FORCE MANUAL 63-119 19 FEBRUARY 2016 Acquisition CERTIFICATION OF SYSTEM READINESS FOR DEDICATED OPERATIONAL TESTING  COMPLIANCE WITH THIS INSTRUCTION IS MANDATORY ACCESSIBILITY:  Publications and forms are available on the e-publishing website at www.e-Publishing.af.mil for downloading or ordering. RELEASABILITY:  There are no releasability restrictions on this publication.  OPR:   HQ USAF/TEP  Supersedes:   AFMAN 63-119, 20 June 2008 Certified by: SAF/AQX  (Mr. John M. Miller) Pages: 91   This  Air  Force  Manual  (AFMAN)  supports  implementation  of  Air  Force  Policy  Directive (AFPD)  63-1,  Integrated  Life  Cycle  Management,  and  Air  Force  Instruction  (AFI)  63-101/20-101, Integrated Life Cycle Management.  This AFMAN applies to Operational Test and directs a process  for  certification  of  system  readiness  for  dedicated  operational  test  and  evaluation (OT&E)  as  required  by  Department  of  Defense  Instruction  (DoDI)  5000.02,  Operation  of  the Defense  Acquisition  System.    This  AFMAN  must  be  used  in  conjunction  with  AFI  99-103, Capabilities-Based  Test  and  Evaluation,  which  requires  certification  for  additional  types  of operational  testing  beyond  those  required  by  DoDI  5000.02.    AFI  63-101/20-101  also  requires use  of  this  AFMAN  and  certification  process.    This  AFMAN  applies  to  all  Air  Force organizations involved in system acquisition or modification, including Air National Guard and US  Air  Force  Reserve  Command  units  and  members.    Mandates  to  the  acquisition  executive chain are not considered Wing-level mandates; therefore, waiver “tiering” (as defined in AFI 33-360,  Publications  and  Forms  Management)  does  not  apply  IAW  the  acquisition  chain  of authority  specified  in  this  AFMAN  and  AFI  63-101/20-101.    Refer  recommended  changes  and questions  about  this publication to  the Office of  Primary  Responsibility (OPR)  using AF  Form 847,  Recommendation  for  Change  of  Publication,  routed  through  the  functional  chain  of command.    This  AFMAN  may  be  supplemented  IAW  AFI  33-360.    Any  organization supplementing  this  manual  must  send  the  proposed  document  to  SAF/AQXS  (mail  to:  usaf.pentagon.saf-aq.mbx.saf-aqxa-workflow@mail.mil) to:  usaf.pentagon.af-te.mbx.af-tep-workflow@mail.mil)  for  review  prior  to  publication.    Ensure all  records  created  as  a  result  of  processes  prescribed  in  this  publication  are  maintained  IAW AF/TEP and (mail   2  AFMAN63-119  19 FEBRUARY 2016 AFMAN 33-363, Management of Records, and disposed of IAW Air Force Records Disposition Schedule (RDS) in the Air Force Records Information Management System (AFRIMS). SUMMARY OF CHANGES This  document  was  substantially  revised  and  must  be  completely  reviewed.    It  reflects changes  in  the  DoD  5000-series  documents,  Chairman  of  the  Joint  Chiefs  of  Staff  instructions (CJCSI),  CJCS  manuals  (CJCSM),  and  numerous  AFIs  and  AFPDs.    Recently  published statutory  and  regulatory  requirements  were  added  as  well  as  policy  and  direction  for  cyber resiliency,  integrated  testing,  information  systems  technology,  and  vulnerability  reporting  for software.    A  flowchart  (Fig  2.4)  was  added  to  clarify  the  coordination  and  flow  of  the certification  memo.    Many  templates  were  renamed,  combined,  or  eliminated  to  make  the certification process more current.  All terminology, references, and definitions were updated.   Chapter 1—THE CERTIFICATION PROCESS – APPLICABILITY AND RESPONSIBILITIES    1.1. Overview   ................................................................................................................  1.2. Applicability   ..........................................................................................................  1.3. Responsibilities   ......................................................................................................  1.4. Links to Reference Documents   ..............................................................................  Chapter 2—THE CERTIFICATION PROCESS    2.1. Overview   ................................................................................................................  2.1.1.  The certification process involves a procession of systematic reviews of the applicable certification templates found in Figure 2.   .............................................  2.2. Template Subject Matter   ........................................................................................  Figure  2.1.  Matrix of Certification Templates.   ........................................................................  2.3. Team Effort   ............................................................................................................  2.4. Tailoring the Process   .............................................................................................  2.5. Continuous Process   ................................................................................................  Figure  2.2. Notional Timing of Certification Process Reviews.   ..............................................  Figure  2.3. T&E of Cybersecurity Mapped to the Acquisition Life Cycle.   .............................  2.6. The Certification Review Cycle   .............................................................................  Figure  2.4. Notional Certification Process Flowchart.   .............................................................  2.7. Certification Memo Purpose   ..................................................................................  2.8. Updating the Templates   .........................................................................................  Chapter 3—TEMPLATE STRUCTURE AND USE    3.1. Interlocking Matrix   ................................................................................................  5 5 5 6 9 10 10 10 10 11 11 11 12 13 14 15 15 17 19 20 20 AFMAN63-119  19 FEBRUARY 2016   3  3.2. Consolidation of Multiple Sources.   .......................................................................  3.3. Answering Template Line Items.   ...........................................................................  3.4. Focus on Ends, Not Means.   ...................................................................................  3.5. Assigning Responsibilities.   ....................................................................................  3.6. Certification Template Tracking Tool.   ..................................................................  3.7. Information Collection and Records   ......................................................................  Attachment 1—GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION   Attachment 2—ACQUISITION STRATEGY AND SCHEDULE   Attachment 3—ANALYSIS OF ALTERNATIVES (AOA)   Attachment 4—CAPABILITIES-BASED REQUIREMENTS DOCUMENTS (CBRD)   Attachment 5—THREAT & INTELLIGENCE DOCUMENTS   Attachment 6—INTEGRATED TEST TEAM (ITT) STANDUP AND ITT CHARTER   Attachment 7—AIR FORCE CONCEPTS   Attachment 8—LIFE CYCLE SUSTAINMENT PLAN (LCSP)   Attachment 9—INFORMATION TECHNOLOGY (IT) AND NATIONAL SECURITY SYSTEMS (NSS)   Attachment 10—TEST & EVALUATION MASTER PLAN (TEMP)   Attachment 11—INTEGRATED TEST PLANNING   Attachment 12—CYBER RESILIENCY   Attachment 13—CONTRACTOR TESTING   Attachment 14—GOVERNMENT DEVELOPMENTAL TEST AND EVALUATION (DT&E)   Attachment 15—SOFTWARE DEVELOPMENT AND MATURITY   Attachment 16—LIVE FIRE TEST AND EVALUATION (LFT&E)   Attachment 17—MODELING AND SIMULATION (M&S)   Attachment 18—CONFIGURATION MANAGEMENT   Attachment 19—DEFICIENCY IDENTIFICATION AND RESOLUTION PROCESSES   Attachment 20—PRODUCTION REPRESENTATIVE TEST ARTICLES   Attachment 21—SYSTEM PERFORMANCE   Attachment 22—OPERATIONAL TEST AND EVALUATION PLAN                         20 20 20 20 21 21 22 48 49 50 52 53 54 55 56 58 60 62 65 67 69 71 73 74 75 77 78 79   4  AFMAN63-119  19 FEBRUARY 2016 Attachment 23—INTEGRATED TECHNICAL, HEALTH, AND SAFETY REVIEWS   Attachment 24—OPERATIONAL TEST TEAM TRAINING   Attachment 25—SUPPORT EQUIPMENT (SE)   Attachment 26—SUFFICIENCY OF SPARES   Attachment 27—SUPPORT AGREEMENTS   Attachment 28—PACKAGING, HANDLING, STORAGE AND TRANSPORTATION   Attachment 29—PERSONNEL   Attachment 30—CONTRACTOR SUPPORT   Attachment 31—TECHNICAL DATA   Attachment 32—TEST AND EVALUATION RESOURCES              81 83 84 85 86 87 88 89 90 91 AFMAN63-119  19 FEBRUARY 2016   5  THE CERTIFICATION PROCESS – APPLICABILITY AND RESPONSIBILITIES Chapter 1 1.1.  Overview 1.1.1.  This  AFMAN  provides  a  process  for  structured  risk  evaluation  (cost,  schedule, performance,  and  safety)  associated  with  transitioning  from  developmental  test  and evaluation (DT&E) to dedicated operational test (OT).  Coupled with other AFIs and JCIDS documentation, the process will ensure data captured and lessons learned are considered and documented ultimately supporting a partial or Full Deployment Decision (FDD) and/or Full Rate  Production  (FRP)  decision.    It  establishes  a  disciplined  review  and  “certification process”  in  the  early  stages  of  acquisition  and  modification  programs,  and  culminates  in more  successful  operational  test  outcomes.    The  certification  process  is  a  tool  to  help acquisition managers at all levels identify risks, reach negotiated agreements on issues, and render more accurate assessments of system readiness to begin dedicated OT.  The process is supported by 31 templates (i.e., checklists) in Attachments 2 through 32 based on DoD and Air Force policy, historical information, best practices, practical advice, and lessons learned from numerous acquisition  programs.   The certification  process helps document the pursuit of  a  credible  risk  reduction  program  and  an  effective  development  program.    This certification  process  is  mandatory  and  is  implemented  as  a  continuous  effort,  not  a  single event  in  time.    Note:    While  use  of  the  certification  process  is  mandatory,  the  contents  of each  template  are  not  mandatory  and  are  subordinate  to  DoD  and/or  AF  direction;  i.e. Directives,  Instructions,  and  Manuals.    The  templates  should  be  used  in  parallel  with,  not substitutes for, formal DoD or Air Force policy and guidance. 1.2.  Applicability 1.2.1.  DoDI  5000.02  directs  the  Services  to  establish  a  process  for  evaluating  and determining materiel system readiness for Initial Operational Test and Evaluation (IOT&E).  This process is used for all acquisition category (ACAT) programs, including programs in the operations  and  maintenance  (O&M)  phase  (e.g.,  modifications  and  sustainment)  where  OT will support a deployment or FRP decision.  This AFMAN supports fielding and production decisions for programs with dedicated OT portions of an integrated test program. 1.2.2.  DoDI  5000.02  requires  the  Service  Operational  Test  Readiness  Review  (OTRR) process  be  executed  for  programs  on  Director,  Operational  Test  and  Evaluation  (DOT&E) oversight  prior  to  any  OT.    AFI  99-103  requires  Air  Force  program  managers  (PM)  for acquisition  and  sustainment  programs  requiring  a  deployment  or  FRP  decision  to  use  this certification  process  to  evaluate  system  readiness  for  operational  testing  in  support  of deployment and/or FRP decisions. Prior to IOT&E, the process includes a review of all test results (contractor, DT&E, and any early OT); an assessment of the system’s progress against the  key  performance  parameters  (KPP),  key  system  attributes  (KSA),  and  critical  technical parameters (CTP) documented in the test and evaluation master plan (TEMP).  Additionally, the  process  will  include  an  analysis  of  identified  technical  risks  to  verify  those  risks  were managed  through  DT&E;  a  review  of  system  certifications,  and  review  of  the  OT&E entrance criteria specified in the TEMP or other OT plans.   6  AFMAN63-119  19 FEBRUARY 2016 1.2.3.  Mandates to the acquisition executive chain are not considered wing-level mandates; therefore,  waiver  “tiering”  (as  defined  in  AFI  33-360)  does  not  apply  IAW  the  acquisition chain  of  authority  specified  in  this  AFMAN  and  AFI  63-101/20-101.    Ensure  any  non-acquisition execution chain activities (e.g. those conducted per Air Force 10-, 13-, 14-, 16-, 21-,  33-,  91-,  99-,  and other  series)  comply  with  the  tier  waiver  authority  in  the  referenced publication. 1.2.4.  Use this AFMAN for commercial-off-the-shelf (COTS) and non-developmental items or  any  program  where  dedicated  OT&E  is  planned.    Note:    Some  systems,  programs,  and activities  may  be  exempt  from  this  AFMAN  according  to  AFI  99-103,  paragraph  1.6  and 5.12. 1.2.5.  For the purposes of this AFMAN, dedicated operational testing refers to that phase of OT&E  that  is  conducted  independently  of  developers  in  support  of  a  deployment  or  FRP decision.  Program offices using an incremental acquisition strategy will need to repeat this certification  process  for  each  increment  of  capability  developed,  produced  and/or  fielded.  Note:  The direction in this AFMAN is based on parameters and descriptions in AFI 99-103, Chapter  2,  paragraph  2.5  et  seq.  which  explain  the  differences  between  various  types  of OT&E.  These paragraphs establish the basis for determining when readiness certification is required. 1.2.6.  This  AFMAN  is  the  primary  source  for  the  OT&E  certification  process  for  all programs  when  the  Air  Force  is  the  lead  Service.    For  multi-Service  programs,  the certification policies of the lead Service are used.  In these cases, this AFMAN may or may not  be  the  governing  document  as  determined  by  the  Integrated  Test  Team  (ITT)  (or equivalent  body).    Nonetheless,  it  should  be  used  for  Air  Force  portions  of  certification activities.  For programs where the Air Force is not the lead Service, Air Force ITT members may  adapt  this  process  to  flow  into  the  other  Service's  certification  process.    Note:    The certification  process  is  optional  for  programs  electing  to  use  a  Sufficiency  of  Operational Test Review (SOTR) which is described in AFI 99-103, paragraph 2.5.11. 1.2.7.  Use  the  appropriate  certification  templates,  modified  as  necessary,  prior  to deployment  of  prototypes,  Joint  Capability  Technology  Demonstrations  (JCTD),  and solutions  in  response  to  Urgent  Operational  Needs  (UON)  and  Joint  Emergent  Operational Needs (JEON) to review the system’s capabilities and limitations and its readiness for initial deployment.    Going  through  the  entire  certification  process  may  not  be  necessary  in  this situation  but  may  provide  insight  on  the  sustainability  of  rapidly  acquired  capabilities.    A Capabilities  and  Limitations  (C&L)  Report  should  be  provided  to  the  developer  and  users.  See AFI 99-103, paragraphs 4.15 and 7.5 for more information about C&L Reports. 1.2.8.  Should  the  systems  described  in  paragraph  1.2.7  become  programs  of  record  after initial  fielding,  then  the  full  intent  of  this  AFMAN  applies  to  subsequent  development  as described in paragraphs 1.2.5 and 1.2.6. 1.3.  Responsibilities 1.3.1.  The certification process cuts across organizational lines and brings together stakeholders from the acquisition, requirements, developer, information technology management, T&E, operations, and sustainment communities.  Other stakeholder organizations are responsible for providing test data, supporting information, studies, AFMAN63-119  19 FEBRUARY 2016   7  analyses, and candid feedback for assigned areas in support of the certification process.  Each line item in the templates suggests a single “most likely” OPR for that item.  Additional offices or organizations may also be involved, but in most cases a single OPR is cited.  The ITT has full authority to add or change OPRs as necessary.  The following organizations or officials (or their representatives) are required to participate in the certification process. (T-1) 1.3.2.  OT&E Certification Official.  The OT& E Certification Official, with advice from the ITT,  and  as  designated  in  the  TEMP,  will  determine  the  broad  scope  and  requirements  for certifying system readiness to begin the dedicated phase of operational testing.  Note:  Under no circumstance shall a PM be the OT&E Certification Official for his/her own program (ref. paragraph 6.5.1 of AFI 99-103). 1.3.3.  Program Managers (PM).  PMs will: 1.3.3.1.  Designate a point of contact (POC) within the Program Office for organizing the certification process as early as possible; preferably before the first ITT meeting after the start of Technology Maturation and Risk Reduction (TMRR) Phase.  This POC may be the Chief Developmental Tester (CDT) or Test Manager.  This POC will brief the template process at the first ITT meeting to ensure all program officials and contractors understand the templates’ purpose and required activities.  The POC will be responsible for gathering information, scheduling reviews, assigning tasks, negotiating consensus on issues and solutions, assembling certification briefings, and drafting the final certification memo according to this AFMAN. 1.3.3.2.  Ensure a robust systems engineering process serves as the underlying foundation for  systems  development  and  for  reviewing  and  tailoring  these  templates,  thus  ensuring end-to-end  functionality,  performance,  and  operability  during  DT&E  and  prior  to certification of readiness for OT. 1.3.3.3.  Provide  a  Safety  Release  and  technical  data  describing  known  hazards  so  that DT and Operational Test Organizations (OTO) may assess test unique risks.  Supporting DT units should forward known hazard data (e.g. Safety Release, previous Test Hazard Analysis  [THA])  to  the  OTO  for  their  independent  hazard  analysis.    See  AFI  91-202, Attachment 16 for Safety Release content. 1.3.3.4.  Ensure  the  system  is  mature  and  demonstrates  stabilized  performance  in  an operationally  relevant  environment  and  operationally  realistic  manner  prior to certification.    Additionally,  verify  that  all  necessary  test  support  is  available  and  the system  has  a  high  likelihood  of  a  successful  OT.    For  details  about  system  maturity levels, see DoD Technology Readiness Assessment (TRA) Guidance, April 2011. 1.3.3.5.  Document the strategy for the certification process in Part III of the TEMP. 1.3.3.6.  Request  additional  stakeholder  organizations  to  participate  in  this  process  as necessary to ensure acquisition program success. 1.3.4.  Air  Force  Operational  Test  and  Evaluation  Center  (AFOTEC).    If  AFOTEC  is  the operational  test  agency  (OTA)  for  the  program,  they  will  participate  in  the  certification process by assisting the PM and carrying out responsibilities as agreed.  They will lead the effort to mobilize resources required for dedicated OT&E; and provide advice, test support,   8  AFMAN63-119  19 FEBRUARY 2016 and  test  data  to  the  PM  and  user  throughout  the  development  process.    (T-1)  Note:    For multi-Service programs, the certification policies of the lead Service are used. 1.3.5.  Major  Command  (MAJCOM)  Operational  Test  Organizations  (OTO).    If  the MAJCOM  is  responsible  for  conducting  OT,  they  will  perform  the  same  certification functions  as  AFOTEC  would  have  performed.    MAJCOM  OTOs  will  assist  the  PM  in implementing  this  certification  process  for  force  development  evaluations  (FDE)  or operational  utility  evaluations  (OUE)  when  deployment  and/or  FRP  decisions  are  planned.  Note:    The  acronym  OTO  is  used  in  the  templates  to  denote  either  the  AFOTEC  OTA  or MAJCOM OTO, whichever applies.  See AFI 99-103, Chapter 4, for details. 1.3.6.  Lead  Operating  Command.    The  lead  operating  command,  or  using  commands  as appropriate, will participate in the certification process by assisting the PM and operational testers (i.e., AFOTEC or MAJCOM OTO) and carrying out responsibilities as agreed.  The lead  operating  command  provides  technical  and  subject  matter  expertise  to  support  the readiness  assessment  process,  test  resources,  and  will  ensure  operational  capability requirements documents are complete and up to date according to AFI 10-601, Operational Capability Requirements Development. 1.3.7.  Lead  Developmental  Test  and  Evaluation  Organization  (LDTO).    The  LDTO  will plan,  conduct,  and  report  DT&E  activities;  and  support  operational/integrated  testing  of systems according to AFI 99-103, AFI 63-101/20-101, and MAJCOM policies.  The LDTO will  participate  in  the  certification  process  by  providing  sufficient  analysis,  results  and supporting  data,  operator  comments,  and  recommendations  to  all  participating  PMs  to support the PM’s responsibilities in paragraph 1.3.2. (T-1) 1.3.8.  HQ USAF Staff.  Representatives from SAF/AQ, SAF/A6, HQ USAF/TE, and others as delegated from the above organizations will monitor the certification process for continued effectiveness and periodically update these templates as policy changes dictate.  These staff members should attend certification proceedings when HQ USAF assistance is required. 1.3.9.  OSD  Staff.    Staff  members  from  the  Deputy  Assistant  Secretary  of  Defense  for Developmental Test and Evaluation (DASD(DT&E)) and from the DOT&E may monitor the Air  Force  certification  process  if  the  program  is  on  OSD  T&E  Oversight.    DASD(DT&E) staff  may  also  conduct  Test  Readiness  Reviews  (TRR)  on  selected  programs  at  their discretion. 1.3.10.  Integrated  Test  Team  (ITT).    The  ITT  will  support  the  certification  process  by assigning key members to attend certification process reviews. (T-1)  The CDT will chair and the lead OTO will co-chair this team and be responsible for assigning team member roles.  At its  discretion,  the  ITT  may  direct  a  sub-group  (e.g.,  an  OTRR  Group)  to  carry  out certification responsibilities.  Assigned ITT members should keep the ITT informed of issues and program status. 1.3.11.  Other  Stakeholder  Organizations.   The  ITT  should  invite  participating  test organizations (PTO) and other advisors to support the certification process.  For example, the ITT  should  invite  the  Joint  Interoperability  Test  Command  (JITC)  staff  to  participate  for systems with net-ready key performance parameters (NR-KPP) (i.e., interoperability). AFMAN63-119  19 FEBRUARY 2016   9  1.4.  Links to Reference Documents 1.4.1.  The most current versions of documents referenced in this AFMAN are available electronically.  For Air Force publications, check http://www.e-publishing.af.mil/.  For DoD publications, check http://www.dtic.mil/whs/directives/.  Other useful links:  CJCSI & CJCSM: http://www.dtic.mil/cjcs_directives/, JCIDS Manual: https://www.intelink.gov, Defense Acquisition Guidebook: https://acc.dau.mil/dag, Tech Orders:  http://www.tinker.af.mil/technicalorders/index.asp, Military Standards (MIL-STD): http://quicksearch.dla.mil/.  Note: most, if not all, of these sites require a CAC.   10  AFMAN63-119  19 FEBRUARY 2016 Chapter 2 THE CERTIFICATION PROCESS 2.1.  Overview 2.1.1.  The certification process involves a procession of systematic reviews of the applicable certification templates found in Figure 2  1.  The PM, users and operational testers will coordinate regularly to address OT&E readiness and shortfalls and will brief the OT&E Certification Official (Service Acquisition Executive (SAE), Milestone Decision Authority (MDA) or delegated responsible Program Executive Officer (PEO)) who is responsible for assessing program readiness for OT&E.  The certification process combines risk assessment and management techniques with a system for assigning responsibility and tracking accountability for results.  Proper risk management requires the development of a systematic, disciplined plan to identify problems and risks.  A proven risk management technique is to examine the successes, failures, problems, mitigation, and solutions of similar or past programs for "lessons learned" that can be applied to current programs.  Another technique is to systematically comb through the entire program using specific decision criteria based on historical data. 2.2.  Template Subject Matter 2.2.1.  Figure 2.1, Matrix of Certification Templates, covers a broad range of subjects that have historically impacted systems transitioning from DT&E and Integrated Testing to dedicated OT.  Not all templates apply equally to every program; however, all templates should be considered for applicability at each review to ensure every relevant area at that point in time is covered.  The initial template review should reveal where to begin working on long lead items that usually come to fruition much later in development programs.  The templates are arranged in three notional groups in approximate chronological order:  Test Planning and Documentation, System Design and Performance, and Test Assets and Support. The details for each subject are addressed in corresponding attachments to this manual.  All templates are designed to increase the visibility of potential risk factors and facilitate a streamlined, executive-level review. AFMAN63-119  19 FEBRUARY 2016   11  Figure 2.1.  Matrix of Certification Templates.  2.3.  Team Effort 2.3.1.  The  risk  reduction  process  is  a  team  effort.    Risk  is  managed  only  when  conditions that contribute to risk are adequately addressed.  These risk management efforts are typically within the scope, reach, and authority of certification process participants to effect necessary changes.    These  changes  should  typically  occur  at  levels  not  normally  visible  to  senior decision makers on a day-to-day basis. 2.4.  Tailoring the Process 2.4.1.  As  early  as  practical,  PMs  and  OT&E  Certification  Authorities  should  tailor  the certification  process  to  their  need  for  information,  program  size,  and  complexity.    The Certification  Review  Cycle,  described  in  paragraph  2.6,  should  be  repeated  as  often  as necessary. 2.4.2.  Templates Not Program Specific 2.4.2.1.  Since  the  templates  are  not  program  specific,  PMs  and  OT&E  Certification Authorities may tailor them, with operational tester and operational command assistance, to  fit specific programs or  groups of programs.  Some templates  may require greater  or Test Planning & DocumentationSystem Design & PerformanceTest Assets & SupportAnalysis ofAlternativesSufficiencyofSparesSupportAgreementsLFT&EOperational Test TeamTrainingSupportEquipment (SE)PersonnelDT&EContractorTestingConfigurationManagementSystem PerformanceModeling& SimulationDeficiency ID& ResolutionProcessesTechnicalDataPackaging,Handling, Storage &TransportationSoftwareDevelopment &AssuranceOT&E PlanProductionRepresentativeTest ArticlesLife Cycle Sustainment PlanITT Standup &ITT CharterTEMPCyber ResiliencyInformation Technology & NS SystemsIntegrated Test PlanningThreat &Intelligence DocumentsCapabilities BasedRequirementsAir Force ConceptsAcquisitionStrategy &ScheduleIntegrated Tech, Health and SafetyNote:All acronyms in this figure are listed in Attachment 1.Note:  Templates are listed in approximate chronological order. ContractorSupport2109876543271119181716151413122625242322212031302928T&EResources32  12  AFMAN63-119  19 FEBRUARY 2016 lesser emphasis depending on the program and its phase of development.  The templates provide  PMs  maximum  flexibility  in  focusing  and  structuring  their  reviews  without losing  sight  of  the  original  objective—providing  an  executive-level  review  of  the program.    Tailoring  and/or  negating  templates  should  only  be  exercised  with  serious consideration and should be coordinated with all stakeholders. 2.4.3.  Tailoring Level of Detail.  PMs may attach additional information or levels of detail to the templates at their discretion.  Some examples might be exit and pass-fail criteria, action plans, requirements thresholds, lists of acquisition regulations and standards, watch lists, breakdowns of specific line items, and points of contact.  Additional templates can be developed to cover unique areas.  Additionally, aggregation of templates and template line items can reduce redundancy and help managers concentrate on known risk areas.  In short, tailor each certification program to attain the best results. 2.4.4.  Tailoring for Integrated Testing.  At program inception, the ITT must build the certification process into each program’s overarching strategy for T&E. (T-1)  Due to high levels of interdependence between types of T&E described in AFI 99-103, the scope, credibility, and success of dedicated OT&E partially depends on data provided by DT&E and other tests.  Certification reviews therefore examine the types of DT&E data that are planned, and if that data can be used in support of OT&E results. 2.4.5.  Reporting.  The resulting certification briefings and reports should be tailored to match the modified process. 2.5.  Continuous Process 2.5.1.  The certification of system readiness process is viewed as a continuous effort, not a single event in time.  It is not tied to any particular acquisition milestone or decision review.  However, the final certification briefing must be completed 45 days prior to the planned start of dedicated OT, or as mutually agreed between the PM and operational testers.  Any dedicated OT that supports a deployment or FRP decision must be supported by a readiness certification.  Use the following guidelines: 2.5.2.  Starting Early.  Templates may be reviewed in any order that makes sense for the program and phase of development.  All templates are initially reviewed and considered for applicability.  Those that are, in the PM’s judgment, clearly not relevant to the program may be set aside.  Templates previously set aside could become relevant again later if program requirements change (e.g., when a capabilities-based requirements document (CBRD) is re-issued after an insertion of new technology).  The PM begins the certification process as early as practical in new development programs. 2.5.3.  Series of Reviews.  The certification process is a series of reviews culminating in a final readiness briefing as shown in the notional diagram in Figure 2.2, Notional Timing of Certification Process Reviews.  For example, certification reviews (and briefings, if required) should be planned as progress checks prior to early operational assessments (EOA) or before each operational assessment (OA).  This series of reviews is designed to aid in resolving problems or correcting deficiencies as soon as they are discovered, rather than waiting until the final certification review and briefing where late remedial action could cause delays in the start of dedicated OT. AFMAN63-119  19 FEBRUARY 2016   13  Figure 2.2.  Notional Timing of Certification Process Reviews. Note:  All acronyms in this figure are defined in Attachment 1  2.5.4.  Cyber Resiliency.  Cyber test must be considered at program initiation and integrated throughout the acquisition lifecycle.   Intelligence support is critical to developing requirements, integrated Concept of Operations (CONOPS), and cyber test measures.  Cyberspace requirements must be incorporated as part of the Systems Engineering (SE) process.  All aspects of cyber resiliency test including required resources, manpower, and infrastructure must be planned for and documented in the TEMP and Capabilities Based Requirements Documents (CBRD).  Cyber test must be fully coordinated and integrated into all DT and OT events including cooperative vulnerability and adversarial penetration test in an operationally representative cyberspace environment.  Cyber test will highlight System Under Test (SUT) vulnerabilities and risk to mission accomplishment and spur efforts to mitigate that risk. Cyberspace issues may continuously evolve due to changes in system architecture, changing environments, and emerging threats.  Figure 2.3 maps cyber T&E to the Acquisition Life Cycle and reveals potential key time periods for conducting reviews of cyber resilience.  Figure 2.3 highlights earlier more focused, and more frequent reviews apart from the usual Certification of Readiness Reviews depicted in Figure 2.2.  Cyber test may need to be repeated during each of the different life cycle phases.  Early and continuous identification and resolution of cyber vulnerabilities is one of the most difficult areas for system developers, and may require more frequent review of Information Technology (IT) and National Security Systems (NSS), Cyber Resilience, and Deficiency Identification and Resolution Processes Templates (Attachments 9, 12, and 19 respectively).   14  AFMAN63-119  19 FEBRUARY 2016 Figure 2.3.  T&E of Cybersecurity Mapped to the Acquisition Life Cycle. Note:  All acronyms in this figure are defined in Attachment 1.  2.5.5.  Initial Review.  Early on, the PM may concentrate on templates grouped under Test Planning and Documentation as shown in Figure 2.1.  These templates generally address pre-Milestone (MS) B areas of the acquisition process where early fixes to problems generate large future paybacks.  The System Design and Performance group of templates focuses on activities that require completion or are nearly complete prior to MS C.  The Test Assets and Support group of templates helps ensure all required assets come together before dedicated OT begins.  All line items in each template are arranged chronologically as much as possible. 2.5.6.  ITT Involvement.  Early in the EMD phase, each program’s ITT leadership should consult with their OT&E Certification Official to determine how to structure and tailor the certification process.  The ITT should recommend the best forum and frequency for conducting the reviews.  Note:  The ITT may not be the appropriate group for conducting the certification review itself due to the high-level nature of ITT membership versus the detailed nature of the material.  A suggestion is to form a special OTRR Group consisting of the stakeholders outlined in paragraph 1.3.9 and 1.3.10. 2.5.6.1.  Frequency of Reviews.  The ITT lead should recommend to the OT&E Certifying Official how to tailor the reviews to the needs of the program.  In general, the frequency of reviews should increase as the program approaches the final certification date.  Additional reviews may be needed for any DT&E data planned for support of dedicated OT&E.  Certification reviews should be planned prior to all OT activities such as an OA. 2.5.7.  Final Certification.  As a minimum, a final certification review and briefing should occur no later than 45 calendar days (or as mutually agreed) prior to the start of dedicated OT&E.  This lead time helps ensure sufficient time to fix weak areas before starting dedicated OT&E, and for preparation of the certification of readiness memo.  A signed certification readiness memo is submitted to the lead OTO a minimum of 15 calendar days prior to the scheduled start of dedicated OT&E.  These times ensure the operational testers have a minimum of two weeks to finalize their T&E resources and schedules.  Longer or shorter times may be negotiated by mutual agreement. AFMAN63-119  19 FEBRUARY 2016   15  2.6.  The Certification Review Cycle 2.6.1.  A systematic series of candid review-assessment-negotiation-reporting cycles promotes meaningful dialogue among developers, the operational tester(s), and the operating command(s).  Allow sufficient staffing time for multiple cycles.  The certification review OPR will periodically issue a call for roundtable meetings, create an open forum for discussion, consolidate inputs from all participating organizations and stakeholders, and report results to participants, stakeholders and the OT&E Certification Official.  Figure 2.4 shows the notional steps in the Certification Process and review cycle. Figure 2.4.  Notional Certification Process Flowchart.  2.6.2.  Pre-Certification Reviews.  A series of thorough reviews of all operational capabilities-based requirements and resource needs is the first step in assessing a program's readiness to begin dedicated OT.  Suggested review points are shown at the bottom of Figure 2.2.  Each participant (i.e., subject matter expert) should review assigned areas of responsibility and intensify ongoing efforts to reach unmet goals.  The purpose of multiple early reviews is to keep the PM and the OT&E Certification Official better informed as the program nears final certification.  Thus, key issues and risks that impact OT can be identified earlier, ensuring quality, timely direction and feedback are attained from the PM and OT&E Certification Official. 2.6.2.1.  Subject matter experts should compare demonstrated system performance to required system performance and compare available resources to required resources.  A coherent, complete linkage should extend from system/program requirements down through the planned methods and resources for demonstrating technical and operational performance.  Any flaws, inconsistencies, contradictions, voids, deficiency reports (DR), watch items (WIT), or disconnects are potential issues and areas of risk.  Accurate and complete inputs are needed from all participants. 2.6.2.2.  For more complex programs and systems of systems, a greater number of pre-certification reviews may be needed before the final certification review and briefing. 2.6.2.3.  DASD(DT&E) conducts a TRR for selected programs on the DT&E Engagement List.  The PM will work with the DASD(DT&E) representative on the ITT to synchronize the final AFMAN 63-119 certification review and TRR to avoid duplication of effort.   16  AFMAN63-119  19 FEBRUARY 2016 2.6.3.  Assessment.  The reviewers should assess the shortfalls identified in the templates for impacts on the dedicated OT program.  Candid assessments of the system's readiness (i.e., the risk of not passing dedicated OT) are crucial to the success of the certification process. 2.6.3.1.  Standard for Judging Readiness.  Every template and template line item uses the same ideal standard for assessing system readiness and risk level: Will the system be ready for and successfully complete dedicated operational testing in this area?  This judgment should be based on the most current operational capabilities-based requirements document or on sound professional judgment if an operational requirement is not at issue.  Any available exit criteria should be reviewed against the relevant military standards, specifications, and requirements.  The cumulative total of all judgments about these risks indicates if the complete system is ready for dedicated OT.  This candid assessment is the heart of the certification process. 2.6.3.2.  Develop Exit Criteria.  Certification process participants should know what events occur and what operational elements are to be included to achieve program goals before dedicated OT begins.  Specific and testable performance-based exit criteria should be developed for each identified deficiency or issue discovered during DT&E.  Satisfaction of the exit criteria in terms of demonstrated, stabilized system performance is the best means to ensure readiness for dedicated OT.  If possible, use an end-to-end integrated system test before starting OT to make DT&E more operationally relevant and to serve as a predictor of future operational performance.  Subjective value judgments backed up by sound technical and military judgment also may be necessary.  Areas judged not ready require explanation by the program office and an action plan to reach the exit criteria. 2.6.3.3.  If Standards Are Not Met.  Some template line items may not reach the ideal standard (i.e., are not expected to be ready for dedicated operational testing) after close scrutiny.  For example, technical orders (TO) are often unavailable, produced late, or incomplete at the start of dedicated operational testing.  Limitations to test may remain despite best efforts to rectify shortfalls.  Several unavoidable departures from the ideal standard may occur and require constant, long-term management attention.  Negotiation of exit criteria and action plans is required.  In order to mitigate risk to the program an action or ‘get-well’ plan should be developed and vetted with the stakeholders to ensure deliberate course(s) of action are taken to reconcile any discrepancies that might prevent a smooth transition to dedicated OT&E. 2.6.3.4.  Deferred Requirements.  If an incremental acquisition strategy is used, some operational capability requirements (and therefore the OT of those requirements) may require deferment to a later increment.  These deferments may result from program cost-schedule-performance tradeoffs.  Deferment of requirements (specifically, those with operational impact) must be coordinated and documented between the user and PM and eventually reflected in operational capabilities requirements documents.  An assessment of mission operational impacts must be made against deferred requirements (i.e., any dependencies, interdependencies highlighted).  Deferment of any OT will be summarized in the final certification briefing and memo. 2.6.4.  Negotiation.  High to medium risk areas persisting after repeated reviews are likely to impact the conduct of operational testing, specifically, risks associated with mission critical AFMAN63-119  19 FEBRUARY 2016   17  failures must be addressed.  Certification process participants must negotiate workaround plans and solutions, or agree to some limitations on dedicated OT.  The program office is the focal point for attaining negotiated consensus on managing risks.  Workarounds and solutions need to be in the best interests of the Air Force.  OT organization officials should be satisfied that the strength, objectivity, and independence of OT are not compromised, while the program office retains sufficient management flexibility to find optimal solutions.  Again, sound military and technical judgment is necessary to reach a corporate Air Force decision on when to proceed into dedicated operational testing. 2.6.5.  Reporting.  The PM is responsible for consolidating participants' inputs and observations and preparing the certification briefing and/or report to the OT&E Certification Official.  Explicit action plans and exit criteria should be developed for each deficient area to bring risks to acceptable levels. 2.6.5.1.  DT Report.  The LDTO is responsible for providing an unbiased review of the DT results in the form of a briefing/report to the OT&E Certification Official. This review may occur as part of the final certification briefing or be an independent event/product.  It should include an independent risk assessment of the program’s ability to complete OT. 2.6.5.2.  Final Certification Briefing.  The length and format of the certification briefing are discretionary and should be tailored to fit the acquisition or modification program.  The order of the templates should not be changed.  The final product should be an executive-level review of the entire program conveying enough information for senior decision-makers to make informed judgments of system readiness.  The review should broaden senior leadership's perspective to the macro level where overall program risk is assessed along with supporting details, if required. 2.6.5.3.  Reporting to the OT&E Certification Official.  After reviewing the briefing or report, the PM will forward it to the OT&E Certification Official responsible for final certification of system readiness.  The PM should brief the OT&E Certification Official not later than 45 calendar days prior (or as mutually agreed) to the planned start of dedicated OT.  Representatives from appropriate levels of the operating command(s), OTOs, LDTO, and other participating organizations are required to attend the briefing. 2.6.5.4.  Certifications for Incremental or Limited Deployment Acquisition Programs.  Systems may be developed using an incremental strategy and fielded in releases of increasing capability.  These systems require a final certification of readiness for each release, followed by dedicated operational testing for that release or limited deployment IAW DoDI 5000.02.  The final certification for follow-on releases is briefed to the OT&E Certification Official and a certification memo submitted per certification process described in the paragraphs above. 2.7.  Certification Memo Purpose 2.7.1.  The certification of readiness memo documents the level of agreement among certification process participants and specifies the extent of system readiness for dedicated OT within stated constraints.  It confirms the certification process was properly followed and that OT&E entrance criteria were attained.  The signed certification memo is submitted a minimum of 15 calendar days (or as mutually agreed) before the scheduled start of OT&E.     18  AFMAN63-119  19 FEBRUARY 2016 It serves as a quantifiable benchmark of projected capabilities against which to check OT results. 2.7.2.  Contents.  The PM must organize the certification memo to parallel the program's tailored certification process and discuss any agreed-upon deferments or limitations to OT.  The PM should not simply enumerate what was ready for dedicated OT and what was not ready, but summarize the critical areas and processes accomplished.  As a minimum, the PM must address the following areas: 2.7.2.1.  Briefly describe the dedicated OT, OT&E entrance criteria, and which acquisition decision and increment the memo supports.  Include anticipated operational test start and end dates. 2.7.2.2.  Briefly describe how the certification program was structured and executed. 2.7.2.3.  List the templates (or line items, if necessary) that are not ready or have qualifications and caveats and explain why.  Describe any areas of elevated risk and how they were managed.  Describe any proposed action plans, workarounds, and exit criteria, if required. 2.7.2.4.  List any test limitations or test deferrals, the rationale, and future plans to clear the limitations and/or deferrals.  Note that approval of deferred items does not eliminate or alter the requirement for OT of those areas.  PM, ITT and OTA must ensure deferred items are tested in subsequent OT, or the operational requirement document is changed. 2.7.2.5.  List any other system attributes or mission characteristics not ready for OT&E or not expected to meet operational requirements (e.g., known deficiencies). 2.7.2.6.  List any major areas of disagreement with the OTO, user(s), or other participants and accompanying rationale. 2.7.3.  Addressees.  The PM will summarize the final certification briefing in a memo to the OTO commander, with information copies to SAF/AQ, AF/TE, AF/A3/A4/A5/8, AFMC/A3/A2/A5 or AFSPC/A2/3/6/A5/8, as appropriate, the capability director, the PEO, the LDTO, operational/using MAJCOMs, and other participants.  The OT&E Certification Official will sign and release the memo. 2.7.4.  Certification Acknowledgment.  The OTO commander will acknowledge the certification memo before commencing dedicated OT.  The acknowledgment memo allows the OTO commander the opportunity to concur or non-concur with the OT&E Certification Official’s assessment, and restate any reservations or positions on unresolved issues.  If agreement cannot be reached between MDA and OTO at this point, outstanding issues may be elevated to SAF/AQ and AF/TE for final resolution. The OTO commander will send an acknowledgment memo to the addressees listed in paragraph 2.7.2. 2.7.5.  Decertification and Recertification.  Despite the developer's best efforts, systems may fail to perform as planned, and continuation of dedicated OT is not in the best interests of the Government.  The OT&E Certification Official may decertify the system and return it to DT&E based on written recommendation from the OTO or PM following a stop test due to poor system performance. The decertification memo includes the addressees listed in paragraph 2.7.2.  Before the system resumes dedicated OT, the OT&E Certification Official must again certify the system via memo according to paragraph 2.7 after appropriate AFMAN63-119  19 FEBRUARY 2016   19  corrective actions are taken.  If a system is decertified, all relevant templates should be revisited and the process tailored, if necessary, to improve future certification reviews of the system. 2.7.6.  Alternative to Decertification.  For system problems of a less serious or temporary nature, the OTO may pause testing for a brief time to assess the problem and determine if additional DT&E is warranted.  Note:  A series of pauses may indicate more serious problems requiring a stop test and system decertification. 2.8.  Updating the Templates 2.8.1.  The certification process and templates are expected to mature through feedback from certifications and as the acquisition process continues to evolve.  Further changes may result from advanced technologies, improved test and evaluation methods, revised DoD and Air Force policies, and restructuring of DoD T&E processes, procedures, and practices.  All certification process users should forward their observations and suggested improvements to AF/TEP.  Feedback is essential to keep the process and templates up to date.   20  AFMAN63-119  19 FEBRUARY 2016 Chapter 3 TEMPLATE STRUCTURE AND USE 3.1.  Interlocking Matrix 3.1.1.  PMs and OT&E Certification Officials shall utilize the templates to facilitate the review and help structure an executive-level briefing.  The templates (Attachments 2 through 32) form a matrix of interlocking subject areas spanning an entire acquisition or modification program.  Each template introduces order and helps reduce risk in a specific phase or aspect of a program.  Some duplication and cross-referencing between templates are necessary because acquisition and modification programs rely on many overlapping activities.  Decisions about risk in one area often affect other areas.  Cross-referencing also facilitates broad area reviews as well as special subject area reviews.  Closely associated templates are cited (e.g., “See A15”) to help find parallel information in other templates.  Note:  The templates are intended as checklists to facilitate the review and help structure the executive-level briefing.  All acronyms and abbreviations are described in Attachment 1. 3.2.  Consolidation of Multiple Sources.  Each template consolidates as much practical information as possible from multiple sources into a succinct "checklist."  Only a few of the most important AFIs, CJCSIs, DoDIs, and DoDDs are cited as footnotes to each template since complete document lists are impractical for this type and level of review, and different groups of documents may apply to different programs.  Programmatic and regulatory details are left to OPRs and collateral agencies more thoroughly conversant with specialized guidance.  Citation of minimum detail should help PMs, OT&E Certification Officials, testers, and users stay squarely focused on quality and readiness issues at the executive-level review. 3.3.  Answering Template Line Items.  Each template contains line items phrased as statements of fact rather than questions.  Each line item should elicit a brief summary of program status in that subject area rather than a superficial "yes" or "no" response.  The word “system” refers to software and hardware, as well as the human components of the program under review.  The entire group of statements covers the template subject area, but further analysis may be required in certain cases.  Line items may be answered individually or in groups depending on how the certification OPR, ITT, and/or PM tailor the certification process.  Each template can function as a "tailored checklist" and as a road map for future activities in preparation for dedicated OT.  As a general rule, aggregation of line items (and even whole templates) should increase as the review and briefing rise through the chain of command. 3.4.  Focus on Ends, Not Means.  The templates emphasize "what must be done" rather than "how to do it."  No specific problem solving methods are advocated, leaving PMs maximum flexibility to implement their own "best practices."  Certification participants are free to discuss and decide on the best way to remedy identified problems.  The templates focus on the ends, and the participants focus on the means. 3.5.  Assigning Responsibilities.  A single lead OPR is suggested for many of the line items on the templates to assist PMs and other participants to focus responsibility and increase accountability for results.  Final determination of each OPR should be made as required to improve organizational efficiency based on who is best suited to complete each task or final product.  Final approval authority for some line items may lie at higher levels.  While other AFMAN63-119  19 FEBRUARY 2016   21  agencies are expected to participate on a collateral basis, multiple OPRs and offices of collateral responsibility (OCR) are not listed since responsibility would be defocused, and all variations between programs cannot be covered.  Once identified and agreed upon, the OPR must produce a high quality review in the assigned areas and gain the required level of participation from OCRs. (T-1) The PM, with assistance from the ITT and CDT/Test Manager, is the OPR for ensuring the entire certification process is properly executed. 3.6.  Certification Template Tracking Tool.  An automated certification process tracking tool for all templates is available on AF/TE’s AF Portal at: https://www.my.af.mil/gcss-af/USAF/AFP40/d/s6925EC1351550FB5E044080020E329A9/Topics/Template_Tracking_Tool__2_/Template%20Tracking%20Tool%20V4.3%206%20Oct%202015.xls.  Modify this tool as needed to match any changes made to the templates. 3.7.  Information Collection and Records 3.7.1.  No information collections are created by this publication but an OT readiness certification memo will be produced through this process. 3.7.2.  Ensure that all program records created as a result of the processes prescribed in this publication are maintained according to AFMAN 33-363, and disposed of IAW AFRIMS and RDS.  Richard W. Lombardi Acting Assistant Secretary of the Air Force (Acquisition)   22  AFMAN63-119  19 FEBRUARY 2016 GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION Attachment 1 References Title 10, U.S.C. § 139, § 2302(5), § 2366, and § 2399, Operational Test and Evaluation of Defense Acquisition Programs  DoDI 3020.45, Defense Critical Infrastructure Program (DCIP) Management, 21 Apr 08 DoDD 3200.11, Major Range and Test Facility Base (MRTFB), 27 Dec 07 DoDI 3216.02 / AFI 40-402, Protection of Human Subjects and Adherence to Ethical Standards in Air Force Supported  Research, 10 Sep 14 DoD Manual 4140.01, Volume 7, DoD Supply Chain Materiel Management Procedures: Supporting Technologies, 10 Feb 14 DoD Manual 4140.01-M, Volume 2, DoD Supply Chain Materiel Management Procedures: Demand and Supply Planning, 10 Feb 14 DoDD 5000.01, The Defense Acquisition System, 12 May 03 DoDI 5000.02, Operation of the Defense Acquisition System, 7 Jan 15 DoDI 5129.47, Center for Countermeasures, 26 Jan 15 DoDI 5200.01, DoD Information Security Program and Protection of Sensitive Compartmented Information, 9 Oct 08 incorporating Change 1, 13 Jun 2011 DoD 5200.08-R, Physical Security Program, 9 Apr 07, Change 1, 27 May 09 DoDI 5200.39, Critical Program Information (CPI) Identification and Protection Within Research, Development, Test, and Evaluation (RDT&E), 28 May 2015 DoDI 5200.44, Protection of Mission Critical Functions to Achieve Trusted Systems and Networks (TSN), 5 Nov 12 DoDD 5250.01, Management of Intelligence Mission Data (IMD) in DoD Acquisition, 22 Jan 13 DoD 7000.14-R, Department of Defense Financial Management Regulations (FMRS), Vol 2A DoDI 8500.01, Cybersecurity, 14 Mar 14 DoDI 8510.01, Risk Management Framework (RMF) for DoD Information Technology (IT), 12 Mar 14  CJCSI 3010.02D, Guidance for Development and Implementation of Joint Concepts, 22 Nov 13 CJCSI 3170.01I, Joint Capabilities Integration and Development System, 23 Jan 15 JCIDS Manual series, Manual for the Operation of the Joint Capabilities Integration and Development System (JCIDS Manual), 12 Feb 15 NIPRNET:  https://www.intelink.gov/wiki/JCIDS_Manual; SIPRNET:  http://www.intelink.sgov.gov/wiki/JCIDS_Manual  CJCSI 6510.01F, Information Assurance (IA) and Support to Computer Network Defense CND), 9 Feb 11 AFMAN63-119  19 FEBRUARY 2016   23  Committee on National Security Systems Instruction (CNSSI) 1253, Security Categorization and Control Selection for National Security Systems, 27 Mar 14 AFPD 10-9, Lead Command Designation and Responsibilities for Weapon Systems, 8 Mar 07 AFPD 16-10, Modeling and Simulation, 23 Jan 15 AFI 10-601, Operational Capability Requirements Development, 6 Nov 13 AFI 10-401, Air Force Operations Planning and Execution, 13 Mar 12 AFI 14-111, Intelligence Support to the Acquisition Life Cycle, 18 May 12 AFI 16-1001, Verification, Validation and Accreditation (VV&A), 1 Jun 96 AFI 21-102, Depot Maintenance Management, 18 Jul 12 AFI 21-115 (Interservice), Product Quality Deficiency Report Program, 20 Jul 93 AFI 25-201, Intra-Service, Intra-Agency, and Inter-Agency Support Agreements Procedures, 18 Oct 13  AFI 32-7061, The Environmental Impact Analysis Process, 12 Mar 03 AFI 32-7086, Hazardous Materials Management, 4 Feb 15 AFPD 33-1, Cyberspace Support, 9 Aug 12 AFPD 33-2, Information Assurance Program, 3 Aug 11  AFPD 33-3, Information Management, 8 Sep 11 AFI 33-210, Air Force Certification and Accreditation (C&A) Program (AFCAP), 23 Dec 08 AFI 33-360, Publications and Forms Management, 25 Sep 13 AFMAN 33-363, Management of Records, 1 Mar 08 AFI 36-2201, Developing, Managing, and Conducting Training, 15 Sep 10, Chg 3, 7 Aug 13 AFI 36-2251, Management of Air Force Training Systems, 5 Jun 09 AFI 48-101, Aerospace Medicine Enterprise, 8 Dec 14 AFI 62-601, USAF Airworthiness, 11 Jun 10 AFI 63-101/20-101, Integrated Life Cycle Management, 7 Mar 13, Chg 2, 23 Feb 15 AFI 63-104, The SEEK EAGLE Program, 21 Jan 05 AFI 63-125, Nuclear Certification Program, 8 Aug 12 AFPAM 63-113, Program Protection Planning for Life Cycle Management, 17 Oct 13 AFPAM 63-128, Integrated Life Cycle Management, 10 Jul 14  AFI 63-501, Air Force Acquisition Quality Program, 31 May 94  AFI 63-131, Modification Management, 19 Mar 13 AFI 65-601, Vol I, Budget Guidance and Procedures, Chapter 14, 16 Aug 12 AFPD 90-11, Air Force Strategy, Planning, and Programming Process, 6 Aug 15   24  AFMAN63-119  19 FEBRUARY 2016 AFI 90-802, Risk Management, 11 Feb 13  AFPD 91-4, Directed Energy Weapons (DEW) Safety, 21 Oct 11 AFI 91-202, The US Air Force Mishap Prevention Program, 24 Jun 15  AFI 91-204, Safety Investigations and Reports, 12 Feb 14 AFI 91-217, Space Safety and Mishap Prevention Program, 11 Apr 14 AFI 99-103, Capabilities-Based Test and Evaluation, 16 Oct 13  AFI 99-109, Major Range and Test Facility Base (MRTFB) Test and Evaluation Resource Planning, 5 Feb 15 Technical Order (TO) 00-5-1, Air Force Technical Order System, 1 Oct 14 TO 00-5-3, Air Force Technical Manual Acquisition Procedures, 1 Apr 15 TO 00-5-16, Software Managers and User’s Manual For The USAF Automated Computer Program Identification Number System (ACPINS), 15 Apr 1 TO 00-35D-54, USAF Deficiency Reporting, Investigation, and Resolution, 1 Aug 15 MIL-STD-882E, DoD Standard Practice System Safety, 11 May 12 MIL-STD-1472G, DoD Design Criteria Standard Human Engineering, 11 Jan 12 National Institute of Standards and Technology (NIST) Committee on National Security Systems (CNSSI) 4009 Glossary, 6 Apr 15 CNSSP No. 11, Acquisition of Information Assurance (IA) and IA-Enabled Information Technology (IT) Products, 1 Jun 13 CNSSP No. 12, National IA Policy for Space Systems Used to Support National Security Missions, 28 Nov 12 Memorandum of Agreement [MOA] on Multi-Service Operational Test and Evaluation (MOT&E) and Joint Test and Evaluation (JT&E); Aug 00 http://www.dote.osd.mil/pub/policies/2000/200008MOAonMOTEandJTE.pdf NIST Special Publication (SP) 800-18 Rev. 1, Guide for Developing Security Plans for Federal Information Systems, Feb 06  NIST SP 800-30 Revision 1, Guide for Conducting Risk Assessments, Sep 12 NIST SP 800-37 Rev 1, Guide for Applying the Risk Management Framework to Federal Information Systems, Feb 10 includes updates as of 06-05-2014 NIST SP 800-39, Managing Information Security Risk, Mar 11 NIST SP 800-53 Rev 4, Security and Privacy Controls for Federal Information Systems and Organizations, Apr 13 includes updates as of 01-22-2015 NIST SP 800-53A Rev 4, Guide for Assessing the Security Controls in Federal Information Systems and Organizations, Dec 14 14 includes updates as of 12-18-2014 NIST SP 800-57, Rev 3, Key Management, Parts 1 (Rev3), 2, 3 (Rev 1), Jul 12  AFMAN63-119  19 FEBRUARY 2016   25  Glossary, Defense Acquisition Acronyms and Terms, 15th ed, Dec 12, Defense Acquisition University Defense Acquisition Guidebook, 15 May 13 DoD Cybersecurity Test and Evaluation Guidebook Version 1.0, 1 Jul 15 DoD Cyber Guide for Program Managers, 26 May 15 DoD Guide for Achieving Reliability, Availability, and Maintainability, 3 Aug 05  DoD Technology Readiness Assessment (TRA) Guidance, April 11, revised 13 May 11 DoD Test and Evaluation Management Guide, 6th ed., Dec 12, Defense Acquisition University Incorporating Test and Evaluation into Department of Defense Acquisition Contracts, Oct 11 DOT&E Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 Aug 14 DASD(AT&L) Memo, Document Streamlining—Program Protection Plan (PPP), 18 Jul 11 Air Force Test and Evaluation Guidebook, Version 1, Dec 04 Prescribed Forms No forms are prescribed by this publication. Adopted Forms AF Form 847, Recommendation for Change of Publication SF 368, Product Quality Deficiency Report Abbreviations and Acronyms A—Attachment ACAT—Acquisition Category ADM—Acquisition Decision Memorandum AFDD—Air Force Doctrine Document AFI—Air Force Instruction AFMAN—Air Force Manual AFMC—Air Force Materiel Command AFMSRR—Air Force Modeling and Simulation Resource Repository AFOTEC—Air Force Operational Test and Evaluation Center AFPD—Air Force Policy Directive AFRIMS—Air Force Information Management System AFROC—Air Force Requirements Oversight Council AFSPC—Air Force Space Command Ao—Operational Availability   26  AFMAN63-119  19 FEBRUARY 2016 AO—Authorizing Official AoA—Analysis of Alternatives APB—Acquisition Program Baseline ASR—Alternative Systems Review ATEC—Army Test and Evaluation Command ATO—Authorization to Operate ATS—Automatic Test Systems BDRSK—Battle Damage Repair Spares Kit C—a symbol for "contractor" C4—Command, Control, Communications, and Computers C4ISR—Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance C&L—Capabilities and Limitations CAT—Category CBRD—Capabilities-Based Requirements Document (this acronym used for this AFMAN only) CCA—Clinger-Cohen Act CDD—Capability Development Document CDR—Critical Design Review CDT—Chief Developmental Tester CJCSI—Chairman of the Joint Chiefs of Staff Instruction CJCSM—Chairman of the Joint Chiefs of Staff Manual CLS—Contractor Logistics Support CMP—Configuration Management Plan CNSSI—Committee on National Security Systems (CNSS) Instruction COA—Courses of Action COI—Critical Operational Issue COMSEC—Communications Security CONEMP—Concept of Employment CONOPS—Concept of Operations COOP—Continuity of Operations Plan COTS—Commercial-Off-The-Shelf CPD—Capability Production Document CSE—Common Support Equipment AFMAN63-119  19 FEBRUARY 2016   27  CTF—Combined Test Force CTP—Critical Technical Parameters DAB—Defense Acquisition Board DAG—Defense Acquisition Guidebook DASD (DT&E)—Deputy Assistant Secretary of Defense for Developmental Test and Evaluation  DAU—Defense Acquisition University DCR—DOTMLPF Change Recommendation DEW—Directed Energy Weapons DIA—Defense Intelligence Agency DIP—DIACAP Implementation Plan DoD—Department of Defense DoDD—Department of Defense Directive DoDI—Department of Defense Instruction DOE—Design of Experiments DOTMLPF—Doctrine, organization, training, materiel, leadership and education, personnel, and facilities DOT&E—Director, Operational Test and Evaluation DR—Deficiency Report, Deficiency Reporting DRB—Deficiency Review Board DRR—Design Readiness Review DSC—Decision Support Question DSOR—Depot Source of Repair DT&E—Developmental Test and Evaluation EDM—Engineering Development Model ELA—Elevated Level of Assurance EOA—Early Operational Assessment e.g.—for example ESOH—Environment, Safety, and Occupational Health et seq—and all that follows FCA—Functional Configuration Audit FDD—Full Deployment Decision FDDR—Full Deployment Decision Review   28  AFMAN63-119  19 FEBRUARY 2016 FDE—Force Development Evaluation FMRS—Financial Management Regulations FOC—Full Operational Capability FOT&E—Follow-on Operational Test and Evaluation FRP—Full-Rate Production FRPDR—Full Rate Production Decision Review FSA—Functional Solutions Analysis GFE—Government Furnished Equipment HPT—High Performance Team HSI—Human Systems Integration HQ USAF—Headquarters, United States Air Force IA—Information Assurance (replaced by “cybersecurity”) IATT—Interim Authorization to Test IAVA—Information Assurance Vulnerability Alert IAW—in accordance with ICD—Initial Capabilities Document ICS—Interim Contractor Support ID—Identification i.e.—that is IMD—Intelligence Mission Data IOC—Initial Operational Capability IOT&E—Initial Operational Test and Evaluation IS—Information System ISSM—Information System Security Manager ISACA—Information Systems Audit and Control Association ISP—Information Support Plan IT—Information Technology ITAB—Information Technology Acquisition Board ITC—Integrated Test Concept ITT—Integrated Test Team JCTD—Joint Capability Technology Demonstration JDRS—Joint Deficiency Reporting System AFMAN63-119  19 FEBRUARY 2016   29  JEON—Joint Emergent Operational Need JITC—Joint Interoperability Test Command JP—Joint Publication JPG—Joint Planning Guidance JRMET—Joint Reliability and Maintainability Evaluation Team JROC—Joint Requirements Oversight Council KPP—Key Performance Parameter KSA—Key System Attribute LCMP—Life Cycle Management Plan LCSP—Life Cycle Sustainment Plan LDTO—Lead Developmental Test and Evaluation Organization LFT&E—Live Fire Test and Evaluation LMDP—Life Cycle Mission Data Plan LRIP—Low-Rate Initial Production LRU—Line-Replaceable Unit LSC—Logistics Support Concepts MAC—Mission Assurance Category MAIS—Major Automated Information System MAJCOM—Major Command MCOTEA—Marine Corps Operational Test and Evaluation Activity MDA—Milestone Decision Authority MDAP—Major Defense Acquisition Program MDD—Materiel Development Decision MIPRB—Material Improvement Project Review Board MOA—Memorandum of Agreement MOE—Measure of Effectiveness MOP—Measure of Performance MOS—Measure of Suitability MOT&E—Multi-Service Operational Test and Evaluation MOU—Memorandum of Understanding MRSP—Mobility Readiness Spares Package MS—Milestone   30  AFMAN63-119  19 FEBRUARY 2016 MSSP—Modeling and Simulation Support Plan M&S—Modeling and Simulation MUA—Military Utility Assessment NCOW—RM—Net-Centric Operations and Warfare Reference Model NDI—Non-Developmental Item NEPA—National Environmental Policy Act NetRA—Network Risk Assessment NIST—National Institute of Standards and Technology NLT—not later than NR—KPP—Net-Ready Key Performance Parameter NSS—National Security System OA—Operational Assessment OCR—Office of Collateral Responsibility O&M—Operations and Maintenance OPSEC—Operations Security OPR—Office of Primary Responsibility OPTEVFOR—Operational Test and Evaluation Force OSD—Office of the Secretary of Defense OSS&E—Operational Safety, Suitability, and Effectiveness OT—Operational Testing OTA—Operational Test Agency OTO—Operational Test Organization OT&E—Operational Test and Evaluation OTRR—Operational Test Readiness Review OUE—Operational Utility Evaluation OV—Operational View PDR—Preliminary Design Review PEO—Program Executive Officer PESHE—Programmatic Environment, Safety, and Occupational Health Evaluation PHS&T—Packaging, Handling, Storage and Transportation PIA—Privacy Impact Assessment PIR—Post-Implementation Review AFMAN63-119  19 FEBRUARY 2016   31  PIT—Platform Information Technology PM—Program Manager PPBE—Planning, Programming, Budgeting, and Execution POA&M—Plan of Action and Milestones PPP—Program Protection Plan PTO—Participating Test Organization QOT&E—Qualification Operational Test and Evaluation QT&E—Qualification Test and Evaluation R&D—Research and Development R&M—Reliability and Maintainability RDS—Records Disposition Schedule RDT&E—Research, Development, Test and Evaluation RFP—Request for Proposal RM&A—Reliability, Maintainability, and Availability RMF—Risk Management Framework RSR—Requirements Strategy Review SAE—Service Acquisition Executive SAF—Secretary of the Air Force SAP—Security Assessment Plan SAR—Security Assessment Report SCG—Security Classification Guide SCRM—Supply Chain Risk Management SE—1.) Support Equipment, 2.) Systems Engineering SEP—Systems Engineering Plan SF—Standard Form SFR—Systems Functional Review SIMCERT—Simulator Certification SIMVAL—Simulator Validation SIP—System Information Profile SISSU—Security, Interoperability, Supportability, Sustainability, and Usability SOTR—Sufficiency of Operational Test Review SOW—Statement of Work   32 AFMAN63-119  19 FEBRUARY 2016 SP—Security Plan SP—Special Publication SPG—Strategic Planning Guidance SRR—Systems Readiness Review SSAA—Systems Security Authorization Agreement STAR—System Threat Assessment Report STAT—Scientific Test and Analysis Techniques SUT—System Under Test SV—System View SVR—System Verification Review T&E—Test and Evaluation TDS—Technology Development Strategy TDSB—Test Data Scoring Board TEMP—Test and Evaluation Master Plan TIPT—Test Integrated Product Team TO—Technical Order TOMA—Technical Order Management Agency TPP—Technology Protection Plan TRR—Test Readiness Review TSP—Transition Support Plan TTP—Tactics, Techniques, and Procedures UON—Urgent Operational Need USAF—United States Air Force USC—United States Code USD—Undersecretary of Defense V&V—Verification and Validation VV&A—Verification, Validation, and Accreditation WSEP—Weapons System Evaluation Program WIT—Watch Item Terms Note:  See AFI 10—601and AFI 63-101/20-101 for definitions of terms relating to the requirements and acquisition processes.  A common understanding of terms is essential to effectively implement this instruction.  In some cases, definitions from multiple sources are AFMAN63-119  19 FEBRUARY 2016   33  offered where they may be of value.  Italicized words and notes in parenthesis are not part of the formal definition and are offered only for clarity.  For additional terms and definitions not listed below, see Joint Publication (JP) 1-02, Department of Defense Dictionary of Military and Associated Terms.  An unofficial source is the Test and Evaluation Management Guide, 6th edition, Defense Acquisition University (DAU) Press. Acquisition Category (ACAT)—Acquisition categories determine the level of review, decision authority, and applicable T&E policies and procedures.  They facilitate decentralized decision making and execution, and compliance with statutorily imposed requirements.  See DoDI 5000.02, Enclosure 2 for details. Build—A version of software that meets a specified subset of the requirements that the completed software will meet or the period of time during which such a version is developed.  Note:  It may take several builds to reach a releasable version. Source: Glossary of Terms, home.btconnect.com/managingstandard/gloss. Capabilities and Limitations (C&L) Report—An optional, quick-look report of limited scope that operational testers provide to operational units to support rapid and/or early fielding of developing capabilities before dedicated operational testing is complete and formal production begins.  It provides the most current operational test perspectives on system capabilities and limitations based on testing done to date, and describes any untested or unknown areas. Capabilities—Based Requirements Document (CBRD)—Any formal requirements document (i.e., ICD, CDD, CPD, or DCR) used to support the Joint Capabilities Integration and Development System.  Note:  This definition is used solely to support this AFMAN. Capabilities—Based Testing—A T&E mission-focused strategy for verifying that a capabilities solution will enable operations at an acceptable level of risk.  Capabilities-oriented evaluations are the primary T&E methodology used throughout system testing, but traditional evaluations of system performance measured against specification-like requirements are also used.  Capabilities-based testing requires understanding operational concepts and involves developing strategies for T&E and plans to determine whether a capability solution option merits fielding.  (AFI 99-103) Chief Developmental Tester (CDT)—A designated government T&E professional in an MDAP or MAIS program office selected to coordinate, plan, and manage all DT&E activities, to include contractor testing, and who makes technically informed, objective judgments about DT&E results. For non-MDAP and non-MAIS programs, this person is known as the Test Manager. (AFI 99-103, based on 10 U.S.C. § 139b) Common Support Equipment (CSE)—Items currently in the DoD inventory and are applicable to multiple systems.  Technical documentation are cataloged as part of the federal logistics information system. Covered Product Improvement Program—See Covered System. Covered System—“covered system” is defined by 10 USC 2302. Critical Operational Issue (COI)—1.  Operational effectiveness and operational suitability issues (not parameters, objectives, or thresholds) that must be examined during operational testing to determine the system’s capability to perform its mission.  (paraphrased from DAU’s Test and Evaluation Management Guide)  2.  A key question to be answered by operational   34  AFMAN63-119  19 FEBRUARY 2016 testers when evaluating a system's overall operational effectiveness, suitability, and operational capabilities.  (AFI 99-103) Critical Technical Parameter (CTP)—Measurable critical system characteristic that, when achieved, allows the attainment of operational performance requirements.  A technical measure derived from user requirements.  Failure to achieve a critical technical parameter should be considered a reliable indicator that the system is behind in the planned development schedule or will likely not achieve an operational requirement.  (paraphrased from Defense Acquisition Guidebook) Cybersecurity—Prevention of damage to, protection of, and restoration of computers, electronic communications systems, electronic communications services, wire communication, and electronic communication, including information contained therein, to ensure its availability, integrity, authentication, confidentiality, and nonrepudiation. (DoDI 8500.01) Blue Team—1. The group responsible for defending an enterprise’s use of information systems by maintaining its security posture against a group of mock attackers (i.e., the Red Team). Typically the Blue Team and its supporters must defend against real or simulated attacks 1) over a significant period of time, 2) in a representative operational context (e.g., as part of an operational exercise), and 3) according to rules established and monitored with the help of a neutral group refereeing the simulation or exercise (i.e., the White Team).  2. A group of individuals that conduct operational network vulnerability evaluations and provide mitigation techniques to customers who have a need for an independent technical review of their network security posture. The Blue Team identifies security threats and risks in the operating environment, and in cooperation with the customer, analyzes the network environment and its current state of security readiness. Based on the Blue Team findings and expertise, they provide recommendations that integrate into an overall community security solution to increase the customer's cybersecurity readiness posture. Often times a Blue Team is employed by itself or prior to a Red Team employment to ensure that the customer's networks are as secure as possible before having the Red Team test the systems. . (CNSSI 4009 Glossary) Red Team—A group of people authorized and organized to emulate a potential adversary’s attack or exploitation capabilities against an enterprise’s security posture. The Red Team’s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment. Also known as Cyber Red Team.  (CNSSI 4009 Glossary) Enterprise Architecture—Enterprise architecture (EA) is "a well-defined practice for conducting enterprise  analysis, design, planning, and implementation, using a holistic approach at all times, for the successful development and execution of strategy. Enterprise architecture applies architecture principles and practices to guide organizations through the business, information, process, and technology changes necessary to execute their strategies. These practices utilize the various aspects of an enterprise to identify, motivate, and achieve these changes."  (Federation of EA  Professional Organizations, Common Perspectives on Enterprise Architecture, Architecture and Governance Magazine, Issue 9-4, November 2013) Dedicated Operational Testing—Operational test and evaluation that is conducted independently from contractors, developers, and operating commands and used to support production or fielding decisions.  (AFI 99-103) AFMAN63-119  19 FEBRUARY 2016   35  Deficiency  Report  (DR)—The  generic  term  used  within  the  USAF  to  record,  submit,  and transmit deficiency data which may include, but is not limited to, a Deficiency Report involving quality, materiel, software, warranty, or informational deficiency data submitted using Standard Form  (SF)  368,  Product  Quality  Deficiency  Report,  or  equivalent  format.    (TO  00-35D-54, USAF Deficiency Reporting, Investigation, and Resolution) Category  I  Deficiency—Those  which  may  cause  death,  severe  injury,  or  severe  occupational illness;  may  cause  loss  or  major  damage  to  a  weapon  system;  critically  restricts  the  combat readiness  capabilities  of  the  using  organization;  or  which  would  result  in  a  production  line stoppage. Category  II  Deficiency—Those  that  impede  or  constrain  successful  mission  accomplishment (system  does  not  meet  minimum  operational  requirements  but  does  not  meet  the  safety  or mission impact criteria of a Category I deficiency).  It may also be a condition that complements, but  is  not  absolutely  required  for,  successful  mission  accomplishment.    The  recommended enhancement, if incorporated, will improve a system’s operational effectiveness or suitability. Enhancement—A condition that improves or complements successful mission accomplishment but  is  not  absolutely  required.    The  recommendation,  if  incorporated,  will  enhance  a  system’s operational  safety,  suitability  and/or  effectiveness.    An  enhancement  report  should  not  be designated  as  such  solely  due  to  an  “out-of-scope”  condition  as  described  in  contractual requirements.  Contact the Contracting Officer immediately if there is a determination that any out-of-scope requirements are contemplated for addition to the contract. Deployment—1.  The movement of forces within operational areas.  2.  The relocation of forces and materiel to desired operational areas.  Deployment encompasses all activities from origin or home station through destination.  (JP 1-02) Developmental  Test  and  Evaluation  (DT&E)—Test  and  evaluation  conducted  to  evaluate design  approaches,  validate  analytical  models,  quantify  contract  technical  performance  and manufacturing  quality,  measure  progress  in  system  engineering  design  and  development, minimize  design  risks,  predict  integrated  system  operational  performance  (effectiveness  and suitability) in the intended environment, and identify system problems (or deficiencies) to allow for early and timely resolution.  DT&E includes contractor testing and is conducted over the life of the system to support acquisition and sustainment efforts.  (Defense Acquisition Guidebook) Early  Operational  Assessment  (EOA)—An  analysis,  conducted  in  accordance  with  an approved  test  plan,  of  the  program’s  progress  in  identifying  operational  design  constraints, developing  system  capabilities,  and  mitigating  program  risks.  For  programs  that  enter development at Milestone B, the lead OTA will (as appropriate) prepare and report EOA results after program initiation and prior to the Critical Design Review.  (DoDI 5000.02) Elevated  Level  of  Assurance  (ELA)—A  measure  of  confidence  that  the  security  features, practices,  procedures,  and  architecture  of  an  information  system  accurately  mediates  and enforces  the  security  policy.    On  the  Common  Criteria  predefined  assurance  scale,  higher (elevated)  levels  indicate  the  most  rigorous,  formal  criteria  for  security  evaluation.    (CNSS National IA Glossary) Evaluation  Criteria—Standards  by  which  the  accomplishment  of  required  technical  and operational  effectiveness  and/or  suitability  characteristics,  or  resolution  of  operational  issues, may be addressed.  (Defense Acquisition Guidebook)   36  AFMAN63-119  19 FEBRUARY 2016 Evaluation  Framework  Matrix—A  table  required  in  the  TEMP  that  shows  the  correlation between  the  COIs,  key  requirements  (KPPs  and  KSAs),  key  test  measures  (CTPs,  MOEs  and MOS), planned test methods, and test resources, facilities, or infrastructure needs. Fielding—The  decision  to  acquire  and/or  release  a  system  to  operators  in  the  field.    (AFI  99-103) Follow—on  Operational  Test  and  Evaluations  (FOT&E)—The  continuation  of  operational test and evaluation (OT&E) after IOT&E, QOT&E, or OUE and is conducted only by AFOTEC.  It  answers  specific  questions  about  unresolved  COIs  and  test  issues;  verifies  the  resolution  of deficiencies  or  shortfalls  determined  to  have  substantial  or  severe  impact(s)  on  mission operations;  or  completes  T&E  of  those  areas  not  finished  during  IOT&E,  QOT&E,  or  OUE.  (AFI 99-103) Force  Development  Evaluation  (FDE)— A type of OT&E performed by MAJCOM OTOs in support  of  MAJCOM-managed  system  acquisition-related  decisions  prior  to  initial  fielding,  or for MAJCOM sustainment or upgrade activities.  (AFI 99-103) Full  Deployment  Decision  (FDD)—FDD  is  the  decision  made  by  the  Milestone  Decision Authority  (MDA)  of  a  Major  Automated  Information  System  (MAIS)  acquisition  program authorizing an increment of the program to deploy software for operational use.  Title 10 U.S.C. § 2445A. Full—Up,  System-Level  Testing—Testing  that  fully  satisfies  the  statutory  requirement  for “realistic  survivability  testing”  or  “realistic  lethality  testing”  as  defined  in  Title  10  §  2366.  (Defense Acquisition Guidebook, Appendix 3) Hazard—1.  A  real  or  potential  condition  that  could  lead  to  an  unplanned  event  or  series  of events  (i.e.  mishap)  resulting  in  death,  injury,  occupational  illness,  damage  to  or  loss  of equipment or property, or damage to the environment  (Mil-Std-882E).  2. A condition with the potential  to  cause  injury,  illness,  or  death  of  personnel;  damage  to  or  loss  of  equipment  or property; or mission degradation  (JP 1-02) Implementing Command—Air Force Materiel Command and Air Force Space Command.  The command  providing  the  majority  of  resources  in  direct  support  of  the  program  manager responsible  for  development,  production,  and  sustainment  activities.    Such  resources  include technical  assistance,  infrastructure,  test  capabilities,  laboratory  support,  professional  education, training and development, management tools, and all other aspects of support, including support for product development and DT&E. (AFI 63-101/20-101) Increment—A  militarily  useful  and  supportable  operational  capability  that  can  be  effectively developed,  produced  or  acquired,  deployed,  and  sustained.    Each  increment  of  capability  will have  its  own  set  of  threshold  and  objective  values  set  by  the  user.    (AFI  10-601)    Note: Generally, only increments are fielded IAW DoDI 5000.02, CJCSI 3170.01, and AFI 63-101/20-101. Information  Support  Plan  (ISP)—A  set  of  information  supporting  interoperability  test  and certification.  Entered  through  the  Global  Information  Grid  Technical  Guidance  Federation (GTG-F)  portal,  the  ISP  contains  or  links  to  the  NR  KPP  along  with  supporting  architectural data.  (DoDI 8330.01) Initial Operational Test and Evaluation (IOT&E)—See Operational Test and Evaluation. AFMAN63-119  19 FEBRUARY 2016   37  Integrated Testing—The collaborative planning and collaborative execution of test phases and events to provide shared data in support of independent analysis, evaluation and reporting by all stakeholders,  particularly  the  developmental  (both  contractor  and  government)  and  operational test and evaluation communities.  (DAG, Chapter 9) Integrated  Test  Team  (ITT)—A  cross-functional  team  of  empowered  representatives  from multiple  disciplines  and  organizations  and  co-chaired  by  operational  testers  and  the  program manager.  The ITT is responsible for developing the strategy for T&E and the TEMP; assisting the acquisition community with T&E matters; and guiding the development of test plans that are integrated.  Note: The ITT is the Air Force equivalent to the T&E Working Integrated Product Team (T&E WIPT) described in DoDI 5000.02.  (AFI 99-103) Interoperability—The ability of systems, units or forces to provide data, information, materiel and  services  to  and  accept  the  same  from  other  systems,  units  or  forces  and  to  use  the  data, information, materiel and services so exchanged to enable them to operate effectively together.  IT and NSS interoperability includes both the technical exchange of information and the end-to-end  operational  effectiveness  of  that  exchange  of  information  as  required  for  mission accomplishment.  Interoperability is more than just information exchange.  It includes systems, processes, procedures, organizations and missions over the life cycle and must be balanced with information assurance.  (AFI 10-601) Joint Reliability and Maintainability Evaluation Team (JRMET)—The team responsible for collecting, analyzing, and categorizing R&M data during DT&E and OT&E.  It is chaired by the program  office  during  DT&E  and  the  operational  tester  during  dedicated  operational  testing.  The JRMET  includes representatives from the supporting  and operating commands, the DT&E and  OT&E  test  teams,  and,  when  appropriate,  system  contractor  personnel  and  nonvoting members. Lead  Command—The  command  that  serves  as  the  operators’  interface  with  the  Program Manager for a system as defined by AFPD 10-9.  (AFI 10-601) Lead  Developmental  Test  and  Evaluation  Organization  (LDTO)—The  lead  government developmental  test  organization  on  the  ITT  that  is  most  qualified  to  conduct  and/or  be responsible  for  overseeing  a  confederation  of  DT&E  organizations,  each  with  different  but necessary skills, in support of an acquisition program.  (AFI 99-103) Life  Cycle  Mission  Data  Plan  (LMDP)—A  statement  of  program  needs  that  is  applied throughout  the  life  of  an  Intelligence  Mission  Data  (IMD)-dependent  acquisition  program  and potentially influences programmatic decisions based on the availability of IMD over the life of the program. Life  Cycle  Sustainment  Plan  (LCSP)—A  plan  for  the  implementation,  management  and oversight  by  the  designated  PM  of  all  activities  associated  with  the  acquisition,  development, production, fielding, sustainment and disposal of a DoD weapon or materiel system across its life cycle.  (paraphrased from AFPAM 63-128) Live Fire Test and Evaluation (LFT&E)—The firing of actual weapons (or surrogates if actual weapons  are  not  available)  at  components,  subsystems,  sub-assemblies,  and/or  full-up,  system-level  targets  or  systems  to  examine  personnel  casualties,  system  vulnerabilities,  or  system lethality;  and  the  evaluation  of  the  results  of  such  testing.    (Defense  Acquisition  Guidebook, Chapter 9)   38  AFMAN63-119  19 FEBRUARY 2016 Logistics Support Elements—A composite of all support considerations necessary to ensure the effective and economical support of a system for its life cycle.  It is an integral part of all other aspects of system acquisition and operation.  (JP 1-02)  Note:  The twelve logistics support elements are: Sustaining/Systems Engineering; Design Interface; Supply Support; Maintenance Planning and Management; Support Equipment/Automatic Test Systems (SE/ATS); Facilities; Packaging, Handling, Storage, and Transportation (PHS&T); Technical Data Management/Technical Orders; Manpower and Personnel; Training; Computer Resources; Protection of Critical Program Information and Anti-Tamper Provisions.  Formerly known as Integrated Logistics Support.  (AFPAM 63-128) Logistics Supportability—The degree to which the planned product support allows the system to meet its availability and wartime usage requirements.  Planned product support includes the following:  test, measurement, and diagnostic equipment; spare and repair parts; technical data; support facilities; transportation requirements; training; manpower; and software.  (Defense Acquisition Guidebook, Chapter 5)  Note: In Air Force documents, the term “logistics supportability” is being replaced by the term “product support.” Logistics System Test and Evaluation—The test methodology, criteria, and tools for evaluating and analyzing the twelve logistics support elements [or product support elements in AFPAM 63-128] as they apply to a system under test.  The objective is to influence system design as early as possible in the acquisition cycle, and verify that the logistics support being developed is capable of meeting peacetime and wartime employment objectives.  (paraphrased from DAU’s Test and Evaluation Management Guide, 5th ed, January 05, Chapter 19) Low—Rate Initial Production (LRIP)—Production of the system in the minimum quantity necessary (1) to provide production-configured or representative articles for operational tests pursuant to § 2399; (2) to establish an initial production base for the system; and (3) to permit an orderly increase in the production rate for the system sufficient to lead to full-rate production upon the successful completion of operational testing.  (10 U.S.C. § 3300)  Note: The LRIP quantity should not normally exceed 10 percent of the total number of articles to be produced as determined at the Milestone B decision. Maintainability—The capability of an item to be retained in or restored to a specified condition when maintenance is performed by personnel having specified skill levels, using prescribed procedures and routines, at each prescribed level of maintenance and repair.  (Defense Acquisition Guidebook, Chapter 5) Major Munitions Program—See Covered System. Measurable—Having qualitative or quantitative attributes (e.g., dimensions, velocity, capabilities) that can be ascertained and compared to known standards.  (See Testable.) Measure of Effectiveness (MOE)—1.  The data used to measure the military effect (mission accomplishment) that comes from the use of the system in its expected environment. That environment includes the system under test and all interrelated systems, that is, the planned or expected environment in terms of weapons, sensors, command and control, and platforms, as appropriate, needed to accomplish an end-to-end mission in combat.  (DAU Glossary)   2. A criterion used to assess changes in system behavior, capability, or operational environment that is tied to measuring the attainment of an end state, achievement of an objective, or creation of an effect.  (JP 1-02) AFMAN63-119  19 FEBRUARY 2016   39  Measure of Performance (MOP)—1. System-particular performance parameters such as speed, payload, range, time-on-station, frequency, or other distinctly quantifiable performance features. Several MOPs may be related to the achievement of a particular measure of effectiveness.  (DAU Glossary)   2.  A criterion used to assess friendly actions that is tied to measuring task accomplishment.  (JP 1-02) Measure of Suitability (MOS)—Measure of an item’s ability to be supported in its intended operational environment. MOS's typically relate to readiness or operational availability and, hence, reliability, maintainability, and the item’s support structure.  (DAU Glossary) Military Utility—The military worth of a system performing its mission in a competitive environment including versatility (or potential) of the system.  It is measured against the operational concept, operational effectiveness, safety, security, and cost/worth.  Military utility estimates form a rational basis for making management decisions.  (Glossary, Defense Acquisition Acronyms and Terms) Military Utility Assessment (MUA)—A determination of how well a capability or system in question responds to a stated military need, to include a determination of its potential effectiveness and suitability in performing the mission.  It is a "characterization" of the capability or system as determined by measures of effectiveness, measures of suitability, measures of performance, and other operational considerations as indicators of military utility, as appropriate, and answers the questions, "What can it do?" and "Can it be operated and maintained by the user?"  (AFI 99-103) Multi—Service—Involving two or more military Services or DoD components.  (AFI 99-103) Multi—Service Operational Test and Evaluation (MOT&E)—OT&E conducted by two or more Service OTAs for systems acquired by more than one Service.  MOT&E is conducted according to the T&E directives of the lead OTO, or as agreed in a memorandum of agreement between the participants.  Note: MAJCOM OTOs may at times be responsible for conducting MOT&E in lieu of AFOTEC.  (AFI 99-103) Objective Value—Value of an attribute that is applicable when a higher level of performance represents a significant increase in operational utility.  The objective value is the desired operational goal achievable at a higher risk in cost, schedule, and technology.  Performance above the objective does not justify the additional expense.  (AFI 10-601) Operational Assessment (OA)—An OA incorporates substantial operational realism to assess progress toward achieving operational capabilities made by an independent operational test organization (OTO), with user support as required, on other than production systems.  The focus of an operational assessment is on significant trends noted in development efforts, programmatic voids, areas of risk, adequacy of requirements, and the ability of the program to support adequate operational testing.  Operational assessments may be made at any time using technology demonstrators, prototypes, mockups, engineering development models, or simulations, but will not substitute for the dedicated OT&E necessary to support full production decisions.  (AFI 99-103) Operational Availability (Ao)—1.  A measure of the degree to which an item is in the operable and committable state at the start of a mission when the mission is called for at an unknown (random) time.  (DAU’s Test and Evaluation Management Guide)    2.  The percentage of time that a system or group of systems within a unit are operationally capable of performing an   40  AFMAN63-119  19 FEBRUARY 2016 assigned mission and can be expressed as uptime/(uptime+downtime).  Development of the Operational Availability metric is a Requirements Manager responsibility.  (Glossary, Defense Acquisition Acronyms and Terms) Operational Effectiveness— The overall degree of mission accomplishment of a system or end item used by representative personnel in the environment planned or expected (e.g., natural, electronic, threat) for operational employment, considering organization, doctrine, tactics, cybersecurity, force protection, survivability, vulnerability, and threat (including countermeasures; initial nuclear weapons effects; and nuclear, biological, and chemical contamination threats). The PM maintains the operational effectiveness of the system by ensuring that it continues to satisfy the documented user operational capability requirements.  (AFI 63-101) Operational Safety—The level of safety risk to the system, the environment, and the occupational health caused by a system or end item when employed in an operational environment. The PM shall utilize the established system safety process to assure operational safety.  (AFI 63-101) Operational Suitability—The degree to which a system or end item can be placed satisfactorily in field use, with consideration given to availability, compatibility, transportability, interoperability, reliability, maintainability, wartime use rates, full-dimension protection, operational safety, human factors, architectural and infrastructure compliance, manpower supportability, logistics supportability, natural environmental effects and impacts, and documentation and training requirements.  (AFI 63-101) Operational Test Agency (OTA)—An independent agency reporting directly to the Service Chief that plans and conducts operational tests, reports results, and provides evaluations of overall operational capability of systems as determined by effectiveness, suitability, and other operational considerations.  Note: DoDD 5000.01 states, “Each Military Department shall establish an independent OTA.”  Therefore, each Service has one designated OTA which are as follows.  The Air Force has the Air Force Operational Test and Evaluation Center (AFOTEC).  The Navy has the Operational Test and Evaluation Force (OPTEVFOR).  The Army has the Army Test and Evaluation Command (ATEC).  The Marine Corps has the Marine Corps Operational Test and Evaluation Activity (MCOTEA). (AFI 99-103) Operational Test and Evaluation (OT&E)—1.  The field test, under realistic combat conditions, of any item of (or key component of) weapons, equipment, or munitions for the purpose of determining the effectiveness and suitability of the weapons, equipment, or munitions for use in combat by typical military users; and the evaluation of the results of such test.  (10 U.S.C.  § 139)  2.  Testing and evaluation conducted in as realistic an operational environment as possible to estimate the prospective system's operational effectiveness, suitability, and operational capabilities.  In addition, OT&E provides information on organization, personnel requirements, doctrine, and tactics.  It may also provide data to support or verify material in operating instructions, publications, and handbooks.  Note: The generic term OT&E is often substituted for IOT&E, QOT&E, FOT&E, OUE, FDE, WSEP, and TD&E, and depending on the context, can have the same meaning as those terms. Operational Testing—A generic term encompassing the entire spectrum of operationally oriented test activities, including assessments, tests, and evaluations.  Not a preferred term due to its lack of specificity.  (AFI 99-103) AFMAN63-119  19 FEBRUARY 2016   41  Operational Test Organization (OTO)—A generic term for any organization that conducts operational testing as stated in its mission directive.  (AFI 99-103) Operational Utility Evaluation (OUE)—Evaluations of military capabilities conducted to demonstrate or validate new operational concepts or capabilities, upgrade components, or expand the mission or capabilities of existing or modified systems. AFOTEC or MAJCOMs may conduct OUEs whenever a dedicated operational test and evaluation event is required, but the full scope and rigor of a formal IOT&E, QOT&E, FOT&E, or FDE is not appropriate or required.  OUEs may be used to support operational decisions (e.g., fielding a system with less than full capability) or acquisition-related decisions (e.g., low-rate production) when appropriate throughout the system life cycle.  OUEs will not be used when IOT&E, QOT&E, FOT&E or FDE are more appropriate per existing guidance and definitions.  (AFI 99-103) Operator—See “User.” Oversight—Senior executive-level monitoring and review of programs to ensure compliance with policy and attainment of broad program goals.  (AFI 99-103) Oversight Program—A program on the OSD T&E Oversight List for DT&E, LFT&E, and/or OT&E.  The list includes all major defense acquisition programs (MDAP) (e.g., ACAT I), Major Automated Information Systems (MAIS) (e.g., ACAT IA), DASD(DT&E), MDA-designated Special Interest programs and any programs selected for OSD T&E Oversight IAW 10 U.S.C. § 3330(a)(1).  The Special Interest designation is typically based on one or more of the following factors: technological complexity; congressional interest; a large commitment of resources; or the program is critical to the achievement of a capability or set of capabilities, part of a system of systems, or a joint program.  Oversight programs require additional documentation and have additional review, reporting, and approval requirements. (DoDI 5000.02) Participating Test Organization (PTO)—Any test organization required to support a lead test organization by providing specific T&E data or resources for a T&E program or activity.  (AFI 99-103) Penetration Testing—1. A live test of the effectiveness of security defenses through mimicking the actions of real-life attackers.  (Information Systems Audit and Control Association (ISACA) dictionary)  2. A method of evaluating the security of a computer system or network by simulating an attack from malicious outsiders (who have no access) and malicious insiders who have some level of authorized access.  (AFI 99-103) Platform IT (PIT)—Information technology, both hardware and software, that is physically part of, dedicated to, or essential in real time to the mission performance of special purpose systems. Examples of platforms that may include PIT are: weapons systems, training simulators, diagnostic test and maintenance equipment, calibration equipment, equipment used in the re-search and development of weapons systems, medical devices and health information technologies, vehicles and alternative fueled vehicles (e.g., electric, bio-fuel, Liquid Natural Gas that contain car-computers), buildings and their associated control systems (building automation systems or building management systems, energy management system, fire and life safety, physical security, elevators, etc.), utility distribution, telecommunications systems designed specifically for industrial control systems including supervisory control and data acquisition, direct digital control, programmable logic controllers, other control devices and advanced   42  AFMAN63-119  19 FEBRUARY 2016 metering or sub-metering, including associated data transport mechanisms (e.g., data links, dedicated networks).  (DoD Cybersecurity T&E Guidebook) Production Representative/Production Configuration—A system that can be used for initial operational test and evaluation (IOT&E), such as a mature engineering development model (EDM), or a low-rate initial production (LRIP) system in its final configuration, conforming to production specifications and drawings.  System-level critical design review (CDR), qualification testing, and functional configuration audit (FCA) should have been completed.  While desirable, the item does not have to be manufactured on a formal production line to be production representative.  (Glossary, Defense Acquisition Acronyms and Terms) Program Manager (PM)—1. The designated individual with responsibility for and authority to accomplish program objectives for development, production, and sustainment to meet the user’s operational needs.  The PM shall be accountable for credible cost, schedule, and performance reporting to the MDA.  (DoDD 5000.01) 2. Applies collectively to system program directors, product group managers, single managers, acquisition program managers, and weapon system managers.  Operating as the single manager, the PM has total life cycle system management authority.  Note: This AFMAN uses the term “PM” for any designated person in charge of acquisition activities, to include those prior to MS A (i.e., before a technology project is officially designated an acquisition program).  (AFI 99-103) Prototype—A model suitable for evaluation of design, performance, and production potential.  (JP 1-02)  Note:  The Air Force uses prototypes during development of a technology or acquisition program for verification or demonstration of technical feasibility.  Prototypes may not be representative of the final production item.  (AFI 99-103) Qualification Operational Test and Evaluation (QOT&E)—A tailored type of IOT&E performed on systems for which there is little to no RDT&E-funded development effort.  Commercial-off-the-shelf (COTS), non-developmental items (NDI), and government furnished equipment (GFE) are tested in this manner.  (AFI 99-103) Qualification Test and Evaluation (QT&E)—A tailored type of DT&E for which there is little to no RDT&E-funded development effort.  Commercial-off-the-shelf (COTS), non-developmental items (NDI), and government furnished equipment (GFE) are tested in this manner.  (AFI 99-103) Recoverability—Following combat damage, the ability to take emergency action to prevent loss of the system, to reduce personnel casualties, or to regain weapon system combat mission capabilities.  (Defense Acquisition Guidebook) Regression Testing—Type of software testing that seeks to uncover new software bugs, or regressions, in existing functional and non-functional areas of a system after changes such as enhancements, patches or configuration changes, have been made to them.  (Myers, Glenford (2004). The Art of Software Testing. Wiley) Release (pertaining to Software Development)—1. A distinct, tested, deployable software element of a militarily useful capability delivered to the government. (CMU/SEI-2013-TN-021, Parallel Worlds: Agile and Waterfall Differences and Similarities, Oct 13)  2. A delivered version of an application which may include all or part of an application. (ISA/IEC/IEEE 24765 Systems and Software Engineering– Vocabulary; December 15, 2010). AFMAN63-119  19 FEBRUARY 2016   43  Reliability—The ability of a system and its parts to perform its mission without failure, degradation, or demand on the support system.  (Defense Acquisition Guidebook) Research, Development, Test and Evaluation (RDT&E) Funding—The type of funding appropriation (3600) intended for research, development, test and evaluation efforts.  (DoD 7000.14-R, Department of Defense Financial Management Regulations (FMRS), Vol 2A, and AFI 65-601, Budget Guidance and Procedures, Vol I)  Note:  The term “research and development” (R&D) broadly covers the work performed by a government agency or the private sector.  “Research” is the systematic study directed toward gaining scientific knowledge or understanding of a subject area.  “Development” is the systematic use of the knowledge and understanding gained from research for the production of useful materials, devices, systems, or methods.  RDT&E includes all supporting test and evaluation activities.  (AFI 99-103) Resilience (Cyber)—The ability to prepare for and adapt to changing conditions and withstand and recover rapidly from disruptions. Resilience includes the ability to withstand and recover from deliberate attacks, accidents, or naturally occurring threats or incidents.  (CNSSI 4009 Glossary) Risk—1.  A measure of future uncertainties in achieving program performance goals and objectives within defined cost, schedule, and performance constraints. Defined by 1) the probability of an undesired event or condition, and 2) the consequences, impact or severity of the undesired event were it to occur.  (DAU Glossary, Defense Acquisition Acronyms and Terms)  2.  Probability and severity of loss linked to hazards.  (JP 1-02) Risk Management Framework (RMF)—DoD  uses Reference NIST SP 800-37, as implemented by Reference DoDI 8510.01, and is applicable to all DoD ISs and Platform Information Technology (PIT) systems. The RMF provides a disciplined and structured process that combines IS security and risk management activities into the system development life cycle and authorizes their use within DoD. The RMF has six steps: categorize system; select security controls; implement security controls; assess security controls; authorize system; and monitor security controls.  (DoDI 8500.01) Safety Release—The PM, in concert with the user and the T&E community, provides safety releases (to include formal ESOH risk acceptance in accordance with DoDI 5000.02, Enclosure 3, paragraph 16), to the developmental and operational testers before any test using personnel.  The safety release addresses known system hazards present during the test and includes formal acquisition risk acceptance for those system hazards. The program’s safety release is used in addition to test or range safety requirements and should support analyses of test/range unique hazards.  However, it may not cover all test/range hazards as the safety release is focused on acquisition ESOH lifecycle hazards of the system.  The program documents safety releases as part of the Program Record.  The PM should provide a transmittal letter to the involved test organization with a detailed listing of the system hazards germane to the test that includes the current risk level and documented risk acceptance along with information on all implemented mitigations. (DAG) Simulator Certification (SIMCERT)—The process of ensuring through validation of hardware and software baselines that a training system and its components provide accurate and credible training.  The process also makes sure the device continues to perform to the delivered specifications, performance criteria, and configuration levels.  It will also set up an audit trail   44  AFMAN63-119  19 FEBRUARY 2016 regarding specification and baseline data for compliance and subsequent contract solicitation or device modification.  (AFI 36-2251) Simulator Validation (SIMVAL)—The process for (1) comparing a training device’s operating parameters and performance to the current intelligence assessment of a weapon system, threat, and interaction between the weapon system and threat, and (2) documenting the differences and impacts. This process includes generation and deployment of an intelligence data baseline of the system, comparison of simulator characteristics and performance, support for the modification and upgrade of the simulator, a comparison of simulator and threat operating procedures, and correction of any significant deficiencies.  Uncorrected deficiencies are identified and published in validation reports.  The process continues throughout the life cycle of the simulator.  (AFI 36-2251) Specification—A document intended primarily for use in procurement which clearly and accurately describes the essential technical requirements for items, materials, or services, including the procedures by which it will be determined that the requirements have been met.  Specifications may be prepared to cover a group of products, services, or materials, or a single product, service, or material, and are general or detail specifications.  (Glossary, Defense Acquisition Acronyms and Terms) Spiral—One subset or iteration of a development program within an increment.  Multiple spirals may overlap or occur sequentially within an increment.  Note: An obsolete term, but may be in older documents.  Generally, spirals are not fielded IAW DoDI 5000.02, CJCSI 3170.01, and AFI 63-101/20-101. Strategy for T&E—A high-level conceptual outline of all T&E required to support development and sustainment of an acquisition program. Sufficiency of Operational Test Review (SOTR)—An examination by MAJCOM operational testers of all available test data to: 1) determine if adequate testing has been accomplished for programs of limited scope and complexity; and 2) to assess the risk of fielding or production without a dedicated OT&E.  An examination of existing test data, not an operational test per se. Support Equipment— (SE) Peculiar SE—SE under development in support of the system being tested. Common SE—Fielded SE that supports existing systems used in dedicated OT&E. Unique SE—Contractor or Government furnished SE for RDT&E use only. Survivability—The ability to withstand or repel attack, or other hostile action, to the extent that essential functions can continue or be resumed after onset of hostile action.  (DoD 5200.08-R, Physical Security Program)  Survivability consists of susceptibility, vulnerability, and recoverability.  (Defense Acquisition Guidebook) Susceptibility—The inherent capacity of an asset to be affected by one or more threats or hazards.  (DoDI 3020.45, Defense Critical Infrastructure Program (DCIP) Management).    (Susceptibility is a function of operational tactics, countermeasures, probability of enemy fielding a threat, etc.)  Susceptibility is considered a subset of survivability.  (Defense Acquisition Guidebook, Chapter 4) AFMAN63-119  19 FEBRUARY 2016   45  Sustainment—1 The provision of personnel, logistic, and other support required to maintain and prolong operations or combat until successful accomplishment or revision of the mission or of the national objective.  (JP 1-02)  2. The Service's ability to maintain operations once forces are engaged.  (AFDD 12, Air Force Glossary)  3. Activities that sustain systems during the operations and support phases of the system life cycle.  Such activities include any investigative test and evaluation that extends the useful military life of systems, or expands the current performance envelope or capabilities of fielded systems.  Sustainment activities also include T&E for modifications and upgrade programs, and may disclose system or product deficiencies and enhancements that make further acquisitions necessary. System—1 The organization of hardware, software, material, facilities, personnel, data, and services needed to perform a designated function with specified results, such as the gathering of specified data, its processing, and delivery to users. 2.) A combination of two or more interrelated pieces of equipment (or sets) arranged in a functional package to perform an operational function or to satisfy a requirement. (DAU) System of Systems—a set or arrangement of related interdependent systems that provide a given capability. The loss of any part of the system will significantly degrade the performance or capabilities of the whole. (CJCSI 3170.01) Testable—The attribute of being measurable with available test instrumentation and resources.  Note:  Testability is a broader concept indicating whether T&E infrastructure capabilities are available and capable of measuring the parameter.  The difference between testable and measurable may indicate a test limitation.  Some requirements may be measurable but not testable due to T&E infrastructure shortfalls, insufficient funding, safety, or statutory or regulatory prohibitions.  (AFI 99-103) Test and Evaluation (T&E)—The act of generating empirical data during the research, development or sustainment of systems, and the creation of information through analysis that is useful to technical personnel and decision makers for reducing design and acquisition risks.  The process by which systems are measured against requirements and specifications, and the results analyzed so as to gauge progress and provide feedback.  (AFI 99-103) Test and Evaluation Master Plan (TEMP)—Documents the overall structure and objectives of the T&E program.  It provides a framework within which to generate detailed T&E plans and it documents schedule and resource implications associated with the T&E program.  The TEMP identifies the necessary developmental, operational, and live-fire test activities.  It relates program schedule, test management strategy and structure, and required resources to: COIs; critical technical parameters; objectives and thresholds documented in the requirements document; and milestone decision points.  (DAU’s Test and Evaluation Management Guide) Test and Evaluation Organization—Any organization whose designated mission includes test and evaluation.  (AFI 99-103) Test Deferral—The movement or delay of testing and/or evaluation of a specific critical technical parameter, operational requirement, or critical operational issue to a follow-on increment or later test period.  A test deferral does not change the requirement to test a system capability or function.  (AFI 99-103)   46  AFMAN63-119  19 FEBRUARY 2016 Test Integrated Product Team (TIPT)—Any temporary group consisting of testers and other experts who are focused on a specific test issue or problem.  There may be multiple TIPTs for each acquisition program.  (AFI 99-103) Test Limitation—Any condition that hampers but does not preclude adequate test and/or evaluation of a critical technical parameter, operational requirement, or critical operational issue during a T&E program.  (AFI 99-103) Test Resources—A collective term that encompasses all elements necessary to plan, conduct, and collect/analyze data from a test event or program.  Elements include test funding and support manpower (including temporary duty costs), test assets (or units under test), test asset support equipment, technical data, simulation models, test beds, threat simulators, surrogates and replicas, special instrumentation peculiar to a given test asset or test event, targets, tracking and data acquisition, instrumentation, equipment for data reduction, communications, meteorology, utilities, photography, calibration, security, recovery, maintenance and repair, frequency management and control, and base/facility support services.  (DAU’s T&E Management Guide) Test Resource Plan (TRP)—The single document AFOTEC uses to request personnel and other resource support for operational test and evaluation from MAJCOMs and other agencies.  (AFI 99-103) Test Team—A group of testers and other experts who carry out integrated testing according to a specific test plan.   Note: A combined test force (CTF) is one way to organize a test team for integrated testing.  (AFI 99-103) Threshold Value—A minimum acceptable value of an attribute that is considered achievable within the available cost, schedule, and technology at low-to-moderate risk.  Performance below the threshold value is not operationally effective or suitable or may not provide an improvement over current capabilities.  (CJCSM 3170.01) Type 1 Training—Special Contract Training. One-time or limited nature; contracted with civilian industrial or educational institutions; includes commercial off-the-shelf courses; normally used to train selected personnel to operate and maintain new systems. Type 4 Training—Technical training conducted at operational locations may be delivered by a field training detachment (FTD) or a field training team (FTT). The FTD mission is to qualify personnel on new equipment and in new techniques and procedures, increase personnel skill and knowledge, acquaint personnel with specific systems, keep personnel up to date on training concepts and requirements, and maintain individuals at given proficiency levels. User—Refers to the operating command which is the primary command operating a system, subsystem, or item of equipment.  Generally applies to those operating commands or organizations designated by Headquarters, US Air Force to conduct or participate in operations or operational testing, interchangeable with the term "using command" or “operator.”  In other forums the term “warfighter” or “customer” is often used.  (AFI 10-601)  Also refers to maintainers.  “User” is the preferred term in this AFMAN.   (AFI 99-103) Verification, Validation and Accreditation (VV&A)—A continuous process in the life cycle of a model or simulation as it gets upgraded or is used for different applications.  (AFI 16-1001, Verification, Validation and Accreditation (VV&A)) AFMAN63-119  19 FEBRUARY 2016   47  Verification—Process of determining that M&S accurately represents the developer’s conceptual description and specifications. Validation—Rigorous and structured process of determining the extent to which M&S accurately represents the intended “real world” phenomena from the perspective of the intended M&S use. Accreditation—The official determination that a model or simulation is acceptable for use for a specific purpose. Vulnerability—The characteristics of a system that cause it to suffer a definite degradation (incapability to perform the designated mission) as a result of having been subjected to a certain level of effects in an unnatural (man-made) hostile environment. (Joint Publication 1-02, Department of Defense Dictionary of Military and Associated Terms, 8 November 2010, As Amended Through 15 January 2015))  Vulnerability is considered a subset of survivability.  (DAG) Waiver—A decision not to conduct OT&E required by statute or policy.  (AFI 99-103)    48  AFMAN63-119  19 FEBRUARY 2016 Attachment 2 ACQUISITION STRATEGY AND SCHEDULE A2.1.  Ensure  early  operational  tester  (AFOTEC  or  MAJCOM)  involvement  when  developing the  acquisition  strategy  to  ensure  the  strategy  for  T&E  provides  needed  support.    (PM)    (See A10, A11) A2.2.  Develop realistic, achievable, event-driven acquisition and test schedules and ensure they are harmonized throughout all program documents.  Avoid success-oriented schedules.  (PM) A2.3.  Congressional  and  PPBE  schedule  constraints  are  incorporated  into  the  acquisition schedule.  (PM) A2.4.  Ensure  sufficient  and  timely  RDT&E  funding  and  procurement  appropriations  are programmed during each budget cycle to keep the program in technical balance.  (PM, OTO) A2.5.  Schedule  sufficient  numbers  of  certification  reviews  over  the  program’s  projected  life cycle.  Frequency of reviews should increase as the program nears the start of dedicated OT&E.  (PM) A2.6.  Resolve  open  issues,  particularly  with  requirements,  sufficiently  early  to  permit  orderly planning and transition to dedicated OT&E.  (PM) A2.7.  If an  incremental strategy is used,  a clear distinction must exist  between each increment for determining what will be tested, produced and/or fielded.  (PM, User) (See A4, A10) A2.7.1.  Operational capabilities are clearly assigned to specific increments.  (PM) A2.7.2.  Provisions exist for developing and operationally testing subsequent increments after the initial increment is complete.  (PM) A2.8.  Ensure  contract(s)  capture  the  content  of  the  most  recent  CBRD  or  appropriate requirement document.  (PM) A2.9.  A  CDT  has  been  identified  (if  an  ACAT  1  or  MAIS  program),  or  a  Test  Manager  (or CDT) for other than MDAP and MAIS programs.  (PM) _______________________ Primary References: DoDI 5000.02 DAG, Chapters 2 & 9  AFI 63-101/20-101 AFI 99-103 AFI 10-601  AFMAN63-119  19 FEBRUARY 2016   49  Attachment 3 ANALYSIS OF ALTERNATIVES (AOA) A3.1.  The AoA (if required) may require updating, re-validation, and approval at the appropriate level prior to each milestone. (User) A3.2.  All reasonable alternatives must be objectively described.  The military value of the final alternatives must be clearly identified. (User) A3.2.1.  All  relevant  costs  must  be  identified,  preferably  using  objective  engineering  and business  estimates  derived  from  accepted  Air  Force  cost  analysis  principles  and  processes. (PM) A3.2.2.  All  assumptions  and  constraints  must  be  explicitly  identified  and  supported  by  the latest  CBRD,  AoA  guidance  documents,  or  reasonable  basis  determined  by  the  AoA sponsoring agency.  (User) A3.2.3.  Acceptable  ranges  of  performance  must  be  established  using  rigorous  cost-benefit, trade-off,  and  sensitivity  analyses  to  show  decision  makers  when  and  where  certain degradations in system cost or performance yield outcomes that no longer satisfy the mission need.  (User) A3.3.  Measures  of  Effectiveness  (MOE)  and  Measures  of  Suitability  (MOS)  must  reflect operational utility and show how they were derived from the requirements documents.  (OTO) A3.3.1.  MOEs and MOSs at the operational task level must be "testable" in order to develop DT&E  and  OT&E  plans  and  concepts.    MOEs  must  be  developed  as  early  as  possible  and agreed to between user and tester.  (OTO) A3.3.2.  The AoA’s MOEs, MOSs, Measures of Performance (MOP), and other criteria must be  linked  to  system  performance  thresholds  stated  in  the  latest  threat  and  requirements documents and "track" throughout the program's development.  (OTO) ______________________ Primary references: AFI 10-601    50  AFMAN63-119  19 FEBRUARY 2016 CAPABILITIES-BASED REQUIREMENTS DOCUMENTS (CBRD) Attachment 4 A4.1.  The  appropriate  CBRD  (i.e.,  Initial  Capability  Document  (ICD),  Draft  Capability Development  Document  (CDD),  CDD,  or  Capability  Production  Document  (CPD)),  and CONOPS must be coordinated and approved at appropriate levels prior to each milestone, after major  program  changes,  and  early  enough  to  develop  the  TEMP  and  OT&E  test  concept  and OT&E plan.  (User) A4.1.1.  AF Form 1067s issued for modification programs that introduce new capability must include a  Table  of  Performance  Parameters/Attributes  (KPP,  KSA,  other  or  attributes)  with minimum Threshold/Objective values similar to the format for a CDD/CPD.  (User) A4.1.2.  AF  Form  1067s  issued  for  permanent  sustainment  modifications  should  identify CDD/CPD requirements the modification is intended to sustain.  (User) A4.2.  The  CBRD  must  be  based  on  the  JPG,  Joint  Vision,  Air  Force  Vision,  and  long-range planning inputs from Joint and Air Force concepts.  (User) A4.3.  The  CBRD’s  capabilities  must  accurately  flow  down  through  the  AoA,  acquisition strategy, and TEMP to the OT&E concept and OT&E plan.  (User) A4.4.  The proposed system design must satisfy projected operational requirements in the CBRD and SPG.  (PM) A4.5.  The system must provide the needed capabilities against the most current validated threat described in the system’s threat documents.  (PM) A4.5.1.  Ensure Modeling and Simulation (M&S) requirements are identified early to enable programmed funding.  (PM)  (See A17) A4.5.2.  Cyberspace threats, attack surfaces, and security requirements must be current.  (See Figure 2.3)  (User) A4.6.  Joint,  multi-national,  multi-departmental,  or  multi-service  uses  described  in  the  CBRD must be addressed during the system's development.  (PM) A4.7.  All  thresholds  and  objectives  must  be  stated  in  operational  terms  and  defined  in measurable, beneficial increments of capability.  (User) A4.7.1.  Ensure  measureable  and  testable  criteria  for  how  the  system  supports  military operations,  is  entered  and  managed  on  the  network  and  how  effectively  it  exchanges information  is  specified  with  threshold  and  objective  values  IAW  the  Joint  Capabilities Integration and Development System (JCIDS) Net-Ready KPP requirement. (User) A4.7.2.  Cyber resiliency must be addressed through the JCIDS Survivability KPP. (PM) A4.8.  CBRDs  must  be  stated  in  such  a  manner  that  testable  MOEs,  MOSs,  and  MOPs  are quantitatively measurable through analytically-based evaluation methods when possible.  (User) A4.9.  All  CTPs,  KPPs,  MOEs,  MOSs,  MOPs,  threats,  definitions,  and  other  criteria  must  be consistent (harmonized) across the most  current support  documents (e.g., CBRD, system threat assessment, AoA, Air Force concepts, APB, TEMP).  (User) AFMAN63-119  19 FEBRUARY 2016   51  A4.10.  If increments of operational capability are planned, the CPD must be updated to describe the next increment prior to development of the OT&E concept and OT&E plan.  (User) A4.10.1.  Use Joint Requirements Oversight Council (JROC)-approved “IT Box” strategy for future increments if specified.  (User) A4.11.  Changes  must  be finalized and open issues  resolved early enough to  ensure no adverse impacts on the successful completion of dedicated OT&E.  (User) A4.12.  The  CBRD  must  contain  a  complete  audit  trail  documenting  rationale  for  all requirements changes, including changes from the APB.  (User) A4.13.  Only  systems  with  requirements  to  “protect  users  in  combat”  according  to  USC  10  § 2366 must be listed as “covered systems.”  (User) A4.14.  The CBRD must state the appropriate cybersecurity impact values (High, Moderate, and Low)  for  Confidentiality,  Integrity  and  Availability  as  well  as  listing  the  appropriate  security overlays  as  described  in  DoDI  8510.01,  Risk  Management  Framework  (RMF)  for  DoD  IT.  (User) A4.14.1.  Cybersecurity capabilities must also include the requirement to register the system in the Enterprise Information Technology Data Repository (EITDR), assignment of qualified personnel to RMF roles as well as an initial security control baseline. (User) _____________________ Primary references: DoDI 5129.47 DoDI 8500.01 DoDI 8510.01 DoDI 8580.1 DoDD 5250.01 CJCSM 3170.01 CJCSI 6212.01F AFI 10-601 AFI 99-103    52  AFMAN63-119  19 FEBRUARY 2016 Attachment 5 THREAT & INTELLIGENCE DOCUMENTS A5.1.  Threat assessment document(s) must remain valid and current with updates made prior to each milestone.  (User) A5.1.1.  Address  program  impacts  of  testing  against  emerging  threats  which  may  not  be stated in the current validated CBRD including cyberspace threats and impacts.  (OTO, PM) A5.2.  The  system  threat  assessment  document  must  be  approved  by  AF/A2.    For  ACAT  I programs, the System Threat Assessment Report (STAR) must be validated by DIA.  (User) A5.3.  The  system’s  threat  assessment  document(s)  must  be consistent  with  current  DoD  threat projections and accurately reflected in the CBRD and AoA.  (User) A5.4.  Sufficient  threat  detail  must  be  provided  to  support  system  R&D,  SE,  and  the development  of  realistic  operational  mission  scenarios  in  support  of  the  ITC,  OT&E  plan,  and schedules.  (PM) A5.4.1.  All threats must be described in system-specific terms and include system-to-system interfaces.  (PM) A5.4.2.  Threat shot doctrine and employment tactics must be described.  (PM) A5.4.3.  The reactive threat and potential countermeasures must be described.  (PM) A5.4.4.  Sources for projections and areas of uncertainty must be cited.  (PM) A5.4.5.  Adversarial cyber capabilities and tactics must be understood and described. (PM) A5.5.  Life-cycle mission data plans (LMDPs) shall be established by the program office, or its predecessor  organization,  for  each  Intelligence  Mission  Data  (IMD)-dependent  acquisition program and effort beginning at MS A.  (PM) __________________________ Primary References:  CJCSI 3170.01 CJCSM 3170.01 AFI 10-601 AFI 14-111 DoDI 5129.47 DoDD 5250.01  AFMAN63-119  19 FEBRUARY 2016   53  INTEGRATED TEST TEAM (ITT) STANDUP AND ITT CHARTER Attachment 6 A6.1.  New-start programs direct establishment of an ITT in the initial ADM as soon as possible after the Materiel Development Decision (MDD).  (Every program requires an ITT regardless of how long the program has been in existence.)  (PM) A6.2.  A  current  ITT  Charter  describes  ITT  activities,  membership,  goals,  products, responsibilities, and operating procedures.  (PM) A6.2.1.  A CDT is  identified for MDAP and MAIS programs, or  a Test  Manager (or CDT) for all other programs.  This person and the OTO representative co-chair the ITT.  (PM) A6.2.2.  The charter covers the entire life cycle of the program.  (PM) A6.2.3.  If  the  system  comes  under  an  overarching  ITT  of  related  systems,  the  ITT  Charter includes provisions for managing multiple systems.  (PM) A6.2.4.  All  program  stakeholders  are  represented  (e.g.,  other  Services,  interoperable systems, and organizations supporting all types of T&E activities).  (PM) A6.2.5.  The ITT has sufficient membership participation to be effective.  (PM) A6.3.  The  ITT  directs  formation  of  sub-groups  to  address  specific  tasks  and  responsibilities.  (ITT) A6.4.  Research is completed to identify and nominate an LDTO to the PEO or decision review authority, in coordination with AFMC/A3 or AFSPC/A5, as appropriate.  (ITT) _______________________ Primary References:  DoDI 5000.02, Enclosure 4 AFI 99-103 Air Force T&E Guidebook, Atch 2    54  AFMAN63-119  19 FEBRUARY 2016 Attachment 7 AIR FORCE CONCEPTS A7.1.  The  Air  Force  concepts  must  describe  expected  system  employment  and  operating concepts, strategies, methods, and tactics in concert with the latest CBRD.  (User) A7.1.1.  Sufficient  detail  must  permit  early  development  of  operationally  realistic  test scenarios and tactics for the OT&E test concept and test plans.  (User) A7.2.  Operational effectiveness and suitability requirements, criteria, thresholds, objectives, and definitions in the CBRD must accurately flow down (be linked) to the Air Force concepts, which must in turn be linked to the OT&E test concept and OT&E plan. (User) A7.2.1.  Changes  in the CBRD, system threat assessment document, AoA,  logistics support concepts  (LSC),  and  TEMP  must  be  analyzed  for  potential  impacts  on  Air  Force  concepts, which in turn affect T&E plans.  (User) A7.2.2.  Changes in the Air Force concepts must be finalized and open issues resolved early enough  to  ensure  no  adverse  impacts  on  the  successful  completion  of  DT&E,  Integrated Testing, cyber T&E, and dedicated OT&E.  (User) A7.3.  Air  Force  concepts  must  be  available  to  support  development  of  operationally  relevant DT&E and OT&E scenarios.  (User) _________________________ Primary References: CJCSI 3170.01 AFI 10-601 AFI 10-401 AFPD 10-28  AFMAN63-119  19 FEBRUARY 2016   55  Attachment 8 LIFE CYCLE SUSTAINMENT PLAN (LCSP) A8.1.  The  LCSP  must  describe  the  optimal  system  maintenance  strategies,  concepts,  and methods based on the CBRD’s requirements.  (User) A8.1.1.  The  system  must  use  an  acceptable  inter-Service,  organic,  and/or  contractor  mix.  (PM) A8.1.2.  The  LCSP  must  identify  potential  high-risk  and  problem  areas  (such  as  long  lead items, TOs, system reliability, support equipment, training).  (User) A8.2.  Logistics and readiness criteria, thresholds, objectives, and definitions in the CBRD must accurately flow down  (be linked) to the LCSP,  which  must  in  turn be linked to  the MOEs and MOPs in the OT&E concept and plan.  (User) A8.3.  LCSP  strategies  and  plans  must  be  sufficiently  detailed  to  support  early  development  of the OT&E concept and OT&E plan.  (User) A8.4.  Realistic  operational  and  suitability  test  scenarios  that  support  the  integrated  test  plan must be developed from the LCSP and other Air Force concepts.  (OTO) A8.5.  The  system  must  be  supportable  in  dedicated  OT&E  using  the  LCSP's  strategies  and plans.  (PM) A8.6.  The system's design must successfully address the quantitative and qualitative constraints identified in the LCSP.  (PM) A8.7.  The  Doctrine,  organization,  training,  materiel,  leadership  and  education,  personnel,  and facilities  (DOTMLPF)  elements  must  be  sufficient  to  support  the  LCSP  and  maintenance  plan during dedicated OT&E.  (OTO) A8.8.  The  Depot  Source  of  Repair  (DSOR)  decision  has  determined  the  optimal  maintenance posturing decisions needed to support warfighter operational requirements.  (PM) A8.9.  Reliability  and  maintainability  (R&M)  growth  plans  are  developed,  coordinated,  and documented in the Systems Engineering Plan (SEP) and TEMP.  (PM) A8.10.  The  LCSP  integrates  the  acquisition  and  product  support  strategies  throughout  the system’s life cycle.  The LCSP must support Milestone B and follow-on decisions.  (PM)  Note:  Space systems are exempt from this requirement. _________________________ Primary References:  AFPD 10-28 AFI 63-101/20-101 DoD Guide for Achieving Reliability, Availability, and Maintainability    56  AFMAN63-119  19 FEBRUARY 2016 Attachment 9 INFORMATION TECHNOLOGY (IT) AND NATIONAL SECURITY SYSTEMS (NSS) A9.1.  The  NR-KPP  prescribed  in  the  CBRD,  consists  of  testable  characteristics,  and  contains performance  measures  required  for  the  timely,  accurate,  and  complete  exchange  and  use  of information.  (User) A9.1.1.  Architecture  products  (e.g.,  OV-5,  OV-6,  and  SV-1  to  SV-7)  are  developed  and available to the test community.  (User, PM) A9.1.2.  Key interface profiles are identified and complied with as applicable.  (User) A9.1.3.  Cybersecurity  capabilities  are  planned  and  designed  into  system  specifications  and configurations using the latest threat estimates.  (PM) A9.1.3.1.  An Action Officer (AO) and Information System Security Manager (ISSM) are formally assigned in writing.  (PM) A9.1.3.2.  Impact values for Confidentiality, Integrity and Availability as well as a listing of the security overlays are in requirements documents and the TEMP.  (User)  (See A4) A9.1.3.3.  The  cybersecurity  strategy,  as  an  appendix  to  the  Program  Protection  Plan (PPP), is complete and available to the T&E community.  (PM)  (See A12) A9.1.3.4.  RMF  is  implemented  and  the  T&E  community  invited  to  observe  and participate in process activities.  (PM, LDTO) A9.1.4.  The  High  Performance  Team’s  (HPT)  architecture  expert  has  ensured  compliance with  the  DoD  Information  Enterprise  Architecture,  Version  1.1  and  provided  a  compliance statement to the program office. (User) A9.2.  The Information Support Plan (ISP), Security Classification Guide (SCG), and all RMF –related  documents  (including  a  compiled  list  of  system  characteristics  or  qualities  required  for system  registration,  key  security-related  documents  such  as  a  risk  assessment,  privacy  impact assessment,  system  interconnection  agreements,  contingency  plan,  security  configurations, configuration  management  plan,  and  incident  response  plan)  are  complete  and  available  to  the T&E community as early as possible.  (PM) A9.2.1.  ISP, SCG, RMF documentation, and PPP are consistent with the TEMP, strategy for T&E, and support T&E execution activities.  (PM) A9.3.  System cybersecurity training for AOs, ISSMs, security systems administrators, and users is available and completed.  (PM) A9.3.1.  Trained teams are used to conduct passive and active scans to reveal system/network vulnerabilities, verify system protection and detection capabilities, and complete a Network Risk Assessment (NetRA) or equivalent as outlined in the TEMP and ISP.  (LDTO) A9.3.2.  Trained  and  certified  teams  are  used  as  opposition  forces  to  conduct  penetration testing  for  assessing  the  cybersecurity  posture  of  the  system/network  as  part  of  a vulnerability analysis assessment or equivalent as outlined in the TEMP and ISP.  (LDTO) A9.4.  NetRA and other interoperability or net-ready certification activities are complete.  (PM, LDTO, JITC) AFMAN63-119  19 FEBRUARY 2016   57  A9.4.1.  All  developer/test  passwords,  password  scripts,  and  accounts  in  use  during  system development are deleted prior to operational testing.  (PM) A9.4.2.  Compliance with cybersecurity vulnerability alerts will not impact any other type of system certification or potentially invalidate test data.  (PM, LDTO) A9.4.3.  Data passed to and from other interoperable systems must be compatible.  (PM) A9.4.4.  JITC has provided an OTRR Interoperability Statement as required.  (PM) A9.4.5.  The  AO  has  obtained  an  Interim  Authority  to  Test  (IATT)  or  Authorization  to Operate (ATO) memo (as appropriate) prior to test efforts.  (PM) A9.5.  Systems and subsystems comply with the USAF Electromagnetic Compatibility Program and Radio Frequency Spectrum Management guidelines.  (PM) A9.6.  Other  systems  and  subsystems  required  to  interoperate  with  the  test  articles  (including external systems) are available.  (PM, OTO)  (See A31) ________________________ Primary References: CJCSI 6212.01 DoDI 8500.01 DoDI 8500.2 DoDI 8510.01 AFPD 33-1 AFPD 33-2 AFI 33-210 AFPAM 63-113 DoD CIO Memorandum, DoD Information Enterprise Architecture, Version 1.1, 27 May 09 National  Security  Telecommunications  and  Information  Systems  Security  Policy  (NTISSP) No.11 and No.12    58  AFMAN63-119  19 FEBRUARY 2016 Attachment 10 TEST & EVALUATION MASTER PLAN (TEMP) A10.1.  The  TEMP  must  be  updated,  coordinated,  and  approved  at  appropriate  levels  prior  to each milestone and after major program changes.  (PM) A10.1.1.  Open  issues  must  be  addressed  and  resolved  before  submission  to  HQ  USAF.  Changes  required  by  OSD  or  other  decision  authorities  must  be  incorporated  as  agreed.  (PM) A10.1.2.  Coordination  must  be  timely  and  efficiently  planned  to  minimize  chances  of  late rejection and negative impacts on dedicated OT&E.  (PM) A10.2.  Level of detail must be appropriate for the stage of development, and “TBDs” eliminated as  much  as  possible.    MOEs,  MOSs,  CTPs,  COIs,  and  Decision  Support  Questions  (DSQ)  are included in the Developmental Evaluation Framework (DEF) Matrix.    (PM) A10.3.  The  TEMP  must  accurately  reflect  the  most  recent  CBRD,  system  threat  assessment documents, LSC, Air Force concepts, and AoA.  (PM) A10.4.  The  TEMP  must  clearly  summarize  relationships  between:  1)  the  strategy  for  T&E, program  schedule,  and  required  resources;  2)  CBRD  parameters,  COIs,  CTPs,  MOEs,  MOSs, and  3)  DEF,  KPPs,  CTPs,  KSAs,  Failure  Modes,  Effects  and  Criticality  Analysis  (FMECA), interoperability  requirements,  cyber-security  requirements,  reliability  growth,  maintainability attributes and developmental test objectives,  other evaluation criteria, and decisions supported.  (PM,OTO) A10.4.1.  The  OT&E  concept  and  plan  must  be  executable  in  terms  of  structure,  schedule, and resources.  (PM) A10.4.2.  The requirements strategy (as reflected in the ICD and draft CDD) and acquisition strategy are fully supported (manpower, funding, test infrastructure, articles including M&S, and agencies).  (PM)  (see A2) A10.4.3.  T&E  test  resource  shortfalls  or  limitations  potentially impacting  dedicated  OT&E must be identified.  (PM) A10.4.4.  Describe the M&S assets needed for dedicated OT&E.  (PM, OTO) A10.4.5.  Ensure the VV&A process and agency responsibilities are described for each M&S capability, to include expected products and approvals.  (PM, OTO)  (See A17) A10.4.6.  If LFT&E is required, include the LFT&E strategy in the TEMP.  (PM)  (See A16) A10.4.7.  Appropriate  cyber  test  measures  included  to  evaluate  operational  capability  to protect, detect, react, and restore to sustain continuity of operation.  (PM) A10.4.8.  Ensure Cost Capability Analysis has been completed.  (PM) A10.5.  The TEMP must describe what DT&E, OT&E, or integrated test has done or will do to ensure  the  system  has  the  potential  to  meet  operational  requirements  in  dedicated  OT&E, including assessing schedule and product risks with requisite margins, and assessing mitigation plans of above.  (PM) AFMAN63-119  19 FEBRUARY 2016   59  A10.5.1.  Show  how  all  COIs,  MOEs  and  MOSs  will  be  addressed  in  dedicated  OT&E.  (OTO) A10.5.2.  Contractor-conducted  vs  Government-conducted  DT  and  OT  are  clearly distinguished and mutually supportive.  (ITT) A10.6.  Rationale and provision must be made for any planned OT&E deferred beyond dedicated OT&E into Follow-On Test and Evaluation (FOT&E) or follow-on increments.  (PM) A10.7.  Links  to  required  detailed  information  cited  in  the  TEMP  must  be  functional  and  the linked information complete.  Sufficient detail is available for: (PM) A10.7.1.  Reliability growth curves and planning.  (PM) A10.7.2.  Scientific  Test  and  Analysis  Techniques  (STAT)  calculations  and  analyses.    (PM and OTA) A10.7.3.  Allocation of reliability among key components.  (PM) A10.7.4.  Anticipated development and test problem areas. (PM and LDTO) A10.7.5.  Resolution of past deficiencies. (PM) A10.7.6.  Cyber  T&E  strategy  and  resources  and  includes  specified  cyber  content:  architecture,  operational  environment,  evaluation  structure,  ATO,  time  and  resources, cooperative  vulnerability  and  penetration  assessment,  and  adversarial  assessment.    (LDTO, OTO) A10.7.6.1.  Cyber  test  (cooperative  vulnerability  and  penetration  assessment,  and adversarial assessment) events for DT&E, OT&E, and Integrated Test.  (LDTO, OTO) A10.7.6.2.  RMF planning is described.  (PM) A10.8.  The  requirements  strategy  (as  reflected  in  the  ICD  and  draft  CDD)  and  acquisition strategy are fully supported (manpower, funding, test infrastructure, articles including M&S, and agencies).  (PM)  (see A2) A10.9.  Ensure  the  TEMP  includes  applicable  test  scenarios,  appropriate  data  collection (established T&E database), and performance evaluation over the life cycle of the system. (PM) ___________________________ Primary References: DoDI 5000.02 DAG, Chapter 9  AFI 99-103 AFI 63-101/20-101 AFI 63-107 AFPAM 63-128 DoD Cybersecurity T&E Guidebook DOT&E Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 Aug 2014, Atch D      60  AFMAN63-119  19 FEBRUARY 2016 Attachment 11 INTEGRATED TEST PLANNING A11.1.  Ensure  integrated  test  planning  starts  as  early  as  practical  to  make  T&E  schedules  and resource expenditures more efficient and eliminate duplication of effort.  (PM) A11.1.1.  A  rigorous  SEP  identifies  how  T&E  will  be  used  to  achieve  program  goals  and technical results.  (PM) A11.1.2.  A  rigorous  TEMP  specifies  how  T&E  will  be  planned  and  used  to  verify  and validate  program  requirements  to  ensure  the  system  is  operationally  effective  and  suitable.  (PM)  (See A10) A11.1.3.  DT&E and OT&E plans and concepts are structured so that OT can capture, apply DT&E data to reduce OT&E timelines and requirements.  (ITT)  (See A13, A14, A23, A25) A11.1.4.  OAs  are  planned  at  strategic  points  in  the  development  program.    OAs  and  early user inputs influence system design and function.  (PM, OTO) (See A10) A11.1.5.  Other  types  of  T&E  (e.g.,  cybersecurity,  LFT&E,  contractor)  are  incorporated  as much as practical in the integrated test design.  (PM) A11.1.6.  Dedicated operational test and developmental test objectives are not compromised.  (ITT) A11.1.7.  STAT  process  employed  to  ensure  T&E  is  effective,  efficient,  and  appropriate factors  and  conditions  selected  to  produce  the  data  required  to  characterize  system capabilities.  (PM) A11.1.8.  Ensure  the  Integrated  Test  Concept  (ITC)  and  TEMP  reflect  the  most  current program direction.  (PM) A11.2.  Definitions, formulas, and evaluation criteria used to determine operational effectiveness and suitability must be consistent between all individual test plans and T&E documents.  (ITT) A11.3.  A  common  T&E  database  is  used  to  archive  all  T&E  data  from  all  test  organizations.  (PM) A11.3.1.  Parameters and formats are agreed upon by all test teams.  (ITT) A11.3.2.  Test item configurations are rigorously controlled.  (PM)  (See A18, A22) AFMAN63-119  19 FEBRUARY 2016   61  A11.4.  Integrated test matrices are addressed in the TEMP and depicts all T&E events and who will accomplish them.  (ITT) A11.4.1.  Duplication and voids in testing are minimized.  (ITT)  (See A22) A11.4.2.  A  prudent  number  of  backup  resources  (e.g.,  test  assets,  funds)  are  available  to supplement  all  testing  if  planned  integrated  DT&E/OT&E  data  is  unusable  or  unavailable.  (PM)  (See A22, A25) _______________________ Primary References: DoDI 5000.02, Enclosures 4 and 5 DAG, Chapters 2 and 9 AFI 99-103    62  AFMAN63-119  19 FEBRUARY 2016 Attachment 12 CYBER RESILIENCY A12.1.  Cyber  resiliency  goes  beyond  “cybersecurity”  to  include  cyber-attack  detection  and response.  Ensure cybersecurity phases shown in Figure 2.3 are reviewed to ensure currency of the strategies for operational requirements, acquisition, T&E, and cybersecurity.  Use Figure 2.3 for the rest of this template.  (ITT) A12.1.1.  System's  Cybersecurity  Strategy,  Security  Plan  (SP),  SCG,  and  PPP  are  current.  (PM) A12.1.2.  Cyber resiliency assessments are integrated into DT&E and OT&E.  (ITT) A12.1.2.1.  OT  plan  addresses  required  DOT&E  cybersecurity  content:    TEMP  linkage, architecture,  intelligence  community-validated  cyber  threat,  operational  environment, evaluation  structure,  time  and  resources,  cooperative  vulnerability  and  penetration assessment, and adversarial assessment.  Plan should also address cybersecurity software assurance  considerations.    See  A15.    Ref  DOT&E  Memo,  Procedures  for  Operational Test  and  Evaluation  of  Cybersecurity  in  Acquisition  Programs,  1  Aug  2014,  Atch  E.  (ITT) A12.1.3.  Six-step  RMF  process  (1.  Categorize  System,  2.  Select  Security  Controls,  3. Implement Security Controls, 4. Assess Security Controls, 5. Authorize System, 6. Monitor Security  Controls)  is  followed.    Establish  an  ITT  sub-group  to  monitor  and  control,  if necessary.  (PM) A12.1.4.  Confidentiality,  Integrity,  and  Availability  ratings  as  well  as  a  listing  of  security overlays are properly assigned.  (PM) A12.1.5.  Applicable  overlays  are  applied  so  that  appropriate  security  controls  are  selected and updated.  (PM) A12.1.6.  Cyber-attack surfaces, threats, etc. are properly characterized and updated.  (PM) A12.1.7.  Cyber kill chain is correctly understood, analyzed, and updated.  (PM) A12.1.8.  System's  Functional  Hazard  Analysis  is  current  for  cooperative  vulnerability, penetration assessment and adversarial team baseline, and determining safety and real-world operations considerations. (PM) A12.2.  Cyber  test  infrastructure  (with  appropriate  architecture,  level  of  realism,  and  security) and documentation is available and described in the TEMP.  (PM) A12.2.1.  System owners agree on rules of engagement for all teams.  (PM) A12.2.2.  Reciprocity agreements are in place between teams and other Services.  (PM) A12.2.3.  Test plans with refined cyber T&E scenarios, operational capability requirements, potential test venues, mission threads, and simulated scenarios are developed and approved.  (ITT) A12.2.4.  Funding is available to complete cooperative vulnerability, penetration assessment, and adversarial assessment test events.  (PM) AFMAN63-119  19 FEBRUARY 2016   63  A12.2.5.  The IATT and ATO are available at the appropriate times.  (PM) A12.2.6.  Security  Assessment  Report  (SAR)  is  prepared,  recommended  corrective  actions and system weaknesses are addressed and prepared for.  (PM) A12.2.7.  All  cyber  testing  planned  to  be  conducted  on  a  cyber  range  is  identified  and  all events integrated with OT&E and assessment activities.  (PM) A12.2.8.  All  necessary  linkages  between  the  cyber  range  and  operational  networks  are developed.  (PM) A12.2.9.  Integration  plan  established  for  system  operators,  network  defenders,  and  threat emulations on planned Cyber Range if conducting cyber range testing.  (PM) A12.3.  Cooperative  vulnerability,  penetration  assessment  and  adversarial  teams  are  available and scheduled.  (PM) A12.3.1.  Testability  of  cyberspace requirements  are  determined  and  additional  clarification received as needed.  (PM, OTO) A12.3.2.  Applicability  of  network  defender  participation  in  adversarial  assessment  team OT&E events is determined.  (OTO) A12.3.3.  Limitations  of  generating  operational  effects  during  cybersecurity  adversarial assessment  OT&E  events  due  to  safety  and  real-world  operations  considerations  are identified and documented.  (OTO) A12.3.4.  Quantitative  cyber  resiliency  factors,  descriptors  and  tailored  measures  are identified.  (OTO) A12.3.5.  The threat basis on which vulnerability/penetration testing scenarios will be built is identified and documented in the test concept and scenarios.  (OTO) A12.3.6.  The  Test  Resource  Plan  (TRP)  includes  updated  resources  and  costs  associated with  cooperative  vulnerability,  penetration  assessment  and  adversarial  test  events.    (OTO, PM) A12.4.  Identify security constraints and their impacts on dedicated OT&E.  (PM, LDTO) A12.4.1.  Receipt  of  permissions  and  rules  of  engagement  before  DT&E  cooperative vulnerability, penetration assessment and adversarial events.  (PM, LDTO) A12.4.2.  Cyber  anti-tamper  testing  is  integrated  into  DT&E  and  OT&E  to  the  extent warranted and permissible. (PM, LDTO) A12.5.  System OPSEC plan is current.  (PM)   64  AFMAN63-119  19 FEBRUARY 2016 A12.6.  If  National  Security  Agency  (NSA)  certification  is  required  for  classified/controlled cryptographic  items,  the  program’s  security  verification  test  approach  must  be  included  in  the TEMP. (PM, ITT) _______________________ Primary References: DoDI 8500.01 DoDI 8500.2 DoDI 8510.01 CJCSI 6510.01 CNSSI 1253 NIST SP 800-53 NIST SP 800-57NIST SP 800-18 DoD Cybersecurity T&E Guidebook Defense Acquisition Guidebook Joint Capabilities Integration and Development System Manual  DAG, Chapter 7 AFI 99-103 and references cited in Atch 1  AFPD 63-17 DOT&E Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 Aug 2014, Atch E  AFMAN63-119  19 FEBRUARY 2016   65  Attachment 13 CONTRACTOR TESTING A13.1.  Ensure  all  system  specifications  and  contractor  requirements  support  the  latest  CBRD.  (PM) A13.2.  Ensure  comprehensive  contractor  test  plans  for  development,  qualification,  and production acceptance testing are in place.  (PM, C) A13.2.1.  Requirements  and  specifications  must  flow  down  accurately  and  clearly  from prime contractors to subcontractors.  (C) A13.2.2.  Contractor  test  strategies  and  methods  must  determine  if  all  aspects  of  the specification and the CBRD can be met.  (PM, LDTO) A13.2.3.  Test events should be performed with operationally relevant components/elements under  operationally  relevant  conditions  and  scenarios  as  much  as  possible  with  exceptions agreed to by all stakeholders and/or limitations cited.  (C) A13.2.4.  Sub-system and system pass/fail specification thresholds must be directly traceable to the most current CBRD.  (PM) A13.2.5.  A  realistic,  attainable,  event-driven  test  schedule  must  be  proposed  and  funded.  (C) A13.2.6.  Known risks are reasonably and appropriately managed.  (C) A13.2.7.  All contractor test data must be available in the system’s common T&E data base.  (PM) A13.2.8.  Ensure contractor testing is included in the ITC and described in the TEMP.  (PM)  (See A10, A11) A13.2.9.  Ensure  the  contractor  is  capable  to  plan  and  conduct  special  and  formal  multi-segment  and  system  of  system  testing,  including  test  support,  handling,  calibration,  and transportation.  (PM) A13.3.  Contractor  testing  must  demonstrate  the  system  and/or  components  are  meeting  the CTPs at prescribed threshold levels and within defined time frames at each step in development.  (C) A13.3.1.  Government  systems  engineering  analysis  should  determine  if  test  results  support achievement  of  the  specification  and  if  the  system  is  projected  to  meet  operational requirements.  (LDTO) A13.3.2.  Fault  tree  analysis  must  be  performed  on  the  operational  system  and  its  external and internal interfaces to identify potential operational contributors to mission failure.  (C) A13.4.  Available  government  facilities  are  used  in  contractor  testing  wherever  cost-effective, available, and feasible.  (PM) A13.5.  A deficiency resolution system must be in place and accessible to all test organizations to identify, track, and resolve test failures.  (PM, C)  (See A19)   66  AFMAN63-119  19 FEBRUARY 2016 A13.5.1.  The  contractor’s  DR  process  must  be  compatible  with  the  Government’s  DR process.  (PM)  (See A19) A13.5.2.  All  test  failures  and  resultant  system  design  changes  must  be  documented  and analyzed  for  effectiveness.    Tests  must  be  repeated  as  necessary  to  certify  specification compliance.  (C) A13.5.3.  Document  all  changes  to  specification  threshold  (pass/fail)  values  and  rationale.  (PM) A13.6.  Contractor  T&E  data  and  information  must  be  available  in  the  required  formats  for Government review for impacts on DT&E and dedicated OT&E.  (PM) A13.7.  Planned  contractor  testing  must  be  completed  according  to  the  contract  before government acceptance and dedicated OT&E.  (C) A13.7.1.  Contractor  testing  deferred  beyond  government  acceptance  of  the  system  is documented for final certification of system readiness.  (PM) _______________________ Primary References: AFI 99-103 TO 00-35D-54 DoDI 8500.2 Incorporating Test and Evaluation into Department of Defense Acquisition Contracts  AFMAN63-119  19 FEBRUARY 2016   67  Attachment 14 GOVERNMENT DEVELOPMENTAL TEST AND EVALUATION (DT&E) A14.1.  CBRD  requirements  must  be  accurately  reflected  in  Government  DT&E  plans  and  be demonstrated during contractor and government DT&E.  (PM) A14.2.  When  design-cost-performance requirements, user concurrence must be obtained and documented where appropriate.  (PM) trade-offs  are  made that  may impact  CBRD A14.3.  The DT&E schedule and testing must be planned and executed to allow sufficient time to certify  system  OT&E  readiness,  start  and  complete  dedicated  OT&E  before  FRP  or  fielding.  (PM) A14.3.1.  DT&E, with inputs from LDTO, must validate contractor testing is complete, or a plan exists to finish testing.  (PM)  (See A13) A14.3.2.  Sufficient suitability testing must be conducted to permit credible predictions about system Reliability, Maintainability, and Availability (RM&A).  (LDTO)  (See A8) A14.3.3.  All CTPs must demonstrate satisfactory performance, or be supported by reliability growth plans and/or curves that show threshold attainment.  (PM) A14.4.  A  government-run  DR  system  must  be  in  place  in  support  of  DT&E  and  OT&E  for identifying, tracking, reporting, and resolving DRs.  (PM)  (See A19) A14.4.1.  Correction  of  all  CAT  I  deficiencies  including  cybersecurity  vulnerabilities identified  during  DT  blue  team/red  team  events  are  implemented  before  start  of  dedicated OT&E.  (PM)  (See A19) A14.5.  A formal process is in place to control and track system configuration during DT&E that will support dedicated OT&E.  (PM)  (See A18) A14.5.1.  The  system  design  must  be  stabilized  sufficiently  early  with  no  major  changes implemented in the OT&E test articles.  (PM)  (See A19, A21, A18) A14.6.  Sufficient  operationally  relevant  DT&E  must  be  accomplished,  culminating  in  a  "dress rehearsal"  in  the  final  phase,  to  determine  if  CBRD  requirements  can  be  met  before  dedicated OT&E.  (LDTO) A14.6.1.  Cooperative  vulnerability  and  penetration  assessment,  and  adversarial  assessment  tests of cyber resiliency are complete.  (PM, OTO) A14.6.2.  Sufficient  testing  must  be  accomplished  with  other systems to  support  end-to-end cybersecurity and interoperability certifications.  (PM)  (See A9, A12) A14.6.3.  Required  levels  of  performance  must  be  demonstrated  in  the intended  operational environment based on the CBRD, Air Force concepts, strategies, and plans.  (PM)  (See A21) A14.6.4.  Sufficient  workarounds  acceptable  to  the  OTO  are  identified  for  CAT  II vulnerabilities and deficiencies.  (PM) A14.6.5.  If there are interoperability requirements, DT&E must take place at the system-of-systems level.  (PM)  (See A9)   68  AFMAN63-119  19 FEBRUARY 2016 A14.7.  LFT&E  results  (if  required)  must  be  available  before  start  of  dedicated  OT&E.    (PM)  (See A16) A14.8.  Formal  certifications  may  be  required  from  the  following  sources  (among  others).  Ensure clearances and certifications are available for use in dedicated OT&E.  (PM)  (See A23) A14.8.1.  Non-nuclear Munitions Safety Board A14.8.2.  Directed Energy Weapons Safety Board A14.8.3.  Flight Safety Board A14.8.4.  Airworthiness, Spaceflight Worthiness A14.8.5.  Range Safety A14.8.6.  Nuclear Weapons Center A14.8.7.  Institutional Review Board for Protection of Human Subjects in Testing A14.8.8.  SEEK EAGLE certification completed for threshold systems as a minimum A14.8.9.  AF Spectrum Management Office A14.8.10.  Authorization to Operate (ATO) A14.8.11.  Space and Orbital Safety A14.8.12.  Communication, Navigation, Surveillance / Air Traffic Management (CNS/ATM) Letter of Compliance A14.9.  For integrated testing, minimize duplication and voids in testing and the excessive use of facilities. (OTO)  (See A10, A11, A13, A22, A32) A14.9.1.  DT&E  data  formats  and  parameters  are  compatible  with  other  tests  to  maximize data availability in the common database and usability for OT&E.  (PM, OTO)  (See A11) A14.10.  An agreed-upon plan and rationale must exist (e.g., in the TEMP) for testing any areas or capabilities deferred past the start of dedicated OT&E.  (PM) A14.10.1.  If there are any incomplete test areas, explain why and give impacts on dedicated OT&E with inputs from the OTO.  (LDTO) A14.11.  Ensure  sufficient  interim  DT&E  results  and  evaluations  are  available  to  support certification of readiness for operational testing.  (LDTO)  (See A13) _______________________ Primary References: DoDI 5000.02 DAG, Enclosures 4 and 9 AFI 99-103 AFI 63-104  AFMAN63-119  19 FEBRUARY 2016   69  Attachment 15 SOFTWARE DEVELOPMENT AND MATURITY A15.1.  System  software  functionality,  performance,  and  maturity  must  be  assessed  throughout the  systems  engineering  technical  reviews  from  Systems  Readiness  Review  (SRR)  through OTRR and developmentally tested at the full system level (suitable for that increment) prior to starting dedicated OT&E.  (PM)  (See A21) A15.2.  Define  software-related  exit  criteria  for  MS  B.    These  criteria  may  be  modified  and/or criteria added/deleted in response to CBRD changes during system development.  (PM) A15.3.  Develop  and  implement  a  "requirements  traceability"  metric  to  measure  adherence  of software products (to include architecture, design, and code) to the CBRD.  (PM)  (See A4) A15.4.  Operational databases are complete and sufficient for operational test and contain actual operational data.  (PM) A15.5.  System  level  integration  testing  of  software  and  hardware-software-firmware  interfaces must be monitored, documented, and completed.  (PM)  (See A31) A15.6.  Effective software configuration management and control procedures are in place.  (PM)  (See A19) A15.7.  Software manuals and documentation must be validated and up-to-date with the current software baseline in support of dedicated OT&E.  (PM)  (See A31) A15.8.  Software  and  firmware  configurations  must  be  fully  documented  and  frozen  before starting  dedicated  OT&E.    Changes  must  not  be  implemented  during  dedicated  OT&E  that would impact the configuration being fielded or produced. (PM)  (See A18) A15.8.1.  Incrementally deployed software releases address specific capabilities and testable performance  requirements  and  must  be  assessed ready  for  test.    Each  release  must  undergo dedicated OT&E.  Software builds or increments that are not deployed individually (release) must still support full deployment system OT&E. A15.9.  The software must be stable (i.e., operate error free for a reasonable length of time prior to dedicated OT&E).  (PM)  (See A21) A15.10.  Facilities,  tools,  and  manpower  must  be  sufficiently  representative  to  support  the OT&E plan and schedule, and fielding of the software.  (PM)  (See A8, A25) A15.11.  Required  Software  Assurance  (SwA)  Defense  Information  Systems  Agency  (DISA) Security  Technical  Implementation  Guide  (STIG)  and  CNSSI  1253  controls  and  protection mechanisms  are  identified,  implemented,  and  tested  to  prevent  system  compromise,  maintain integrity and availability, and prevent unauthorized access to systems and data. (PM) A15.11.1.  Require the use of automated vulnerability analysis tools & techniques throughout the  lifecycle.    Determine  appropriate  remediation  strategies  for  all  identified  SwA vulnerabilities.  (PM) A15.12.  For  critical  software,  employ  independent  SwA  Verification  &  Validation  (V&V) organizations through DT. (PM)   70  AFMAN63-119  19 FEBRUARY 2016 A15.13.  Known  software  and  firmware  vulnerabilities,  exploitability  levels,  and  discrepancies affecting  system  performance  or  the  dedicated  OT&E  must  be  properly  documented  and appropriate corrective action(s) taken.  (PM)  (See A19) A15.13.1.  The  software  must  be  analyzed  for  safety  critical  functions  and  determined acceptable for operational use.  (PM) A15.14.  Sufficient regression testing must be accomplished at the unit, integration, and system-of-systems level to ensure changes do not introduce operationally critical faults and/or result in additional defects.  (PM) _______________________ Primary References: TO 00-35D-54 CJCSI 6212.01 DoDI 5200.01 AFI 16-1001 AFI 63-101/20-101 AFI 63-1201  AFMAN63-119  19 FEBRUARY 2016   71  Attachment 16 LIVE FIRE TEST AND EVALUATION (LFT&E) A16.1.  Review  the  most  current  threats  and  operational  scenarios  in  the  CBRD,  threat documents, Air Force concepts, and AoA to assess whether or not a "covered system”.  (PM) A16.1.1.  Consult AF/TEP, users, and OSD/DOT&E (in that order) for concurrence with the determination of covered system status.  (PM) A16.2.  If the system is a covered system, determine LFT&E scope and complete a cost-benefit analysis.  (PM) A16.3.  If  full-up  LFT&E  is  determined  to  be  cost-effective  and  practical,  develop  an  LFT&E strategy, to include the level of funding, and submit to AF/TE and subsequently to OSD/DOT&E for approval.  (PM) A16.3.1.  Describe the LFT&E strategy in the TEMP and submit individual plans for “full-up system level” LFT&E to AF/TE and subsequently to OSD/DOT&E.  (PM) A16.3.2.  Fully  integrate  the  LFT&E  strategy  and  plans  into  the  overall  strategy  for  T&E, TEMP, and integrated test plans.  (PM)  (See A10, A11) A16.3.3.  Plan for and fund LFT&E to be completed before start of dedicated OT&E.  (PM) A16.4.  If full-up LFT&E is determined not to be cost-effective and practical, prepare an LFT&E waiver  request  and  an  alternate  LFT&E  plan  for  the  decision  review  authority  or  PEO  and OSD/DOT&E approval before MS B.  (PM) A16.4.1.  Describe the alternate vulnerability/lethality strategy in the TEMP.  (PM) A16.4.2.  Plan  for  and  fund  “alternate”  LFT&E  to  be  completed  before  start  of  dedicated OT&E.  (PM) A16.5.  Deficiencies  identified  during  LFT&E  that  are  to  be  corrected  must  be  tracked  and retested prior to certification for dedicated OT&E.  (PM) A16.6.  Fully comply with all system-specific congressional direction regarding LFT&E.  (PM) A16.7.  With regard to threat systems for LFT&E: A16.7.1.  Threat  "shot  doctrine"  and  employment  tactics  must  reflect  the  contents  in  the CBRD, Air Force concepts, and threat documents.  (PM) A16.7.2.  Threat systems and threat models are VV&A’d before use in LFT&E.  (PM)  (See A18) A16.7.3.  Identify  limitations  in  the  test  threats  and  voids  in  covering  the  threat  spectrum.  Describe proposed fixes.  (PM) A16.7.4.  Where  limitations  exist  in  test  threat  systems,  obtain  approval  to  fill  gaps  with M&S and alternative systems.  (PM)   72  AFMAN63-119  19 FEBRUARY 2016 A16.8.  Develop  a  data  reduction  and  common  database  for  using  all  validated  threat  test  data throughout the integrated test plan.  (PM) _______________________ Primary References: DoDI 5000.02, Enclosure 5 DAG, Chapter 9 AFI 99-103  AFMAN63-119  19 FEBRUARY 2016   73  Attachment 17 MODELING AND SIMULATION (M&S) A17.1.  Ensure  M&S  requirements  are  identified  in  CBRDs  to  obtain  funding  and  support  for their development or reuse.  (User)  (See A4) A17.2.  Develop a Modeling and Simulation Support Plan (MSSP) that links M&S requirements to the capabilities being developed and tested throughout the program (from the AoA through the MS  C  decision).    The  MSSP  can  be  part  of  existing  program,  engineering  or  technical  plans.  (PM) A17.2.1.  Identify  as  early  as  possible  the  M&S  support  requirements,  to  include  funding, over the entire system life cycle.  (PM) A17.2.2.  The  MSSP  must  address  continuing  ownership  and  maintenance  of  M&S  assets after system fielding.  (PM) A17.2.3.  Identify M&S linkages with planned interfacing and interoperable systems.  (PM) A17.2.4.  Check  for  archived  M&S  tools  (e.g.,  with  Air  Force  Modeling  and  Simulation Resource Repository (AFMSRR)) before building new M&S resources.  (PM) A17.2.5.  Ensure programs obtain data and models for M&S from the required authoritative sources when available and feasible. A17.3.  Ensure M&S assets, test tools, and analysis tools will be available and usable for T&E as required.  Testers must receive adequate training as required.  (PM) A17.4.  Ensure  M&S  V&V  plan  and  comprehensive  schedule  supports  the  integrated  test  plan and the dedicated OT&E plan and schedule.  (PM) A17.4.1.  Scenarios,  test  tools,  and  analysis  tools  required  for  DT&E  must  be  adequately documented.  (PM) A17.4.2.  The  design  engineering  data  must  be  reviewed.    Physics  models  can  be  V&V'd, whereas operations analyses are subjectively V&V'd.  Empirical test data should be used to establish model credibility.  (PM) A17.4.3.  Any M&S used to support dedicated OT&E must be accredited.  (OTO) A17.5.  If  M&S  will  generate  results  used  to  support  a  deployment  and/or  FRP  decision  on  an OSD T&E Oversight program, OSD/DOT&E must approve its use in dedicated OT&E.  (OTO) _______________________ Primary References: DoDI 5200.40 AFI 14-206 AFI 16-1001 AFI 63-101/20-101    74  AFMAN63-119  19 FEBRUARY 2016 Attachment 18 CONFIGURATION MANAGEMENT A18.1.  Configuration management is an established element of a program’s systems engineering process.  (PM) A18.1.1.  The  systems  engineering  process  must  be  used  for  all  system  components  and support  items  (e.g.,  hardware,  software,  support  equipment,  spares,  Government  Furnished Equipment (GFE)).  (PM)  (See A20) A18.2.  A  configuration  control  mechanism  must  be  used  to  ensure  the  orderly  transition  from one decision review to the next, and from development to production.  (PM) A18.2.1.  The Government must have sufficient control or oversight over the configuration to ensure changes do not invalidate the results of DT&E or dedicated OT&E.  (PM)  (See A20) A18.2.2.  The exact system configuration must be traceable throughout the program.  (PM) A18.3.  If  known  deficiencies  remain  in  test  articles  before  start  of  dedicated  OT&E,  the  SEP must describe strategies for managing the following areas: (See A19, A20) A18.3.1.  System  form,  fit,  and  function  must  not  be  adversely  affected  as  a  result  of  each deficiency correction.  (PM) A18.3.2.  The impacts of fixing before versus after dedicated OT&E must be assessed.  (PM) A18.3.3.  All changes are documented and under configuration control.  (PM) A18.4.  The  system  configuration  and  configuration  of  interfacing  systems  must  be  stable  and production representative before the start of dedicated OT&E.  (PM)  (See A20, A15) _______________________ Primary References: AFI 21-102 AFI 63-131  AFMAN63-119  19 FEBRUARY 2016   75  DEFICIENCY IDENTIFICATION AND RESOLUTION PROCESSES Attachment 19 A19.1.  A  contractor-operated  DR  process,  if  established,  will  augment  the  Joint  Deficiency Reporting System (JDRS) process.  (C) A19.2.  JDRS  incorporated  and  open  to  all  stakeholders  for  promptly  identifying,  reporting, tracking, and resolving system deficiencies.  (PM) A19.3.  A  Material  Improvement  Project  Review  Board  (MIPRB)  must  ensure  resolution  of  all DRs and list the impacts to dedicated OT&E.  (PM) A19.4.  A Deficiency Review Board (DRB) will periodically review, validate, and prioritize all open DRs.  (PM) A19.4.1.  DRs should be rank-ordered, and the most critical worked first or as agreed by the user(s), operational tester, and LDTO.  (PM) A19.5.  Open  DRs  from  DT&E  must  not  preclude  successful  conduct  of  dedicated  operational testing and the achievement of operational requirements.  (PM) A19.5.1.  Dedicated  operational  test  results  will  not  be  invalidated  due  to  deferred  DR resolution.  (PM) A19.5.2.  The DR analysis process must be complete and coordinated with users and testers prior to the start of dedicated OT&E.  (PM) A19.6.  Known  DRs  or  capabilities  deferred  beyond  the  start  of  dedicated  OT&E  must  be reviewed and prioritized by a T&E DRB and an impact analysis performed.  (PM) A19.6.1.  Category  I  DRs  must  be  fixed  and  closure  verified  according  to  an  agreed  upon plan.  (PM) A19.6.2.  Category  II  DRs  must  be  fixed  and  closure  verified,  or  suitable  work-arounds provided.  (PM) A19.7.  For  DRs  that  cannot  be  resolved  prior  to  start  of  dedicated  OT&E,  a  plan  exists  for testing deferred capabilities and fixes after dedicated OT&E is accomplished.  (PM) A19.7.1.  The  plan  addresses  how  open  DRs  are  tracked  from  increment  to  increment  after OT&E is complete.  (PM) A19.8.  A  Joint  Reliability  and  Maintainability  Evaluation  Team  (JRMET)  and  a  Test  Data Scoring  Board  (TDSB)  must  be  established  to  review  all  Reliability,  Availability,  and Maintainability (RAM) data.  (ITT)   76  AFMAN63-119  19 FEBRUARY 2016 A19.9.  Plan  of  Action  and  Milestones  (POA&M)  shows  how  cybersecurity  DRs  and vulnerabilities will be resolved.  (PM) Note:  Review the phases shown in Figure 2.3 to ensure currency of the strategies for requirements, acquisition, T&E, and cybersecurity.  (See A12) _______________________ Primary References: AFI 63-501 TO 00-35D-54 AFI 99-103 DoDI 8500.01 DoDI 8500.2 DoDI 8580.1 DoDI 8510.01 DoDI 4630.8 AFI 99-103 AFI 31-401 AFI 33-200 AFI 33-210  AFMAN63-119  19 FEBRUARY 2016   77  Attachment 20 PRODUCTION REPRESENTATIVE TEST ARTICLES A20.1.  Test  articles  (to  include  support  equipment,  software,  GFE)  must  be  as  production-representative as possible to support the dedicated OT&E and schedule.  (PM) A20.1.1.  Sufficient quantities of test articles must be available for dedicated OT&E.  (PM) A20.1.2.  Test articles must have achieved stabilized performance.  (PM) A20.1.3.  Test article requirements are provided as early as possible to ensure sufficient lead time for procurement.  (OTO) A20.2.  Assess any configuration differences between pre-production and production test articles and the expected impact on the validity of dedicated OT&E.  (OTO) A20.3.  Other  systems  and  subsystems  required  to  interoperate  with  the  test  articles  (including external  systems)  must  be  available,  and  of  correct  configuration,  to  permit  testing  in  an operationally realistic manner.  (OTO)  (See A10) A20.3.1.  These  systems  must  be  production  representative  or  as  close  to  production representative as possible.  (PM) A20.3.2.  A process must be in place to manage interoperability with other required systems and subsystems.  (PM)  (See A9, A18) A20.3.3.  Embedded test instrumentation must be transparent to system performance and test execution.  (PM) _______________________ Primary References: DoDI 5000.02, Enclosures 4 and 5 DAG, Chapters 4 and 9    78  AFMAN63-119  19 FEBRUARY 2016 Attachment 21 SYSTEM PERFORMANCE A21.1.  At  the  conclusion  of  OT,  the  system  must  demonstrate  it  is  capable  of  meeting  the CBRD’s  requirements  (i.e.,  is  operationally  effective  and  suitable)  in  its  intended  operational environment using operationally relevant scenarios.  (PM) A21.1.1.  MOEs, MOSs, MOPs, thresholds, objectives, and other test criteria in CBRDs must be reviewed.  (PM) A21.1.2.  The  system  must  demonstrate  it  will  meet  criteria  for  FRP  and/or  deployment decision.  (PM) A21.2.  System DT&E must demonstrate known deficiencies are identified  and corrected, fixes verified, or otherwise resolved or deferred.  (PM)  (See A19) A21.2.1.  Any  remaining  problem  areas  must  be  characterized  and  have  minimal  to  no impact on the outcome of dedicated OT&E.  (PM) A21.2.2.  All CTPs must demonstrate satisfactory performance, or be supported by reliability growth plans and/or curves from the SEP that show suitability threshold attainment.  (LDTO) A21.3.  System  integration  problems  must  be  corrected  to  allow  operators  to  satisfy  mission requirements.  The system must be ready for system- or mission-level testing.  (PM) A21.3.1.  Integration  among  system  components,  subsystems,  and  external  systems  must optimize total system design and performance capabilities.  (PM) A21.4.  If  the  system  was  planned  with  an  incremental  acquisition  strategy,  describe  what capabilities are lacking at this time and when they will be implemented.  (PM) A21.4.1.  Incremental  acquisition  strategy  will  include  impacts  to  existing  system  as additional capabilities are incorporated. A21.5.  LFT&E  (if  required)  must  be  complete  and  achieve  required  (acceptable)  levels  of system survivability or lethality.  (PM)  (See A16) A21.6.  Review  results  of  any  EOA  or  OAs  accomplished  and  ensure  known  deficiencies  are identified and corrected, fixes verified, or otherwise resolved or deferred. ______________________ Primary References: DoDI 5000.02, Enclosure 4  AFI 99-103 DAG, Chapter 9  AFMAN63-119  19 FEBRUARY 2016   79  Attachment 22 OPERATIONAL TEST AND EVALUATION PLAN A22.1.  The OT&E test concept (if required) must be developed and briefed as early as feasible (but not later than 180 days before start of dedicated OT&E).  (OTO)  (See A11) A22.1.1.  The OT&E test concept must be based on the characteristics of the operations and support  environments,  test  equipment  (to  understand  limitations  or  exceptions),  and  test scenarios the system will encounter in dedicated OT&E.  (OTO) A22.1.2.  The OT&E concept and OT&E plan must be developed from the strategies in the  LSC and other Air Force concepts.  (OTO)  (See A7, A8) A22.2.  The  dedicated  OT&E  plan  must  be  developed,  coordinated,  and  approved  as  early  as feasible.  If on OSD OT&E Oversight, OSD/DOT&E must approve the adequacy of the test plan NLT 60 days prior to dedicated OT&E start.  (OTO) A22.3.  A dedicated phase of rigorous, operationally realistic OT&E must be planned.  (OTO) A22.3.1.  Sufficiently  realistic  testing,  to  include  realistic  scenarios,  must  emulate  expected combat and peacetime environments.  (OTO) A22.3.2.  COIs,  MOEs,  and  MOSs  must  have  clearly  defined  linkages  to  the  CBRD,  AoA, and threat documents and be summarized in the TEMP.  (OTO) A22.3.3.  The  elements  of  operational  suitability  and  all  logistics  support  elements  must  be addressed.  (OTO)  (See A8, A25, A26, A28, A31, A32) A22.3.4.  Open requirements, and MOEs) must be resolved prior to OT&E start.  (OTO) issues  and  disconnects (e.g.,  with test  methodologies,  databases, A22.3.5.  Definitions,  formulas,  models,  scenarios,  and  evaluation  criteria  must  be standardized  as  much  as  possible  between  all  test  plans  for  the  system.    (OTO)    (See  A11, A14) A22.3.6.  M&S assets planned for dedicated OT&E should be as consistent as possible with the M&S assets used for the AoA and DT&E.  (OTO)  (See A3, A14, 17) A22.3.7.  The OT&E plan must be coordinated with the other Service OTAs as required and their inputs integrated into the lead OTO plan.  (OTO) A22.3.8.  Ensure  OT  plan  addresses  DOT&E  cybersecurity  content:    TEMP  linkage, architecture,  operational  environment,  evaluation  structure,  time  and  resources,  cooperative vulnerability  and  penetration  assessment,  and  adversarial  assessment.    Ref  DOT&E  Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 Aug 2014, Atch E.  (OTO) A22.4.  All T&E resources (e.g., M&S support, test articles, training, fault analysis, test facilities and  ranges,  contracting,  cyber  cooperative  vulnerability/penetration  assessment  and  adversarial assessment teams, network defenders) must be identified and scheduled.  (OTO)  (See A12, A23, A25, A26, A27, A28, A30, A31, A32) A22.5.  OT&E test plans must be integrated (e.g., capitalize on the activities and data from other tests) as much as practical while ensuring the following: (ITT)  (See A10, A11)   80  AFMAN63-119  19 FEBRUARY 2016 A22.5.1.  DT&E  and  dedicated  OT&E  test  objectives  are  not  compromised  as  a  result  of integrated testing.  (ITT, LDTO/OTO) A22.5.2.  DT&E  and  other  test  data  can  supplement  dedicated  OT&E  as  much  as  possible.  (LDTO) A22.5.3.  Test item configurations are rigorously controlled and operationally representative.  (PM)  (See A18) A22.5.4.  The  operational  test  plan  is  coordinated  with  other  test  organizations  to  ensure duplication and voids in testing are minimized.  (OTO) A22.5.5.  A  prudent  number  of  backup  resources  (e.g.,  test  assets,  funds)  are  available  to supplement dedicated OT&E if planned integrated test data is unusable or unavailable.  (PM) A22.5.6.  A plan exists for dry running test procedures.  (OTO) A22.6.  All OT&E limitations are described (e.g., lack of test articles, time, system capabilities, insufficient operational realism) that may impact the FRP or deployment decision.  (OTO) A22.6.1.  Describe how these limitations (and any waivers) will be addressed in subsequent increments, FOT&E, FDE, and beyond.  (OTO) A22.6.2.  State  the  expiration  dates  of  the  current  authorization  to  operate  (ATO)  including the current risk assessment and number of open liens.  (OTO) A22.7.  Threat  "shot  doctrine"  and  employment  tactics  must  accurately  reflect  Air  Force concepts and CBRD.  (User) A22.7.1.  The  PM  V&V’s  threat  systems  and  M&S  assets,  and  the  OTO  accredits  them before use in dedicated OT&E.  (PM, OTO)  (See A17) Note:  The term OT&E includes IOT&E, FOT&E, Multi-Service Operational Test and Evaluation (MOT&E), Qualification Operational Test and Evaluation (QOT&E), FDE, and OUE as defined in AFI 99-103.   _______________________ Primary References: DoDI 5000.02, Enclosures 4 and 5 DAG, Chapter 9 AFI 99-103 DOT&E Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 Aug 2014, Atch E DASD(AT&L) Memo, Document Streamlining—Program Protection Plan (PPP), 18 Jul 2011  AFMAN63-119  19 FEBRUARY 2016   81  INTEGRATED TECHNICAL, HEALTH, AND SAFETY REVIEWS Attachment 23 A23.1.  The system must be capable of being operated and maintained in its intended operational environment  during  dedicated  OT  with  an  acceptable  level  of  Environment,  Safety,  and Occupational Health (ESOH) risks.  (PM) A23.2.  All ESOH and Human Systems Integration (HSI) hazards with an assessed mishap risk level  of  “Serious”  or  “High”  must  be  mitigated  to  an  acceptable  level  and  a  safety  release provided to the OTO before start of dedicated OT.  (PM) A23.2.1.  The  LSC  and  other  Air  Force  concepts  must  be  reviewed  and  ESOH  constraints and limitations resolved.  (PM) A23.2.2.  All system-related ESOH risks have been assessed and accepted at the appropriate management  level  prior  to  exposing  people,  equipment,  or  the  environment  to  known hazards.  (PM) A23.2.3.  The  safety  release  must  transmit  system  ESOH  hazard  data  to  the  operators, maintainers, and testers.  (PM) A23.2.4.  Environmental  impacts  must  be  identified  and  mitigated  or  eliminated  consistent within cost, schedule, and technical performance considerations.  Data about environmental hazards must be provided to the OTO to support analyses in compliance with NEPA.  (PM) A23.2.5.  HSI efforts must be identified and mitigated to the extent possible to minimize the risks to illness, disability, death or injury to operators and maintainers. A23.3.  Verified  preliminary  TOs  and  technical/procedural  manuals  that  identify  ESOH  risks with  mitigation  measures  must  be  available  to  support  the  dedicated  OT  plan  and  schedule.  (PM)  (See A31) A23.4.  Operator  and  maintenance  personnel  must  have  ESOH  training  completed  in  time  to support the OT&E plan and schedule.  (OTA)  (See A23) A23.5.  Formal certifications may be required from the following sources (among others):  (PM) A23.5.1.  Non-nuclear Munitions Safety Board A23.5.2.  Directed Energy Weapons Safety Board A23.5.3.  Flight Safety Board A23.5.4.  Airworthiness, Spaceflight Worthiness A23.5.5.  Range Safety A23.5.6.  Nuclear Weapons Center A23.5.7.  Institutional Review Board for Protection of Human Subjects in Testing A23.5.8.  SEEK EAGLE A23.5.9.  AF Spectrum Management Office A23.5.10.  ATO   82  AFMAN63-119  19 FEBRUARY 2016 A23.5.11.  Space and Orbital Safety A23.5.12.  Communication, Navigation, Surveillance / Air Traffic Management (CNS/ATM) Letter of Compliance A23.6.  Obtain operational flight clearances  or waivers for systems requiring release or  jettison from aircraft.  (OTO)  (See A14) A23.7.  OTOs must independently examine technical and safety risks involving USAF personnel and property prior to test.  Utilize all available data to include the Safety Release and technical data provided by the PM.  Safety reviews should be accomplished after the technical review to ensure  that  all  test  unique  hazards  are  identified  and  managed  IAW  test  design  and  planned execution.  Rapidly acquired capabilities may result in little/no prior DT hazard analysis or data to  assess  OT  test  hazards;  coordinate  with  the  requesting  MAJCOM  and  OT&E  Certification Official for risk management and/or additional DT analysis. (OTO) A23.8.  Ensure  systems  engineering  principles,  processes,  and  practices  are  properly  applied throughout the system life cycle.  (PM) _______________________ Primary References: DoDI 5000.02, Enclosures 3, 4, 5, 7 AFI 63-101/20-101 DAG MIL-STD-882E AFI 32-7061 AFI 32-7086 AFI 40-402 AFI 62-601 AFI 63-125 AFI 91-202 AFI 91-204 AFI 90-802  AFMAN63-119  19 FEBRUARY 2016   83  Attachment 24 OPERATIONAL TEST TEAM TRAINING A24.1.  OT&E  test  team  training  requirements  and  assets  are  identified  early  and  in  sufficient detail.  (OTO) A24.1.1.  For multi-Service and multi-national systems, any additional training requirements are identified.  (OTO) A24.2.  Required  training  must  be  adequately  contracted  for,  funded,  and  scheduled  to  ensure completion when required in the OT&E plan and schedule.  (PM)  (See A24) A24.2.1.  OT&E test team personnel must be proficiently trained in required T&E procedures and/or operational skills (i.e., Type 1 training) before the start of dedicated OT&E.  (OTO) A24.2.2.  Training  must  include  normal  and  abnormal/emergency  operations  to  operate  and maintain the system(s) according to the LSC and other Air Force concepts.  (PM) A24.2.3.  Operator and maintenance personnel must be fully trained in Tactics, Techniques, and Procedures (TTP), CONOPS, Concept of Employment (CONEMP) for SUT.  (OTO) A24.3.  Dry run test procedures before start of dedicated OT&E.  (OTO) A24.4.  Operator  and  maintenance  personnel  must  have  ESOH  training  completed  in  time  to support the OT&E plan and schedule.  (OTO) _______________________ Primary References: AFI 36-2201, V2    84  AFMAN63-119  19 FEBRUARY 2016 Attachment 25 SUPPORT EQUIPMENT (SE) A25.1.  Peculiar, common, and unique SE must be identified as early as feasible.  (PM) A25.2.  Peculiar  SE  and  its  required  support  (e.g.,  technical  data,  spares)  must  meet  the maintenance times and capabilities stated in the CBRD.  (PM)  (See A21, A26) A25.2.1.  Peculiar SE must be available in required quantities to support the OT&E plan and schedule.  Peculiar  SE  should  also  be  made  available  to  the  ITT  during  DT&E  so  that  any deficiencies can be identified for possible resolution prior to OT&E.  (PM) A25.2.2.  Peculiar  software  SE  and  its  supporting  technical  data,  compilers,  manuals,  etc., must be available if the Government maintains the software.  (PM) A25.3.  Peculiar SE must be in production representative configurations and fully interoperable and compatible with the system(s) it supports.  (PM)  (See A20) A25.3.1.  Assess  any  configuration  differences  between  preproduction  and  production peculiar SE and the expected impact on the validity of dedicated OT&E.  (PM)  (See A18) A25.3.2.  The  Government  must  have  positive  control  or  oversight  over  SE  configurations.  (PM)  (See A18) A25.4.  Common SE and unique SE must be identified and available in the required quantities to support the OT&E plan and schedule.  (User) A25.5.  SE training must be accomplished or scheduled to support the OT&E plan and schedule.  (PM)  (See A23, A29) A25.6.  Full mission simulators and trainers (e.g., flight simulators) are available at appropriate times  and  locations  for  test  team  training  and  evaluation  in  OT&E.    Full  mission  simulators should also be made available to the ITT during DT&E so that any deficiencies can be identified for possible resolution prior to OT&E.  (PM) _______________________ Primary References: AFI 99-103  AFI 36-2251 AFI 63-101/20-101  AFMAN63-119  19 FEBRUARY 2016   85  Attachment 26 SUFFICIENCY OF SPARES A26.1.  Sufficient  spares  must  be  available  to  support  test  assets,  test  scenarios,  and  SE according to the OT&E plan and schedule.  Support levels must be based on the total number of expected  operational  test  events  and  hours.    Sufficient  spares  should  be  made  available  to  the ITT during DT&E so that the schedule can support the entry into OT&E.  (PM)  (See A8) A26.2.  Spares  repair  procedures  and  capabilities  (for  blue  suit  and/or  Contractor  Logistics Support (CLS)) must be in place to support the OT&E plan and schedule.  (PM)  (See A30) A26.3.  Provision must  be made  for timely failure confirmation and repair action reports to the OT&E test team.  (PM)  (See A30, A31) A26.4.  The  management  concepts  for  primary  operating  stocks,  war  readiness  spares  support, and for battle damage repair must be estimated prior to OT&E plan development.  (PM) A26.4.1.  Candidate spares for two-level maintenance must be identified.  (PM) A26.4.2.  Spare  levels  for  Mobility  Readiness  Spares  Package  (MRSP)  and  Battle  Damage Repair Spares Kit (BDRSK) must be identified if required.  (User) A26.5.  A logistics support plan must be developed that accurately reflects the LSC and other Air Force concepts.  (PM)  (See A8) A26.5.1.  Identify the risks and limitations in the spares that support dedicated OT&E.  For spares with limited availability, define how quickly they must be replenished.  (PM) A26.5.2.  The  projected  number  of  spares  and  rates  of  replenishment  must  support  the  ops tempo of the dedicated OT&E.  (PM) _____________________ Primary References: AFI 63-101/20-101    86  AFMAN63-119  19 FEBRUARY 2016 Attachment 27 SUPPORT AGREEMENTS A27.1.  Memorandums  of  Understanding  (MOU)  and  Memorandums  of  Agreement  (MOA) should  establish  the  availability  of  test  and  support  resources  needed  for  the  OT&E  plan  and schedule.  (OTO) A27.1.1.  For  multi-Service  testing,  comply  with  the  terms  of  the  "MOA  on  Multi-Service Operational  Test  and  Evaluation  (MOT&E)  and  Operational  Suitability  Terminology  and Definitions."  (OTO) A27.2.  Interagency  support  agreements  should  be  established  for  using  required  ranges,  test facilities,  airspace,  frequencies,  etc.,  and  base  support  functions  such  as  supply,  transportation, and billeting.  (OTO) A27.3.  Support  agreements  should  be  established  with  other  Government  agencies  for  such functions as data processing, failure analysis, communications, and security.  (OTO) A27.3.1.  Obtain  agreements  for  testing  interoperability,  cybersecurity,  network  risk assessments, etc.  (ITT)  (See A9, A10, A13) A27.4.  Potential for conflict of interest must be strictly avoided, mitigated, or neutralized before any contractor is allowed to participate in the support of dedicated OT&E.  (OTO) (See A30) _______________________ Primary References: AFI 25-201  AFMAN63-119  19 FEBRUARY 2016   87  PACKAGING, HANDLING, STORAGE AND TRANSPORTATION Attachment 28 A28.1.  Shipping  containers,  packaging,  handling,  storage,  and  transportation  components  and methods must be fully qualified and must meet the CBRD’s requirements.  (PM) A28.1.1.  Operationally relevant maintenance demonstrations and scenarios must be used as specified in the LCSP.  (PM) A28.2.  Adequate  numbers  of  production  representative  shipping  containers,  packaging, handling,  and  transportation  vehicles  must  be  used  to  transport  test  articles  to  the  dedicated OT&E sites.  (PM) A28.3.  Formal  or  preliminary  technical  data  must  be  verified  and  available  to  support  the dedicated OT&E plan and schedule.  (PM)  (See A31) A28.4.  Shipping, transportation, receiving, and storage arrangements must be in place with the contractor and host base transportation offices to ensure timely shipping, receiving, and resource protection of test and support assets.  (OTO) A28.5.  OT&E test team maintenance personnel must be adequately trained.  (PM)  (See A24) ______________________ Primary References: AFI 63-101/20-101 DoD Manual 4140.01 DoD Manual 4140.01-M    88  AFMAN63-119  19 FEBRUARY 2016 Attachment 29 PERSONNEL A29.1.  Identify OT&E test team personnel requirements, including software maintenance skills, required  certifications  and  security  clearances.    The  number  of  personnel  and  skill  levels  must reflect typical military users in the operational environment.  (OTO) A29.2.  Written  agreements  must  be  in  place  establishing  the  sources  for  required  personnel.  (OTO)  (See A11, A28) A29.3.  Estimates of maintenance requirements (in terms of man hours and personnel) for LRUs, subsystems, and the full system must be available.  (PM)  (See A8, A25) A29.4.  Contractor support  (Interim  Contractor  Support  (ICS)  and  CLS)  must  be  identified  and consistent with operational concept in accordance with Title 10.  (PM)  (See A30) A29.5.  Required  training,  including  Type  I  and/or  Type  IV  training,  must  be  completed  or scheduled for completion to support the dedicated OT&E plan and schedule.  (PM)  (See A24)  AFMAN63-119  19 FEBRUARY 2016   89  Attachment 30 CONTRACTOR SUPPORT A30.1.  All  contractor  assistance  or  services  required  to  support  dedicated  OT&E  must  be identified  in  the  OT&E  test  plan,  TEMP,  Request  for  Proposal  (RFP),  and  Statement  of  Work (SOW).  (OTO) A30.1.1.  The  potential  for  conflict  of  interest  must  be  strictly  avoided,  mitigated,  or neutralized  before  any  “system  contractor”  is  allowed  to  participate  in  the  support  of dedicated  OT&E.    Contact  the  Contracting  Officer  immediately  if  there  is  a  potential  for conflict of interest.  (OTO) A30.2.  OSD  approval  must  be  obtained  for  the  following  types  of  “system  contractor” involvement in dedicated OT&E:  (OTO) A30.2.1.  Contractor maintenance and support actions may be of the same type that  will be performed as part of ICS or CLS after the system is deployed.  (OTO) A30.2.2.  Contractor conduct and reporting of failure analyses to assist in isolating causes of test failures.  (OTO) A30.2.3.  Contractor  provision  of  system-unique  test  equipment,  test  beds,  test  facilities, instrumentation, data collection, and analysis.  (OTO) A30.2.4.  Contractor logistics support and training (Type I) if such services have not yet been developed and are not available from Government sources.  (OTO) A30.3.  “System  contractor”  report  generation  procedures  must  be  established  for  depot-level repair and maintenance actions.  (PM)  (See A19, A13) A30.4.  Support  contractor  services  must  be  established  for  any  required  data  collection, reduction,  and  analysis  capabilities  needed  in  dedicated  OT&E  that  are  not  performed  by  the Government.  (OTO)  (See A10, A11, A22, A27) _______________________ Primary References: Title 10 §2399 AFI 99-103 AFI 25-201 Incorporating Test and Evaluation into Department of Defense Acquisition Contracts    90  AFMAN63-119  19 FEBRUARY 2016 Attachment 31 TECHNICAL DATA A31.1.  Operator and maintainer technical data (i.e., TOs, engineering drawings, specifications, standards, process and user manuals, technical reports, catalog items) are available to support the OT&E plan and schedule.  Technical data should be made available to the ITT during DT&E so that any deficiencies can be identified for possible resolution prior to OT&E.  (PM) A31.1.1.  All  technical  data  are  managed  according  to  a  Configuration  Management  Plan (CMP).  (PM)  (See A18) A31.1.2.  Technical  data  from  other  interoperable  systems  must  be  available  to  support  the OT&E plan.  (User)  (See A11) A31.1.3.  Technical  data  required  to  evaluate  system  suitability  and  software  supportability must be available.  (User) A31.1.4.  Sufficient  information  is  provided  for  successfully  operating  and  maintaining  the system.  (User) A31.2.  Formal  or  verified  preliminary  TOs  and  technical  data  must  be  available  for  use  in dedicated OT&E.  (PM) A31.3.  A  Technical  Order  Management  Agency  (TOMA)  must  be  in  place  to  manage  TO deliveries, changes, and other TO requirements.  (PM) A31.3.1.  Procedures  must  be  established  to  process  changes  to  technical  data  and  TOs.  (PM) _______________________ Primary References: AFI 21-303 TO 00-5-1  TO 00-5-3  AFMAN63-119  19 FEBRUARY 2016   91  Attachment 32 TEST AND EVALUATION RESOURCES A32.1.  T&E  infrastructure  shortfalls  are  identified  in  draft  and  current  versions  of  the  TEMP.  HQ USAF/TE is informed of shortfalls.  (PM) A32.1.1.  Sufficient  resources  and  funding  are  available  to  start  and  sustain  the  planned OT&E program.  (OTO) A32.2.  Test  ranges  and  facilities  are  properly  equipped,  manned,  funded,  scheduled,  and personnel briefed before start of dedicated OT&E.  (OTO) A32.2.1.  Ensure cyber test infrastructure (with appropriate architecture, level of realism, and security) and documentation is available and described in the TEMP.  (PM)  (See A13) A32.3.  Realistic  targets  (or  V&V’d  simulators)  must  be  in  the  most  current  operational configuration(s) and available in sufficient quantities.  (PM)  (See A17, A18) A32.3.1.  Ensure  test  threat  systems  and  related  support,  including  countermeasures  are identified and programmed as early as possible.  (OTO) A32.3.2.  Sufficient  threat  densities,  either  in  open-air  or  indoor  facilities,  must  rigorously stress the system in as realistic a combat environment as possible.  (OTO)  (See A16) A32.3.3.  Validated cyberspace threats are emulated/employed to the extent possible to create a  cyber-contested  environment  during  adversarial  penetration  and  exploitation  testing.  (OTO) A32.4.  Adequate test instrumentation and data reduction capabilities must be identified, funded, scheduled, and support agreements negotiated on utilization rates and data requirements.  (OTO) A32.5.  M&S assets (including simulators, test drivers, and scenarios) are accredited, scheduled, and available to support the DT&E and OT&E plans and schedules.  (OTO) A32.6.  An  Environmental  Impact  Study  or  assessment  (if  required)  addressing  Federal,  State, Air Force, and local regulations must be completed and approved or waivers granted.  (PM)  (See A23) A32.7.  Other  systems  and  subsystems  required  to  interoperate  with  the  test  articles,  including external systems are available.  (OTO)  (See A9) _______________________ Primary References: DoD 7000.14-R, Vol 2A DoD Cybersecurity Test and Evaluation Guidebook AFI 65-601, Chapter 14 AFI 99-109 AFI 99-103  