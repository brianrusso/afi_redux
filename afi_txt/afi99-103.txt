 BY ORDER OF THE  SECRETARY OF THE AIR FORCE AIR FORCE INSTRUCTION 99-103 6 APRIL 2017 Test and Evaluation CAPABILITIES-BASED TEST AND EVALUATION     COMPLIANCE WITH THIS PUBLICATION IS MANDATORY ACCESSIBILITY:   Publications and forms are available for downloading or ordering on the e-Publishing website at www.e-Publishing.af.mil RELEASABILITY:  There are no releasability restrictions on this publication.  OPR:  AF/TEP  Supersedes:  AFI99-103, 16 October 2013  Certified by: AF/TEP  (Col Hans Miller) Pages: 113 This  publication  implements  Air  Force  Policy  Directive  (AFPD)  99-1,  Test  and  Evaluation Process.   It  describes the planning,  conduct,  and reporting of  cost  effective test and evaluation (T&E) programs as an efficient continuum of integrated testing throughout the system life cycle.  This  AFI  implements  the  policies  in  Department  of  Defense  Directive  (DoDD)  5000.01,  The Defense  Acquisition  System,  and  DoD  Instruction  (DoDI)  5000.02,  Operation  of  the  Defense Acquisition System (collectively called the DoD 5000-series); and Chairman of the Joint Chiefs of  Staff  (JCS)  Instruction  (CJCSI)  3170.01,  Joint  Capabilities  Integration  and  Development System.    This  AFI  must  be  used  in  conjunction  with  AFPD  10-6,  Capability  Requirements Development  (and  the  associated  AF/A5R  Requirements  Development  Guidebooks),  AFI  63-101/20-101,  Integrated  Life  Cycle  Management,  AFI  17-101,  Risk  Management  Framework (RMF)  for  Air  Force  Information  Technology  (IT),  DoDI  8330.01,  Interoperability  of  IT, including  NSS,  21  May  2014;  and  AFGM2015-33-03  (and  AFI  that  will  replace  it) Interoperability & Supportability of Air Force (IT/NSS), 23 July 2015.  The Defense Acquisition Guidebook  (DAG)  contains  non-mandatory  guidance.    This  instruction  applies  to  all  Air  Force organizations,  including  the  Air  National  Guard,  Major  Commands  (MAJCOM),  Direct Reporting Units (DRU), and Field Operating Agencies (FOA).  This instruction applies to all Air Force  acquisition  projects  and  programs  regardless  of  acquisition  category  (ACAT).    The authorities  to  waive  wing/unit  level  requirements  in  this  publication  are  identified  with  a  Tier (“Tier-0, Tier-1, Tier-2, Tier-3”) number following the compliance statement IAW AFI 33-360. Submit  requests  for  waivers  through  the  chain  of  command  to  the  appropriate  Tier  waiver approval authority, or alternately, to the Publication OPR for non-tiered compliance items.  2 AFI99-103  6 APRIL 2017 Waivers  to  mandates  involving  the  acquisition  program  execution  chain  are  processed  in accordance  with  the  acquisition  chain  of  authority  as  specified  in  AFI  63-101/20-101.    Any organization  supplementing  this  instruction  must  send  the  proposed  document  to  AF/TEP:  usaf.pentagon.af-te.mbx.af-tep-workflow@mail.mil  for  review  prior  to  publication.    Ensure that all records created as a result of processes prescribed in this publication are maintained IAW Air Force Manual (AFMAN) 33-363, Management of Records, and disposed of IAW Air Force Records Disposition Schedule (RDS) in the Air Force Records Information Management System (AFRIMS). SUMMARY OF CHANGES This document has been extensively rewritten and should be read in its entirety.  It incorporates multiple  changes  resulting  from  the  7  January  2015  release  of  DoDI  5000.02  and  other  DoD directives and instructions since the previous version of AFI 99-103 published 16 October 2013.  Changes  resulting  from  the  rewrite  of  DoDI  5000.02  include  replacement  of  the  Test  and Evaluation Strategy (TES) by the Milestone (MS) A Test and Evaluation Master Plan (TEMP), the  addition  of  accelerated  acquisition  program  models  and  Developmental  Evaluation Framework (DEF) requirement in the TEMP.  Additions include AF cyber test policy, test-for-Foreign  Military  Sales  (FMS)  policy,  revised  interoperability  policy,  clarification  of  Anti-Tamper  (AT)  test  and  reporting  policy  and  more  direction  WRT  Lead  Developmental  Test  & Evaluation Organization (LDTO) assignment and responsibility. Chapter 2 and Chapter 3 have been swapped to align with AFI convention; “Responsibilities” typically resident in Chapter 2.  Chapter 1— TEST AND EVALUATION CONCEPTS  1.1.  Purpose of Test and Evaluation (T&E). ..................................................................  1.2.  The Acquisition Environment. ................................................................................  Figure  1.1.  Integration of the Requirements, Acquisition, IT, and T&E Process. ....................  1.3.  General T&E Principles. .........................................................................................  1.4.  Integrated Test Team (ITT). ...................................................................................  1.5.  Document Organization. .........................................................................................  1.6.  Applicability and Authority. ...................................................................................  1.7.  Areas Not Covered by this AFI. .............................................................................  1.8.  Compliance Items. ..................................................................................................  Chapter 2— RESPONSIBILITIES  2.1.  Overview of Responsibilities. .................................................................................  2.2.  Director, Operational Test and Evaluation (DOT&E). ...........................................  7 7 7 9 10 12 12 13 13 14 15 15 15  AFI99-103  6 APRIL 2017 2.3.  Deputy Assistant Secretary of Defense for Developmental Test and Evaluation (DASD(DT&E)). .....................................................................................................  2.4.  Headquarters, U.S. Air Force, Director of Test and Evaluation (AF/TE). .............  2.5. 2.6. 2.7. 2.8.  Assistant Secretary of the Air Force for Acquisition, Technology, and Logistics (SAF/AQ).................................................................................................................   Headquarters, U.S. Air Force, Deputy Chief of Staff for Intelligence, Surveillance, and Reconnaissance (AF/A2). ...........................................................   Headquarters, U.S. Air Force, Deputy Chief of Staff for Operations, Plans, & Requirements (AF/A3) and Strategic Plans and Programs (A5/8). .........................   Secretary of the Air Force, Office of Information Dominance and Chief Information Officer (SAF/CIO A6). ........................................................................  2.9.  Headquarters, Air Force Materiel Command (AFMC). ..........................................  2.10.  Headquarters, Air Force Space Command (AFSPC). .............................................  2.11.  Operational MAJCOMs, DRUs, and FOAs. ...........................................................  2.12.  Air Force Operational Test and Evaluation Center (AFOTEC)..............................  2.13.  United States Air Force Warfare Center (USAFWC). ............................................  2.14.  Operational Test Organizations (OTO). .................................................................  2.15.  Program Executive Officer (PEO). .........................................................................  2.16.  Program Managers (PM).........................................................................................  2.17.  Chief Developmental Tester (CDT), Test Manager (TM). .....................................  2.18.  Lead Developmental Test and Evaluation Organization (LDTO). .........................  2.19.  Participating Test Organizations (PTO). .................................................................  2.20.  Integrated Test Team (ITT). ...................................................................................  Chapter 3— TYPES OF TEST AND EVALUATION  3.1.  Major Categories of Test & Evaluation. .................................................................  3.2.  Developmental Test & Evaluation. .........................................................................  3.3.  Types of Developmental Testing. ...........................................................................  3.4.  Operational Test. .....................................................................................................  3.5.  Types of OT&E. .....................................................................................................  Table  3.1.  Summary of Operational Testing Options. .............................................................   3 15 15 16 17 17 17 18 19 20 21 22 22 23 23 26 27 28 28 30 30 30 31 32 32 36 4 AFI99-103  6 APRIL 2017 3.6.  Testing of Training Devices....................................................................................  3.7.  Specialized Types of Test and Evaluation. .............................................................  Table  3.2.  Specialized Types of T&E. .....................................................................................  3.8.  Weapons System Evaluation Program (WSEP)......................................................  3.9.  Other Test Considerations.......................................................................................  Chapter 4— T&E ACTIVITIES SUPPORTING MILESTONE A DECISIONS  4.1.  Pre-MS A Tester Involvement. ...............................................................................  Figure  4.1.  Integration of Requirements, Acquisition, and T&E Events Prior to MS A. ..........  4.2.  Pre-MS A Tester Involvement in Requirements Development. .............................  4.3.  Pre-MS A Tester Involvement in the Acquisition Process. ....................................  4.4.  Formation of the ITT. .............................................................................................  Figure  4.2.  Integrated Test Team. .............................................................................................  4.5.  Determining the LDTO. ..........................................................................................  4.6.  Determining the OTO. ............................................................................................  Figure  4.3.  Determining the Operational Test Organization. ....................................................  4.7.  OSD T&E Oversight and Approval. .......................................................................  4.8.  Lead Service Considerations. ..................................................................................  4.9.  Tester Inputs during Materiel Solution Analysis (MSA). .......................................  4.10.  Developing Test Measures. .....................................................................................  4.11.  Test and Evaluation Master Plan (TEMP). .............................................................  4.12.  Lead DT&E Integrator. ...........................................................................................  4.13.  Reliability Growth Planning. ..................................................................................  4.14.  Program Protection. ................................................................................................  4.15.  Pre-Milestone A Planning for T&E Resources. ......................................................  4.16.  Testing Defense Business Systems (DBS). ............................................................  4.17.  Testing of Urgent Needs. ........................................................................................  4.18.  Additional Early Planning Considerations. .............................................................  Table  4.1.  Topics for Early Test Planning Consideration. .......................................................  36 36 37 37 38 40 40 40 40 41 41 43 44 45 46 47 49 49 49 50 52 52 53 53 54 55 55 56  AFI99-103  6 APRIL 2017 Chapter 5— T&E ACTIVITIES SUPPORTING MILESTONE B DECISIONS  5.1.  Post MS A. ..............................................................................................................  Figure  5.1.  Integration of Requirements, Acquisition, and T&E Events Prior to MS B. ..........  5.2.  T&E Funding Sources. ...........................................................................................  5.3.  Formal Contractual Documents. .............................................................................  5.4.  Limitations on Contractor Involvement in Operational Testing. ............................  5.5.  Testing IT and DBS. ...............................................................................................  5.6.  Modeling and Simulation (M&S) in Support of T&E. ...........................................  5.7.  Pre-MS B DT&E Planning. ....................................................................................  5.8.  LFT&E Planning. ....................................................................................................  5.9.  Early Operational Assessment (EOA) Planning and Execution. ............................  5.10.  Tester Involvement in Requirements Documentation. ...........................................  5.11.  Critical Technical Parameters (CTP). .....................................................................  5.12.  Testing COTS, NDI, and GFE. ...............................................................................  5.13.  Scientific Test and Analysis Techniques (STAT). ..................................................  5.14.  Cyber Test. ..............................................................................................................  5.15.  RFP TEMP. .............................................................................................................  5.16.  MS B TEMP ...........................................................................................................  5.17.  Tailored Integrated Documentation. .......................................................................  5.18.  Management of T&E Data. .....................................................................................  5.19.  Deficiency Reporting (DR) Process. .......................................................................  5.20.  DRs for Cyber Vulnerabilities. ...............................................................................  5.21.  Independent Technical, Environmental and Safety Reviews. .................................  5.22.  Test Deferrals, Limitations, and Waivers. ..............................................................  Chapter 6— T&E ACTIVITIES IN SUPPORT OF MILESTONE C AND BEYOND  6.1.  Post MS B. ..............................................................................................................  Figure  6.1.  Integration of Requirements, Acquisition, and T&E Events Supporting MS C and Beyond. .............................................................................................................  6.2.  Refining the ITC in the TEMP. ...............................................................................   5 57 57 57 57 58 58 59 59 60 61 63 63 63 64 64 64 65 65 66 66 68 69 69 70 71 71 71 71 6 AFI99-103  6 APRIL 2017 6.3.  Developing Test Plans That Are Integrated. ...........................................................  6.4.  Realistic Testing. ....................................................................................................  6.5.  Certification of System Readiness for Dedicated Operational Testing. .................  6.6.  Plans and Briefings for Operational Testing. ..........................................................  6.7.  OSD Involvement. ..................................................................................................  6.8.  Operational Tester DR Responsibilities. .................................................................  6.9.  Interoperability Certification Testing. ....................................................................  6.10.  Tracking and Closing DRs. .....................................................................................  6.11.  Modifications. .........................................................................................................  6.12.  Integrated Testing During Sustainment and Follow-on Increments. ......................  6.13.  Disposing of Test Assets. ........................................................................................  6.14.  OT Reporting on Fielding of Prototypes or Pre-Production Systems. ....................  Chapter 7— TEST AND EVALUATION REPORTING  7.1.  General Reporting Policy. .......................................................................................  7.2.  DT&E Reports. .......................................................................................................  7.3.  DT&E Report Distribution. ....................................................................................  7.4.  Operational Test Reports. .......................................................................................  7.5.  Capabilities and Limitations (C&L) Reports. .........................................................  7.6.  AT Reports. .............................................................................................................  7.7.  Operational Test Report Distribution. .....................................................................  7.8.  Briefing Trail. .........................................................................................................  7.9.  Distributing and Safeguarding Test Information. ...................................................  7.10.  Information Collection and Records. ......................................................................  Attachment 1— GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION Attachment 2— INFORMATION REQUIREMENTS FOR OSD T&E OVERSIGHT PROGRAMS   72 72 73 75 75 75 75 76 76 76 76 77 78 78 78 78 78 79 80 80 80 80 81 82 112   AFI99-103  6 APRIL 2017 7 Chapter 1 TEST AND EVALUATION CONCEPTS 1.1.  Purpose  of  Test  and  Evaluation  (T&E).  The  fundamental  purpose  of  T&E  is  to  ensure DoD  acquires  systems  that  work  and  meet  specified  requirements.    Additionally,  overarching functions  of  T&E  are  to  mature  system  designs,  manage  risks,  identify  and  help  resolve deficiencies  as  early  as  possible,  assist  in  reducing  unintended  cost  increases  during development,  operations,  and  throughout  the  system  life  cycle,  and  ensure  systems  are operationally  mission  capable  (i.e.,  effective,  suitable,  survivable,  and  safe).    T&E  provides knowledge  of  system  design,  capabilities,  and  limitations  to  the  acquisition  community  to improve system performance before production and deployment, and to the user community for optimizing  system  operations  and  sustainment  after  production  and  deployment.    The  T&E community will: 1.1.1.  Collaborate  with  capability  requirements  sponsors  and  system  developers  to  field effective  and  suitable  systems  that  meet  program  baseline  goals  for  cost,  schedule,  and performance. 1.1.2.  Provide timely, sufficient, accurate, and affordable information to decision makers to support production and fielding decisions. 1.1.3.  Provide data and information in support of managing risks during acquisition, fielding, and  sustainment  by  accurately  characterizing  system  technical  and  operational  performance throughout the system life cycle. 1.1.4.  Support  the  acquisition  and  sustainment  communities  in  acquiring  and  maintaining operationally mission capable systems for Air Force users. 1.1.5.  Provide  information  to  users  to  assess  mission  impacts,  develop  policy,  improve requirements, and refine tactics, techniques, and procedures (TTP). 1.2.  The  Acquisition  Environment.  The  Integrated  Life  Cycle  Management  (ILCM) Framework is the overarching system of concepts, methods, and practices the Air Force uses to effectively  manage  systems  from  capability  gap  identification  through  final  system  disposal.  The goals of ILCM are to recapitalize Air Force capabilities through maximum acquisition cycle time efficiency, provide agile support that will optimize fielded capabilities and the supply chain, minimize  the  logistics  footprint,  and  reduce  total  ownership  cost.    ILCM  begins  with capabilities-based  requirements  development  and  continues  with  capability-based  acquisition, T&E,  expeditious  fielding,  sustainment,  and  final  disposition.    See  AFI  63-101/20-101,  for details. 1.2.1.  Software Intensive Acquisition.  DoDI 5000.02 describes various defense acquisition program  models  tailored  to  the  type  of  product  being  acquired  or  need  for  accelerated acquisition.    The  objective  is  to  balance  needs  and  available  capability  with  resources,  and place  capability  into  the  hands  of  the  user  quickly.  The  success  of  the  strategy  depends  on phased  definition  of  capability  needs  and  system  requirements,  maturation  of  technologies, and disciplined development and production of systems with increased capability.  Models 2 and  3  (Figures  4  and  5)  in  DoDI  5000.02  address  software-intensive  programs;  Model  3 highlighting rapid  delivery of capability.  Regardless of acquisition strategy, an  appropriate  8 AFI99-103  6 APRIL 2017 level  of  Developmental  Test  (DT)  and  Operational  Test  (OT)  are  required  prior  to  fielding new  capabilities.    Further,  each  limited  deployment  software  release  that  impacts  the system’s Net-Ready Key Performance Parameters (NR-KPP) will drive the requirement for NR-KPP certification or assessment. 1.2.1.1.  System acquisition is  increasingly software-intensive allowing deployment of a series  of  releases  within  a  formal  acquisition  increment.    A  distinct,  tested,  deployable software element of a militarily useful capability to the government will be referred to as a  “release.”    A  release  may  be  a  subset  of  a  formal  acquisition  increment  or  the  final product.    Releases  incorporate  multiple  “builds:”  a  version  of  software  that  meets  a specified  subset  of  the  requirements  but  is,  in  itself,  not  deployable.    For  consistency, “release”  will  be the  smallest fieldable/deployable  software  element  in  all  future  AF  TEMPs,  Operational  Test  Plans (OTPs),  and  test  reports  as  well  as  updates  to  previous  documents.    Reference  the glossary  in  Attachment  1  to  distinguish  the  terms:    “release,”  “build,”  “block,”  and “increment.” the  only  accepted term  used to  describe 1.2.1.2.  Each software release must undergo DT and OT prior to deployment IAW DoDI 5000.02.  A  risk  analysis  will  be  conducted  by  the  lead  Operational  Test  Organization (OTO) documenting the degree of risk and potential impact on mission accomplishment for  each  capability.  (T-1)  The  results  of  this  analysis  are  expected  to  be  part  of  the program's TEMP and will be used to determine the appropriate level of Operational Test and Evaluation (OT&E) to assess operational effectiveness, suitability, cybersecurity and cyber  resiliency.    Documentation  and  coordination  requirements  can  be  minimized  by identifying,  in  advance,  multiple  activities  or  build  phases  to  be  approved  at  any  given milestone or decision point. 1.2.2.  Collaborative  Concepts  and  Processes.  ILCM  is  based  on  concepts  and  processes described in the AF/A5R Requirements Development Guidebooks, AFI 63-101/20-101, AFI 17-130, Air Force Cybersecurity Program Management, AFI 17-101, and this AFI.  Figure 1.1 shows the acquisition process as the “master clock” for the integration of requirements, acquisition, IT activities, and T&E events.  Sections of Figure 1.1 are used at the beginning of  Chapter  4,  Chapter  5,  and  Chapter  6  to  illustrate  key  events  during  each  acquisition phase.    These  diagrams  represent  the  full  spectrum  of  processes  and  events.    DoD  and  AF guidance  provides  program  managers  (PM)  with  the  flexibility  to  tailor  programs,  within certain limits, to meet specific program requirements. 1.2.3.  Integrated  Warfighting/Cross-Domain  Test  and  Evaluation.  The  ability  to successfully conduct a mission may require the integration of activities and products from a combination  of  weapon  systems,  support  systems,  and  enabling  systems  that  operate  in  air, space,  and  cyberspace.    Cross-domain  testing  of  interoperable  systems  is  essential  in identifying vulnerabilities and evaluating mission performance. 1.2.4.  Capabilities-Based Testing.  Capabilities-based testing evaluates the capability of the system  to  effectively  accomplish  its  intended  mission  in  a  realistic  mission  environment  in addition  to  meeting  individual  technical  specifications.    The  current  emphasis  on  joint military  operations  in  an  information-intensive  environment  means  that  Air  Force  systems will  seldom  operate  in  combat  as  completely  independent  entities.    Air  Force  systems  are expected  to  fully  integrate  with  systems,  activities,  and  products  from  all  Services  and  AFI99-103  6 APRIL 2017 9 National  agencies.    Capabilities-based  testing  requires  a  full  understanding  of  joint operational concepts in order to develop test scenarios that will provide meaningful results. 1.2.5.  Interoperability,  AT,  and  Cyber  Test.  Nearly  all  systems  today  have  Information Technology  (IT)  content,  direct  and  indirect  network  connections,  interfacing  systems,  and data  exchanges  requiring  some  level  of  interoperability,  AT,  cybersecurity  and  cyber resiliency testing.  The lowest bar in Figure 1.1 shows additional requirements from the 17-series  AFIs  for  IT  and  software-intensive  systems  as  they  are  integrated  with  the requirements, acquisition, and T&E processes.  Interoperability test including assessment of the  NR-KPP(s)  is  critical  to  ensuring  interoperable  systems;  interoperability  guidance  is found in DoDI 8330.01.  AT is required on systems with Critical Program Information (CPI) IAW DoDD 5200.47E and testing of this capability should be coordinated with SAF/AQLS as  the  Air  Force  OPR.    Additionally,  system  cybersecurity  design  and  cyber  test  should  be considered  at  program  initiation  and  integrated  throughout  the  acquisition  life  cycle  IAW DoDI 5000.02.  Cybersecurity is defined in DoDI 8500.01; primarily system and information protection.  The concept of cyber operational resiliency is captured in the DoD Cybersecurity Test  and  Evaluation  Guidebook;  detection  of  and  recovery  from  cyber  attack.    In  this  AFI, cyber test includes both cybersecurity testing (system defense against cyber attack) and cyber resiliency testing (system detection and response if defense is defeated). Figure 1.1.  Integration of the Requirements, Acquisition, IT, and T&E Process. Notes: 1. Represents a notional flow and is not all inclusive.  Programs may be tailored with approval of the MDA.  See AFI 63-101/20-101. 2. All test-relevant acronyms in this figure are listed in Attachment 1.   10 AFI99-103  6 APRIL 2017 1.3.  General  T&E  Principles.  The  objective  of  T&E  is  to  provide  accurate,  objective,  and defensible  information  to  the  decision  makers  (e.g.,  Milestone  Decision  Authority  (MDAs))  to make  informed  acquisition  decisions  as  well  as  meet  requirements  of  10  United  States  Code (U.S.C.)  §139b  and  10  U.S.C.  §2399.    Developmental  test  assesses  system  compliance  with mandated  requirements,  contracted  specifications,  and  acquisition  baselines,  and  provides  such feedback  to  system  developers  early  in  the  program.    Operational  test  gauges  weapon  system performance, in terms of effectiveness and suitability through comprehensive, rigorous test in a realistic  operational  environment. testing:  collaborative developmental and operational test planning and execution throughout the program life  cycle.    The  following  T&E  principles  are  IAW  DoD  5000-series  documents  and  lessons learned.  The unifying theme is that all testers must collaborate to the fullest extent possible to effectively evaluate programs and systems regardless of organizational affiliation.  Because the acquisition process is fluid, testers must ensure the intent of this AFI is implemented at all times.   Efficiencies  are  gained through integrated 1.3.1.  Tailoring.  The  Integrated  Test  Team  (ITT)  ensures  that  all  strategies  for  T&E, concepts,  plans,  briefings,  and  reports  are  flexible  and  tailored  to  fit  the  specific  needs  of acquisition  programs  consistent  with  sound  systems  engineering  practices,  program  risk, statutory  and  regulatory  guidelines,  the  time-sensitive  nature  of  users’  requirements,  and common  sense.    Reduced  documentation  and  approvals  enable  accelerated  delivery  of capabilities;  e.g.  a  single  TEMP  or  CDD  could  cover  all  releases  for  software  intensive programs. If a project or program is authorized to enter the acquisition process at other than the  beginning  (e.g.,  entry  at  MS  B),  the  ITT  reviews  all  activities  that  would  normally  be accomplished prior to that point and ensure any mandatory prerequisites are accomplished. 1.3.2.  Pre-MS A Tester Involvement.  The early provision of T&E expertise and technical and operational  insight  to acquisition professionals  and requirements developers, preferably starting  before  MS  A,  is  key  to  successful  initiation  of  new  programs.    The  earlier  the involvement,  the  greater  the  opportunity  to  reduce  unintended  increases  to  development, operations,  and  life  cycle  costs.    Candidate  materiel  solution  approaches  are  better understood and risks reduced when testers make technical contributions to early acquisition planning activities.  Deficiencies must be identified as early as possible to enable resolution, increase  program  efficiency  and  economy  of  effort.    Reference  paragraph  4.1  for  more details on pre-MS A guidance.   Considering  cost,  schedule, 1.3.3.  Event  -Driven  Schedules  and  Exit  Criteria. performance,  adequate  time  and  resources  must  be  planned  and  provided  for  all  T&E activities IAW DoDI 5000.02 and AFI 63-101/20-101.  T&E activities must demonstrate the system  meets  established  engineering  objectives,  operational  capability  requirements,  and exit criteria before moving to the next phase of development.  The PM will use a TEMP as the  primary  planning  and  management  tool  for  the  integrated  test  program.    The  PM  must ensure  the  system  is  operationally  production  representative,  stable,  and  mature  before  it  is certified  ready  for  dedicated  operational  testing.    See  AFMAN  63-119,  Certification  of System  Readiness  for  Dedicated  Operational  Testing,  for  more  details.    For  details  about system  maturity  levels,  see  DoD  Technology  Readiness  Assessment  (TRA)  Guidance,  April 2011. 1.3.4.  Integrated  Testing.  Integrated requires  collaborative  planning  and collaborative  execution  of  test  phases  and  events  to  provide  shared  data  in  support  of independent  analysis,  evaluation,  and  reporting  by  all  stakeholders,  particularly  the testing  AFI99-103  6 APRIL 2017 11 developmental  (both  contractor  and  government)  and  operational  test  and  evaluation communities.    Effective  ITTs  plan  and  execute  testing  that  is  integrated  across  the  entire program  life  cycle  including  program’s  requirements  generation  and  system  engineering processes  to  include  cybersecurity  and  cyber  resiliency.  In  addition,  ITTs  evaluate  system interoperability  of  a  system  of  systems  or  family  of  systems,  as  applicable,  and  integrate developmental and operational test.  Integrated testing is a concept for test management and design, not a new type of T&E.  It structures T&E to reduce the time needed to field effective and suitable systems by providing qualitative and quantitative information to decision makers throughout  the  program’s  life  cycle.    Integrated  testing  minimizes  the  gaps  and  can  reduce duplicative testing  by implementing  integrated  testing  techniques  and  objectives  to  the  maximum  extent  possible. Integrated  testing  does  not  eliminate  dedicated  IOT&E  required  for  Major  Defense Acquisition  Programs  (MDAP)  and  programs  on  oversight  by  10  U.S.C.  §2399  and  DoDI 5000.02 or Force Development Evaluation (FDE) for MAJCOM OT&E. testing  between  contractor,  developmental,  and  operational 1.3.4.1.  Integrated  testing  must  be  designed  into  the  earliest  program  strategies,  plans, documentation,  and  test  plans,  preferably  starting  before  MS  A  for  new  starts  and immediately after Materiel Development Decision (MDD) for programs starting post-MS A.    Test  planning  must  consider  the  entire  life  cycle  of  program  activities  from technology  development  through  disposal,  including  testing  relevant  to  manufacturing and sustainment.  The earlier integrated testing strategies are developed and adopted, the greater  the  efficiencies  and  benefits.    If  done  correctly,  integrated  testing  will  identify system design improvements early in developmental test and evaluation (DT&E), reduce the  amount  of  T&E  resources  needed  for  OT&E,  and  help  PMs  control  unintended increases to development, operations, and life cycle costs. 1.3.4.2.  Test  planning,  including  cyber  test  planning,  must  be  integrated  with  the requirements  generation  process  and the  system  engineering  process,  yielding requirements  that  are  testable  and  achievable,  and  test  plans  that  provide  actionable capabilities-oriented  test  results.    It  requires  an  understanding  of  how  systems  will  be employed in operational environments and mandates that strategies for T&E and plans be designed to determine whether a new capability solution merits fielding.  Furthermore, in light of the joint operational environment, effective test planning and execution integrates with testing of other systems to evaluate interoperability.  Proactive planning will allow the OTO to use data from DT for OT; when such testing is conducted on a stable system in an operationally relevant environment. (M&S),  contractor testing,  developmental  and  operational 1.3.4.3.  Integrated  testing  may  include  all  types  of  test  activities  such  as  modeling  and simulation testing, interoperability testing of a system of systems or family of systems, as appropriate, cyber testing,  and  certification  testing  as  described  in  Chapter  3.    All  types  of  testing, regardless  of  the  source,  should  be  considered,  including  tests  from  other  Services  for multi-Service  programs.    Tests  will  be  integrated  to  the  maximum  extent  possible  and will use the reciprocity principle as much as possible, i.e., "test by one, use by all."  Note:  This AFI will use the term “integrated testing” to capture this broad intent.  “Integrated DT&E/OT&E”  is  the  most  common  combination,  but  many  other  combinations  are possible.  12 AFI99-103  6 APRIL 2017 1.3.4.4.  All  testers  collaborate  as  an  ITT  to  generate  an  overarching  strategy  for  T&E and test plans that are integrated.  These plans must leverage all available test activities and resources while minimizing redundant testing and waste.  The result is an integrated test  approach  with  harmonized  test  plans  that  efficiently  work  together  throughout  the acquisition  program,  and  not  necessarily  a  single  test  plan.    An  integrated  test  concept (ITC) must be developed as part of the TEMP when initiating test planning as described in paragraphs 4.11,  6.2,  6.3 and  6.4. 1.3.4.5.  Integrated  testing  must  provide  appropriate  data  collection  instrumentation  and shared data in support of independent analyses for all stakeholders.  Shared data provides continuous written feedback from test organizations to the PM and other stakeholders on all  aspects  of  program  development.    For  each  program,  a  common  T&E  database  is required  according  to  paragraph  5.18.1  that  includes  descriptions  of  the  test environments  and  conditions  to  ensure  commonality  and  usability  by  other  testers.    It does not necessarily include the earliest engineering design or data from early prototypes which may not be relevant. 1.3.5.  Objectivity.  All Air Force T&E activities must be objective, unbiased, and free from outside influences to  ensure the integrity of evaluation results IAW AFPD 99-1.  Air Force programs ensure objective DT&E by designating an LDTO that is separate from the program office.  An independent OTO is assigned to ensure objective OT&E for all programs. 1.4.  Integrated  Test  Team  (ITT).  The  PM  establishes  an  ITT  as  soon  as  possible  after  the MDD  as  shown  in  Figure  1.1  to  create  and  manage  the  strategy  for  T&E  for  the  life  of  each program.  The ITT construct is central to carrying out integrated testing and is equivalent to the T&E  Working-level  Integrated  Product  Team  (WIPT)  described  in  the  DoDI  5000.02  and  the Defense Acquisition Guide (DAG).  The Chief Developmental Tester (CDT) and the lead OTO’s designated test director co-chair the ITT using the general T&E principles outlined in paragraph 1.3.  For non-MDAP and non-Major Automated Information System (MAIS) programs the term “Test  Manager  (TM)”  will  be  used  consistent  with  AFI  63-101/20-101.    Note:  When  this  AFI refers to the CDT, it also includes the TM.  CDTs and/or TMs will advise the PM and the ITT. ITT membership includes all organizations needed to implement a comprehensive and integrated test strategy for as long as T&E is needed.  Typical ITT member organizations are described in paragraph  4.4.4.    Also  see  the  Air  Force  Test  and  Evaluation  Guidebook  for  details  on  ITT structure,  responsibilities,  charters,  and  functions.    The  Guidebook  is  available  on  the  AF/TE portion https://www.my.af.mil/gcss-afbvpcp/USAF/ep/browse.do?programId=t6925EC2D581C0FB5E044080020E329A9&channelPageId=s6925EC1355030FB5E044080020E329A9. Portal Force of the Air 1.5.  Document  Organization.  This  AFI  follows  the  acquisition  process  phases  in  DoDI 5000.02 as shown in Figure 1.1.  Chapter 4, Chapter 5, and Chapter 6 contain direction most pertinent  to  achieving  the  goals  of  MS  A,  B,  and  C  respectively.    Each  chapter’s  activities typically support that particular MS or phase, but depending on program needs, may be partially completed  or  even  deferred  to  the  next  phase.    The  sequence  of  activities  presented  generally follows the flow of Figure 1.1, but in all cases, planning for each area should be started as early as practical.  Note: Programs that enter the acquisition process after MS A must accomplish the necessary  “stage-setting”  activities  specified  for  the  preceding  milestones  in  Chapter  4  and Chapter 5.  AFI99-103  6 APRIL 2017 13 1.6.  Applicability  and  Authority.  The  policies  and  processes  in  this  AFI  apply  to  AF  T&E organizations and all programs, projects and activities that support ILCM. These include but are not  limited  to  acquisition,  MAJCOM-directed  acquisition,  sustainment  and  modification programs,  projects  and  activities.    These  policies  and  processes  apply  regardless  of  funding source or ACAT level,  unless otherwise noted.  See DoDI 5000.02, Enclosure 1, for details  on ACATs.    Air  Force  Special  Access  Programs  (SAP)  and  other  sensitive  programs  (e.g.,  BIG SAFARI  projects)  will  follow  the  intent  of  this  AFI  to  the  extent  that  security  considerations allow.    When  the  Air  Force  is  not  the  lead  Service  for  test,  Air  Force  testers  follow  the  lead Service’s  or  Joint  T&E  policies.    Joint  T&E  of  nuclear  weapons  systems  and  nuclear  weapons systems  components  will  be  governed  by  this  AFI  unless  otherwise  specified  by  the  joint Memorandum of Understanding (MOU) developed by the Air Force and Department of Energy.  Exceptions  to  policy  will  be  coordinated  with  SAF/AAZ,  Security  and  Special  Program Oversight,  SAF/AQL,  Special  Programs,  SAF/AQI,  Information  Dominance,  or  AF/TE,  as applicable.    Note:  In  this  AFI,  guidance  provided  for  “MAJCOM”  test  activities  shall  be understood to apply also to FOA and DRU test activities (except the Air Force Operational Test and Evaluation Center (AFOTEC)). 1.6.1.  Hierarchy  of  Authority.  Authority  for  this  AFI  flows  from  congressional  statute through  DoD-level  issuances,  and  AFPD  99-1,  Test  and  Evaluation.    Specific  details  for implementing  this  policy  are  delegated  to  and  more  appropriately  developed  by  Air  Force MAJCOMs, FOAs, and DRUs, and their subordinate designated T&E organizations based on specific mission areas and needs. 1.6.2.  Hierarchy  of  Knowledge  Management.  It  is  not  possible  for  this  AFI  to  prescribe detailed T&E policy and TTP for each of the Air Force’s many mission areas, programs, and T&E  activities.    Therefore,  all  T&E  organizations  must  establish  tailored,  disciplined,  and collaborative processes for planning, executing, and reporting T&E activities. 1.6.3.  Qualification of Test Personnel.  In order to apply the T&E principles in paragraph 1.3,  a  highly  trained  and  qualified  T&E  workforce  is  required.    Government  personnel performing test should be at least be ACQ level 1 T&E certified and personnel managing or directing  test  at  a  test  organization  and  personnel  performing  acquisition  test  management duties at a program office should have at least two years of test experience and preferably an ACQ Level 2 T&E certification.  Supervisors and commanders at all levels are expected to enforce applicable qualification standards in accordance with this and other applicable DoD and Air Force policy. 1.7.  Areas  Not  Covered  by  this  AFI.  The systems, programs, and activities listed in the sub-paragraphs below are not within the purview of this AFI. 1.7.1.  Activities  associated  with  the  space  experimentation  program  described  in  AFI  10-1202, Space Test Program (STP) Management. 1.7.2.  The  management  procedures  of  this  AFI  do  not  apply  to  science  and  technology (S&T) programs, demonstrations, experiments, or projects, which are managed in accordance with AFI 61-101, Management of Science and Technology.   However, when S&T activities are  conducted  post  Milestone  A  or  under  the  authority  of  a  PEO,  the  exemption  no  longer applies unless specifically authorized by AFMC/A3 or AFSPC/A5, as applicable.  Projects or experiments  resulting  in  a  fielded  capability,  (i.e.,  “leave-behind”)  will  follow  direction  in this  AFI  once  they  become  programs  of  record  or  are  transitioned  to  PEO  management.   14 AFI99-103  6 APRIL 2017 Those  managing  S&T  activities  exempted  from  this  AFI  will  follow  its  intent  as  much  as possible while balancing the nature of the S&T mission.  S&T organizations should consider tailored  application  of  these  principles  to  streamline  transition  efforts  and  reduce  future program costs. 1.8.  Compliance  Items.  Each  unit  (wing  or  equivalent,  and  below,  DRU,  FOA)  compliance item is identified with a Tier waiver authority number.  A “T-0” denotes a requirement external to the USAF; requests for waivers must be processed through command channels to AF/TEP for consideration.   For  “T-1” items,  the waiver  authority is  the MAJCOM/CC (delegable no lower than the MAJCOM Director), with the concurrence of AF/TE. 1.8.1.  The AFOTEC/CC is delegated waiver authority for AFOTEC “T-1” compliance items with concurrence of AF/TE. 1.8.2.  IAW  AFI  63-101/20-101,  mandates  to  the  acquisition  execution  chain  are  not considered  Wing  level  mandates  and  tiering  does  not  apply.  When  tiering  does  apply  for wing/unit level requirement, waiver authority is identified with a Tier (“T-0, T-1, T-2, and T-3”) number following the compliance statement IAW AFI 33-360.  AFI99-103  6 APRIL 2017 15 Chapter 2 RESPONSIBILITIES 2.1.  Overview of Responsibilities.  All Air Force testers, to include test execution organization personnel  and  program  office  test  management  personnel  will  follow  the  T&E  principles articulated  in  Chapter  1  of  this  AFI  using  the  types  of  tests  described  in  Chapter  3.    Testers must collaborate with each other, the broader acquisition community, and requirements sponsors using the ITT as the T&E focal point for each program. 2.2.  Director,  Operational  Test  and  Evaluation  (DOT&E).  DOT&E  responsibilities  are described in DoDD 5141.02, Director of Operational Test and Evaluation (DOT&E). 2.3.  Deputy  Assistant  Secretary  of  Defense  for  Developmental  Test  and  Evaluation (DASD(DT&E)).  DASD(DT&E)  responsibilities  are  described  in  DoDI  5134.17,  Deputy Assistant Secretary of Defense for Developmental Test and Evaluation (DASD(DT&E)). 2.4.  Headquarters, U.S. Air Force, Director of Test and Evaluation (AF/TE).  AF/TE will: 2.4.1.  Function as the chief T&E advisor to Air Force senior leadership IAW Headquarters Air  Force  Mission  Directive  (HAFMD)  1-52,  Director  of  Test  and  Evaluation.    Be responsible  to  the  Chief  of  Staff  of  the  Air  Force  (CSAF)  for  establishing  Air  Force  T&E policy, advocating for T&E resources required to support weapons system development and sustainment, and resolving T&E issues and disputes. 2.4.2.  Act as the final Air Staff T&E review authority and signatory for TEMPs (to include Request  for Proposal  (RFP) TEMP) prior to  Service Acquisition Executive (SAE) approval and signature.  AF/TE will approve/sign TEMPS for ACAT I programs and any program on DOT&E oversight.    Note: The term Service Acquisition Executive (SAE) is equivalent to the term Component Acquisition Executive (CAE) used in DoD directives and instructions. 2.4.3.  Collaborate  with  requirements  sponsors  and  system  developers  to  improve  the development,  testing,  and  fielding  of  Air  Force  systems  or  subsystems.    Participate  in  high performance teams  (HPTs),  ITTs,  and integrated product  teams  (IPTs)  as necessary  to  help ensure program success. 2.4.4.  Respond  to  and  mediate  Air  Force  T&E  issues  between  HQ  USAF  principals, MAJCOMs, Air Force testers, the Services, OSD, and Congress. 2.4.5.  Review  and/or  prepare  T&E  information  for  release  to  OSD  and  ensure  timely availability of T&E results to decision makers. 2.4.6.  Oversee the Air Force T&E infrastructure and ensure adequate facilities are available to support Air Force T&E activities.  Administer various T&E resource processes and chair or serve on various committees, boards, and groups listed in HAFMD 1-52. 2.4.7.  Act as the Air Force Foreign Materiel Program (FMP) Executive Agent and point of contact  for  the  Air  Staff  and  other  governmental  agencies  and  organizations  IAW  AFI  99-114, Foreign Materiel Program (S). 2.4.8.  Serve  as  the  Cross  Functional  Authority  for  T&E  personnel  managed  in  accordance with  the  Air  Force  Acquisition  Professional  Development  Program  (APDP)  and  in  16 AFI99-103  6 APRIL 2017 accordance  with  DoDI  5000.66,  Operation  of  the  Defense  Acquisition,  Technology,  and Logistics Workforce Education, Training, and Career Development Program, and 10 U.S.C., Defense  Acquisition  Workforce  Improvement  Act.    AF/TE,  in  collaboration  with  SAF/AQ and other functional authorities, functional managers and career field managers, will manage the development of a pool of qualified T&E personnel to fill Critical Acquisition Positions, including Key Leadership Positions (KLP). 2.4.9.  Provide  advice  on  ITT  charter  development  and  membership  requirements.    Review ITT charters for programs on OSD oversight. 2.4.10.  Manage the Air Force Joint Test & Evaluation (JT&E) Program and represent the Air Force  at  the  JT&E  Executive  Steering  Group,  Senior  Advisory  Council,  and  Technical Advisory Board IAW DoDI 5010.41 and AFI 99-106, Joint Test and Evaluation Program. 2.4.11.  Provide policy, guidance, and oversight of all M&S in support of T&E. 2.4.12.  Perform other duties listed in HAFMD 1-52. 2.5.  Assistant  Secretary  of  the  Air  Force  for  Acquisition,  Technology,  and  Logistics (SAF/AQ).  SAF/AQ  is  the  Air  Force  SAE,  and  is  responsible  for  all  acquisition  functions within the Air Force.  SAF/AQ will: 2.5.1.  Ensure  systems  are  certified  ready  for  dedicated  operational  testing  according  to paragraph  6.5.1  and  AFMAN  63-119,  Certification  of  System  Readiness  for  Dedicated Operational Testing.  Although AFMAN 63-119 requires the SAE to evaluate and determine system  readiness  for  IOT&E,  the  SAE  may  delegate  this  authority  in  writing  to  a  lower milestone  decision  authority  (MDA)  for  the  program,  such  as  a  Program  Executive  Officer (PEO). 2.5.2.  Ensure  T&E  responsibilities  are  documented  as  appropriate  in  TEMPs,  Acquisition Strategies  (AS),  System  Engineering  Plans  (SEP),  Life  Cycle  Sustainment  Plans  (LCSP), Program Protection Plans (PPP), and other program documentation.  Per SAF/AQE business rules,  the  PEO  delivers  the  SSS  and  draft  acquisition  documents  to  SAF/AQE  for  HAF review. 2.5.3.  Regarding  Live  Fire  Test  and  Evaluation  (LFT&E),  SAF/AQ  or  designated representatives will: 2.5.3.1.  Recommend  candidate  systems  to  DOT&E  for  compliance  with  LFT&E legislation after coordinating the proposed nominations with AF/TE. 2.5.3.2.  Approve  LFT&E  strategies  and  Air  Force  resources  required  to  accomplish LFT&E plans and forward to DOT&E.  Forward LFT&E waivers (and legislative relief requests, if appropriate) to DOT&E, if required.  See paragraph 5.8.4 for details. 2.5.4.  Approve  and  sign  TEMPs  for  all  ACAT  I,  IA,  and  other  programs  on  OSD  T&E Oversight.  Forward these Air Force-approved TEMPs to DOT&E and USD(AT&L) for final OSD approval. 2.5.5.  Ensure  leaders  knowledgeable  of  T&E  policies  and  requirements  are  selected  for MDAP and MAIS programs.  SAF/AQ or a designated representative will: 2.5.5.1.  Ensure  that  a  CDT  is  designated  for  each  MDAP  and  MAIS  program  IAW  10 U.S.C. §139b.  AFI99-103  6 APRIL 2017 17 2.5.5.2.  Ensure  that  Defense  Acquisition  Workforce  Improvement  Act  (DAWIA)  T&E acquisition-coded CDT positions for MDAP and MAIS programs are designated as KLPs IAW  the  Under  Secretary  of  Defense  (Acquisition,  Technology,  and  Logistics) (USD(AT&L)) KLP policy including DoDI 5000.66.  The occupant of this CDT position must be appropriately qualified IAW AFI 63-101/20-101, AFI 36-1301, Management of Acquisition Key Leadership Positions (KLP) and current USD(AT&L) and AF/TE policy and guidance. 2.5.5.3.  Ensure that an LDTO is designated for each program/project. 2.5.6.  Develop  and  implement  plans  to  ensure  the  Air  Force  has  provided  appropriate resources  for  developmental  testing  organizations  with  adequate  numbers  of  trained personnel  IAW  the  Weapon  Systems  Acquisition  Reform  Act  of  2009,  Public  Law  (P.L.) 111-23 §102(b)(1). 2.5.7.  Review  AT  Validation  and  Verification  (V&V)  and  test  plans  as  the  AF  AT  OPR (SAF/AQLS). 2.6.  Headquarters, U.S. Air Force, Deputy Chief of Staff for Intelligence, Surveillance, and Reconnaissance (AF/A2).  AF ISR CIO will: 2.6.1.  Ensure  appropriate  AF/A2  personnel  participate  early  in  ITTs  as  soon  as  they  are formed for acquisition and sustainment programs with ISR capabilities. 2.6.2.  Include adequate and recurring T&E of ISR systems in AF ISR policies. 2.6.3.  Review  T&E-related  documentation  to  ensure  cybersecurity  testing  fully  supports system acquisition, fielding and sustainment. 2.6.4.  Develop  and  implement  Risk  Management  Framework  (RMF)  oversight  policy  for ISR AOs to support cyber test infrastructure requirements. 2.7.  Headquarters,  U.S.  Air  Force,  Deputy  Chief  of  Staff  for  Operations,  Plans,  & Requirements (AF/A3) and Strategic Plans and Programs (A5/8).  AF A3 and A5/8 ensure: 2.7.1.  Appropriate  AF  A3,  A5/8  personnel  support  ITTs  and  participate  in  development  of strategies for T&E. 2.8.  Secretary  of  the  Air  Force,  Office  of  Information  Dominance  and  Chief  Information Officer (SAF/CIO A6).  SAF/CIO A6 will: 2.8.1.  Ensure  appropriate  AF  A6  personnel  participate  early  in  ITTs  as  soon  as  they  are formed  for  acquisition  and  sustainment  programs  with  IT  and  National  Security  System (NSS) capabilities. 2.8.2.  Develop and implement security and cybersecurity policies that include adequate and recurring  T&E  of  IT  and  NSS  IAW  DoDI  8500.01,  Cybersecurity,  DoDI  5200.39,  Critical Program  Information  (CPI)  Identification  and  Protection  Within  RDT&E,  DODI  5200.44, Protection  of  Mission  Critical  Functions  to  Achieve  Trusted  Systems  and  Networks  (TSN), DoDI 5000.02, and AFI 63-101/20-101. 2.8.3.  Partner  with  the  requirements,  acquisition,  and  T&E  communities  to  ensure  planned capabilities  are  tested  to  satisfy  net-centric,  security,  and  cyber  requirements  as  shown  in Figure 1.1 and Table 3.2.  18 AFI99-103  6 APRIL 2017 2.8.3.1.  Working  with  AF/TE,  advocate  for  funding  for  identified  T&E  infrastructure and interoperability certification test. 2.8.3.2.  Identify  qualified  and/or  certified  organizations  for  planning  and  conducting cyber test. 2.8.4.  Review  T&E-related  documentation  to  ensure  interoperability  certification  testing, security testing, and cyber testing fully support system acquisition, fielding, and sustainment according to paragraphs 4.14,  5.10,  5.14 and Table 3.2.     2.8.5.  Implement  measures  to  ensure  NR-KPPs,  including  the  associated  key  interface profiles  (KIP),  are  clearly  defined  in  the  system  architecture,  and  are  interoperable, resourced, tested, and evaluated according to the Air Force Enterprise Architecture, AFI 17-140, Air Force Architecting, CJCSI 5123.01G, Charter of the Joint Requirements Oversight Council (JROC), and OSD, JCS, and Joint Interoperability Test Command (JITC) policies. 2.8.6.  Facilitate  security,  net-readiness,  and  interoperability  certifications  as  early  as practical.    Assist  in  the  certification  of  readiness  for  operational  testing  IAW  AFMAN  63-119. 2.8.7.  Provide networthiness recommendations for test and evaluation of IT systems. 2.8.8.  Establish  and  implement  procedures  to  ensure  interoperability  test,  evaluation,  and certification of IT before connection to a DoD network IAW DoDI 8330.01and AFGM2015-33-03. 2.8.8.1.  Ensure  T&E-related  data  that  supports  interoperability  certification  testing, acquisition,  fielding,  and  sustainment  are  documented  in  the  system’s  Information Support Plan (ISP), IAW DoDI 8330.01 and AFGM2015-33-03. 2.8.8.2.  Designate a representative to the DoD Interoperability Steering Group (ISG) to coordinate with program offices and the JITC on Interim Certificates To Operate (ICTO) for systems experiencing delays in required interoperability certification testing and other related actions. 2.8.9.  Develop  and  implement  Risk  Management  Framework  (RMF)  oversight  policy  for AOs to support cyber test infrastructure requirements. 2.9.  Headquarters, Air Force Materiel Command (AFMC).  HQ AFMC will: 2.9.1.  Develop  AFMC  DT&E  guidance,  procedures,  and  Memorandums  of  Agreement (MOAs) for non-space programs in assigned mission areas to supplement this AFI.  Forward draft  copies  to  AF/TEP  Workflow  (usaf.pentagon.af-te.mbx.af-tep-workflow@mail.mil) and (usaf.pentagon.saf-aq.mbx.saf-aqxs-policy-workflow@mail.mil) for review prior to publication. SAF/AQXS workflow 2.9.2.  Ensure  nuclear  weapon  system  T&E  policies  and  issues  are  managed  IAW  AFI  63-103 and  AFI  63-125.  Assist  with  development  and approval  of nuclear  weapon subsystem test plans. 2.9.3.  Establish  and  provide  for  DT&E  training,  organization,  and  T&E  infrastructure resources.  AFI99-103  6 APRIL 2017 19 2.9.4.  Assist the PM and ITT in identifying key government DT&E organizations, to include selection of LDTO candidates, CDTs, and TMs as soon as possible after MDD according to paragraphs 4.4 and  4.5.  Participate in ITTs and Test Integrated Product Teams (TIPTs) as necessary. 2.9.5.  Establish policy for T&E focal points (e.g., on-site test authority or equivalent office) that  provide  T&E  support  and  advice  WRT  test  programs  and  projects  to  acquisition  and T&E  practitioners  at  centers  and  complexes.    These  T&E  focal  points  will  address  T&E needs at all program management reviews. 2.9.6.  Conduct long-range planning to ensure T&E infrastructure and processes are in place to support required testing. 2.9.7.  Ensure  centers  and  complexes  participate  in  T&E  resource  investment  planning processes. 2.9.8.  Review and coordinate on test plans, test reports, and test-related correspondence for programs on OSD T&E Oversight. 2.9.9.  Develop  and  maintain  a  qualified  DT&E  workforce  for  both  test  execution  at  test organizations and acquisition test management within program offices. 2.9.10.  Oversee and inspect AFMC compliance with this instruction. 2.9.11.  Develop  and  publish  LDTO  qualifications  and  LDTO  candidate  list  for  AFMC acquisition programs. 2.9.12.  Ensure RDT&E representation at pre-MDD activities to assist in early development of  operational  requirements  and  enabling  or  operating  concepts,  early  development  of  the strategy for T&E, cyber strategy, and early acquisition planning IAW AFI 10-601, AFI 63-101/20-101, and this AFI.  Identify organizations responsible for these activities.  AFMC has RDT&E  support  staff  that  should  be  supporting  the  pre-MDD  early  systems  engineering analyses. 2.10.  Headquarters, Air Force Space Command (AFSPC).  HQ AFSPC will: 2.10.1.  Develop  HQ  AFSPC  T&E  guidance,  procedures,  and  MOAs  for  space  and cyberspace  programs  to  supplement  this  AFI.    Forward  draft  copies  to  AF/TEP  Workflow (usaf.pentagon.af-te.mbx.af-tep-workflow@mail.mil) SAF/AQXS  workflow usaf.pentagon.saf-aq.mbx.saf-aqxs-policy-workflow@mail.mil to for publication. review  prior and 2.10.2.  Establish and provide for space-related DT and OT training, organization, and T&E infrastructure resources. 2.10.3.  Assist the PM and ITT in identifying key government DT&E organizations for space programs, to include selection of LDTO candidates, CDTs, and TMs as soon as possible after MDD according to paragraphs 4.4 and  4.5.  Participate in ITTs and TIPTs as necessary. 2.10.4.  Establish policy for and maintain a T&E focal point (e.g., test authority or equivalent office)  that  provides  T&E  support  and  advice  to  acquisition  and  T&E  practitioners  at  the command’s product center.  These T&E focal points will address T&E needs at all program management reviews.  20 AFI99-103  6 APRIL 2017 2.10.5.  Conduct long-range planning to ensure T&E infrastructure and processes are in place to support required testing. 2.10.6.  Ensure  HQ  AFSPC  and  Space  and  Missile  Systems  Center  (SMC)  participation  in T&E  resource  investment  planning  processes.    Advocate  for  and  procure  space  and cyberspace T&E infrastructure, resources, and requirements. 2.10.7.  Review and coordinate on test plans, test reports, and test-related correspondence for programs on OSD T&E Oversight. 2.10.8.  Develop  and  maintain  a  qualified  DT&E  and  OT&E  workforce.    Apportion  space-qualified OT&E workforce to Air Combat Command as requested. 2.10.9.  Establish  and  maintain  capability  to  conduct  operational  test  of  cyber  warfare capabilities,  cyber  operations  capabilities,  and  evaluated  level  of  assurance  (ELA)  testing; see  DoDI  O-3600.03,  Technical  Assurance  Standard  (TAS)  for  Computer  Network  Attack (CNA) Capabilities. 2.10.10.  Oversee and inspect AFSPC compliance with this instruction. 2.10.11.  Implement the T&E policies in DoDI S-3100.15, Space Control, for space control systems,  and lead test activities associated with  the implementation  of DoDI 8100.04,  DoD Unified Capabilities (UC), for the Air Force. 2.10.12.  Ensure RDT&E representation at pre-MDD activities to assist in early development of  operational  requirements  and  enabling  or  operating  concepts,  early  development  of  the strategy for T&E, cyber strategy, and early acquisition planning IAW AFI 10-601, AFI 63-101/20-101, and this AFI.  Identify organizations responsible for these activities.  AFSPC (at SMC)  has  RDT&E  support  staff  that  should  be  supporting  the  pre-MDD  early  systems engineering analyses. 2.11.  Operational MAJCOMs, DRUs, and FOAs.  MAJCOMs, DRUs, and FOAs will: 2.11.1.  Develop  T&E  guidance,  procedures,  and  MOAs  to  supplement  this  AFI.    Forward draft copies to AF/TEP and SAF/AQXA Workflow addresses for review prior to publication.  Ensures systems engineering considerations, as identified by the Program Office, (including, but  not  limited  to  environment,  safety,  and  occupational  health;  human  systems  integration (HSI);  maintenance/sustaining  engineering;  product  and  system  integrity;  and  software engineering)  are  addressed  in  all  ICDs,  CDDs,  CPDs,  and  DCRs  as  appropriate.  The  lead command will advocate for and carry out T&E responsibilities for assigned weapon systems during  their  life  cycle  IAW  AFPD  10-9,  Lead  Command  Designation  and  Responsibilities for Weapon Systems. (T-1) 2.11.2.  Perform the responsibilities in paragraphs 2.11.3 through  2.11.16 when designated the OTO according to Figure 4.3. (T-1) 2.11.3.  Collaborate  with  requirements  sponsors  and  system  developers  to  execute  the development, testing, and fielding of Air Force systems and subsystems.  Develop clear and testable operational requirements and approved enabling and operating concepts prior to MS B.    Keep  these  documents  current  to  support  the  most  current  phases  of  T&E.    See paragraph 2.7.1.  Participate in HPTs, ITTs, and TIPTs as necessary to help ensure program success. (T-1)  AFI99-103  6 APRIL 2017 21 2.11.4.  Review and coordinate on T&E-related documentation impacting MAJCOM systems under test. (T-1) 2.11.5.  Oversee  the  T&E  policies  and  activities  of  assigned  T&E  organizations  to  ensure compliance with HQ USAF, OSD, and MAJCOM T&E policies. (T-1) 2.11.6.  Advocate for test resources. (T-1) 2.11.7.  Ensure appropriate and adequate T&E training is provided for personnel involved in T&E activities. (T-1) 2.11.8.  Provide support for the OSD-sponsored JT&E Program  and joint  test projects  IAW AFI 99-106 and the approved Test Resource Plan (TRP). (T-1) 2.11.9.  Ensure operational testing (e.g., Operational Assessments (OAs), Operational Utility Evaluations  (OUEs),  and  FDEs)  is  planned,  conducted,  and  results  reported  for  assigned systems  and  programs  when  AFOTEC  is  not  involved  according  to  paragraphs  4.4.7  and  4.6. (T-1) 2.11.10.  Support AFOTEC-conducted OT&E as agreed by the ITT, TIPTs, and documented in TRPs and TEMPs. (T-1) 2.11.11.  Continue operational testing of acquisition programs according to paragraphs 3.5.4 through    3.5.11,  and    4.6.    When  applicable,  provide  information  to  DOT&E  according  to paragraphs 4.7,  4.11.4.2,  6.6,  6.7,  7.4, and Attachment 2, Information Requirements for OSD T&E Oversight Programs. (T-0) 2.11.12.  Coordinate  fielding  recommendations  and  fielding  decisions  with  the  system  PM and OTO to support full rate production decisions (FRP). (T-1) 2.11.13.  Support PMs, working with the CDT/TM with the process to certify systems ready for dedicated operational testing IAW AFMAN 63-119. (T-1) 2.11.14.  Identify  and  report  DRs  IAW  TO  00-35D-54,  USAF  Deficiency  Reporting, Investigation, and Resolution.  Monitor open DRs from earlier testing. (T-0) 2.11.15.  Conduct  Tactics  Development  &  Evaluations  (TD&E)  and  Weapons  System Evaluation Program (WSEP) to characterize and/or enhance operational capabilities. (T-1) 2.11.16.  Request AFOTEC assistance and/or involvement as needed. (T-1) 2.12.  Air Force Operational Test and Evaluation Center (AFOTEC).  AFOTEC will: 2.12.1.  Develop  AFOTEC  guidance,  procedures,  and  MOAs  for  operational  testing  to supplement this AFI.  Forward draft copies to AF/TEP Workflow and SAF/AQXS Workflow prior to publication. (T-1) 2.12.2.  Carry  out  the  responsibilities  of  the  Air  Force  independent  OTA  described  in  Air Force  Mission  Directive  (AFMD)  14,  Air  Force  Operational  Test  and  Evaluation  Center (AFOTEC), and DoDD 5000.01.  (T-0) 2.12.3.  As  the  Air  Force  OTA  for  programs  as  determined  in  paragraph  4.6,  monitor  Air Force  acquisition  programs  for  operational  test  applicability,  and  provide  formal  notice  of AFOTEC  involvement  to  program  stakeholders  when  warranted.    Provide  timely  responses  22 AFI99-103  6 APRIL 2017 and  inputs  to  support  program  schedules.    Function  as  the  lead  OTA  for  multi-Service programs when designated. (T-1) 2.12.4.  Program  for  AFOTEC-conducted  T&E  activities  and  list  costs,  schedules,  and resources  in  test  resource  plans  (TRP).    Coordinate  AF  portion  of  Multi-Service  OT&E resources  where  the  AF  is  not  the  Lead  OTA.    Coordinate  TRPs  with  supporting organizations in  sufficient time for funds and personnel  to  be budgeted during the Program Objective Memorandum (POM) cycle.  (T-1) 2.12.5.  Generate OA and dedicated OT reports to support key acquisition decisions. 2.13.  United  States  Air  Force  Warfare  Center  (USAFWC).  The  USAFWC  will  exercise “coordinating authority” for operational testing as defined in the USAFWC Charter as follows: 2.13.1.  Initiate  dialogue  and  close  collaboration  with  MAJCOMs  to  ensure  priorities  for operational testing are synchronized and candidates for collaborative testing are identified. 2.13.2.  Coordinate  with  and  support  AFOTEC-conducted  operational  testing  for  weapon systems’ initial acquisition and fielding decisions as requested. 2.13.3.  Identify and help eliminate redundant operational test activities. 2.13.4.  Sponsor,  oversee,  and  execute  comprehensive  integrated  warfighting/cross-domain T&E activities to enhance operational capabilities. 2.14.  Operational  Test  Organizations  (OTO).  AFOTEC  and  other  OTOs  as  determined  in paragraph 4.6 will: 2.14.1.  Help form and co-chair (with the CDT or TM, as appropriate) ITTs for programs as determined  in  paragraph  4.6.    The  ITT  must  be  formed  as  early  as  possible,  preferably immediately after MDD according to paragraphs 2.16.3 and  4.4. (T-1) 2.14.2.  Participate  in  HPTs  as  necessary  to  ensure  testability  of  capability  requirements attributes (i.e. KPPs, Key  System  Attributes  (KSA), and Additional  Performance Attributes (APA)).  Assist in development of capability requirements documents (i.e. ICD, CDD, CPD) and  enabling  and  operating  concepts,  Courses  of  Action  (COAs),  and  Analyses  of Alternatives (AoAs). (T-1) 2.14.3.  Participate  in  preparation  of  strategies  for  T&E  and  test  plans  that  are  integrated.  Prepare  the  OT&E  portions  of  the  TEMP  and  coordinate  OT  strategy  inputs  with OSD/DOT&E for ACAT-ID and OSD-oversight programs. (T-0) 2.14.4.  Collaborate with other OTOs and AF/TEP to ensure operational testing is conducted by the appropriate test organization(s) according to paragraph 4.6. (T-1) 2.14.5.  Provide  independent  operational  testing  expertise  and  level  of  support  to  FDEs  as negotiated. (T-1) 2.14.6.  Plan  and  conduct  operational  testing  in  support  of  Air  Force-sponsored  rapid acquisition  programs,  Quick  Reaction  Capabilities  (QRCs),  and  Urgent  Operational  Needs (UONs).  See paragraph 4.17. (T-1) 2.14.7.  Use  approved  CONOPs/Operating  Concepts,  Mission  Profiles,  etc.  along  with validated capability requirements attributes (KPP, KSA, and APAs) as the primary source of evaluation criteria.  Report results as directed in Chapter 7. (T-1)  AFI99-103  6 APRIL 2017 23 2.14.8.  Determine  the  quantity  of  test  articles  required  for  OT&E  in  consultation  with  the MAJCOM and the PM. (T-0) 2.14.9.  Participate  in  the  certification  of  readiness  for  dedicated  operational  testing  IAW AFMAN 63-119. (T-1) 2.14.10.  Identify,  validate,  submit, enhancements IAW TO 00-35D-54.  (T-0) track,  and  prioritize  system  deficiencies  and 2.14.11.  Mark  and  handle  cybersecurity  vulnerabilities  according  to  appropriate  security classification guidance. (T-1) 2.14.12.  Maintain a qualified OT&E workforce. (T-1) 2.14.13.  Ensure  T&E  training  is  provided  for  personnel  involved  in  operational  test activities. (T-1) 2.14.14.  Submit  significant  test  event  reports  to  the  appropriate  agencies  (e.g.,  PM,  CDT, TM, LDTO, Participating Test Organizations (PTOs), operational MAJCOM, PEM, Program Executive Officer (PEO), Center Test Functional leaders, AF/TE, and/or DOT&E). (T-1) 2.15.  Program Executive Officer (PEO).  The PEO will: 2.15.1.  Assist  the  PM  and  ITT  in  identifying  key  government  DT&E  organizations  and personnel,  to  include  LDTO  candidates,  CDTs,  and  TMs  as  soon  as  possible  after  MDD according to paragraphs 4.4 and  4.5. 2.15.2.  Approve LDTO. 2.15.3.  Act  as  final  field-level  approval  authority  prior  to  forwarding  TEMPs  to  SAF/AQ and AF/TE for final Air Force coordination and approval and approve TEMPs when assigned as MDA and program is not on OSD oversight.  See paragraph 5.14.2. 2.15.4.  Act as the OT&E Certification Official for delegated programs according to AFMAN 63-119 and paragraph 6.5 of this AFI. 2.16.  Program Managers (PM).  The PM (or designated T&E representative) will: 2.16.1.  Appoint a T-coded CDT or TM to manage all DT&E for the program office. 2.16.2.  Determine  whether  the  assigned  program  is  on  DOT&E  oversight  and/or  on  the DASD(DT&E)  special list  and  adjust  program  manpower accordingly. interest  or  engagement 2.16.3.  Ensure  that  CDT/TM  forms  an  ITT  with  the  selected  lead  OTO  immediately  after MDD, according to paragraphs  1.4 and  4.4. 2.16.4.  Ensure  CDT  or  TM  leads  development  of  the  ITT  charter  and  coordinate  with stakeholder organizations. 2.16.5.  Ensure an LDTO is selected and designated as early as possible (i.e., at or before MS A) according to paragraphs 4.4 and  4.5.  Determine the scope of DT&E needed throughout the project or program life cycle IAW Chapters 4 and 5. 2.16.6.  Ensure  timely  government  access  to  contractor  and  other  T&E  data,  deficiency reporting  processes,  and  all  program  T&E  results  through  a  common  T&E  database (described in  paragraph  5.18.1) available to  program  stakeholders with  a need to  know  as  24 AFI99-103  6 APRIL 2017 determined  by  the  ITT.    Official  government  Deficiency  Reports  (DRs),  however,  must  be input into the Joint Deficiency Reporting System (JDRS). 2.16.7.  Direct the development of a strategy for T&E, TEMP, and developmental/integrated test  plans  in  support  of  the  program  requirements,  acquisition,  cyber  test  strategies  and  the PPP. 2.16.8.  Document and track all T&E related risks throughout the life cycle of the system. 2.16.9.  Regarding LFT&E, the PM or designated representative will: 2.16.9.1.  Ensure  systems  are  screened  and  correctly  designated  as  “covered  systems,” “major munitions programs,” or “covered product improvement programs” if required by 10 U.S.C. § 2366.  Note: these three terms are encompassed by the single term “covered system” in  the DAG.  Coordinate the proposed nominations with  AF/TEP and the PEO before obtaining SAF/AQ approval.  Forward approved nominations to DOT&E. 2.16.9.2.  Plan,  program,  and  budget  for  LFT&E  resources  if  the  system  is  a  “covered system”  or  “major  munitions  program”  to  include  test  articles,  facilities,  manpower, instrumented threats, and realistic targets. 2.16.9.3.  Identify  critical  LFT&E  issues.    Prepare  and  coordinate  required  LFT&E documentation  to  include  the  TEMP  and  LFT&E  strategy,  plans,  and  reports.    Review briefings  pertaining  to  the  System  Under  Test  (SUT)  before  forwarding  to  AF/TEP Workflow. 2.16.9.4.  Prepare  LFT&E  waiver  requests  and  legislative  relief  requests,  if  required,  to include an alternative plan for evaluating system vulnerability or lethality. 2.16.10.  Ensure plans for models and simulations created for T&E purposes are developed, documented  and  maintained  in  the  Modeling  and  Simulation  Support  Plan  IAW  AFI  63-101/20-101 and AFI 16-1005, Modeling & Simulation Management. 2.16.11.  As early as practical, direct the development of a cyber test strategy for pre-MS A through acquisition IAW AFI 63-101/20-101, DoDI 8500.01 and DoDI 5000.02.  The cyber test  strategy  will  support  requirements  for  authorization  IAW  DoDI  8510.01,  Risk Management Framework (RMF) for DoD Information Technology (IT), AFI 17-101, and AFI 63-101/20-101.  Define the cyber strategy for the weapons system; sufficient elements must be  incorporated  into  the  system  design  to  ensure  both  cybersecurity  and  cyber  resiliency.  Cybersecurity  is  determined  through  an  iterative  process  that  starts  with  supply  chain management  and  the  screening  of  personnel  tasked  to  develop  products,  continues  with system  design,  configuration,  and  development  that  incorporates  the  core  security  controls identified  by  NIST  Special  Publication  800-53,  along  with  countermeasures  to  protect systems  against  threats  in  cyber  contested  environments,  and  comprises  operational  and sustainment processes designed to minimize the introduction of malicious logic to the system and  resolve  vulnerabilities  found  during  testing.  A  successful  cyber  test  strategy  should include but is not limited to the following: 2.16.11.1.  Ensure resiliency requirements/objectives  to  test  measures  and  objectives  throughout  the  system’s  life cycle. cybersecurity traceability of and cyber  AFI99-103  6 APRIL 2017 25 2.16.11.2.  Identify  test  areas  that  overlap  RMF  Process  to  assess  cybersecurity  and authorize business and PIT systems. 2.16.11.3.  Documentation sufficient to support a system-of-systems approach to testing.  Documentation  should  provide  information  on  the  network/cyber  architecture  (major systems  and  subsystems,  interconnections  between  subsystems,  access  points,  and external  connections),  system  boundaries,  intended  operational  environment,  and  the anticipated cyber threat. 2.16.11.4.  Support for a cyber test strategy that includes a systematic mapping of mission dependence on cyber, using relevant data from all available sources, including contractor-developed  vulnerability information  security  assessments, inspections, component-and subsystem-level tests, system-of-system tests, and testing in an operational environment. identification  reports, 2.16.12.  Ensure  all  DT&E  (both  contractor  and  government)  is  conducted  according  to government-approved  test  plans  and  other  program  documentation.    Ensure  the  TEMP, Acquisition Strategy, SEP, ISP, and PPP are synchronized and mutually supporting. 2.16.13.  Assist OTOs in determining the resources and schedule for operational testing and reporting. 2.16.14.  Ensure  operational  test  and  evaluation  is  conducted  for  all  acquisition  or sustainment  programs  requiring  a  FRP  or  fielding/deployment  decision  (full  or  partial capability) according to paragraph 3.5. 2.16.15.  Plan for test and evaluation of Integrated Product Support Elements throughout the system life cycle IAW AFI 63-101/20-101. 2.16.16.  Ensure  formation  of  TIPTs,  such  as  the  Material  Improvement  Program  Review Board (MIPRB) and the Joint Reliability and Maintainability Evaluation Team (JRMET), to track and resolve deficiencies.  See paragraph 5.19. 2.16.17.  Ensure  all  stores  are  certified  IAW  AFI  63-104,  The  SEEK  EAGLE  Program.    If assistance  is  needed,  contact  the  Air  Force  SEEK  EAGLE  Office.   Hazards  of Electromagnetic Radiation to Ordnance (HERO) criteria must be considered IAW AFMAN 91-201, Explosives Safety Standards. 2.16.18.  Resource and support development of the test strategy IAW AFI 65-601, Vol 1. 2.16.19.  Track,  evaluate,  and  take  appropriate  actions  on  DRs  IAW  Technical  Order  (TO) 00-35D-54,  USAF  Deficiency  Reporting,  Investigation,  and  Resolution,  DoDI  8510.01,  and AFI 63-145, Manufacturing and Quality Management.  Continue supporting DR evaluation and resolution during operational testing and system sustainment. 2.16.20.  Work with ITT to determine and document security classification of cyber test data. 2.16.21.  Implement an effective system certification process for operational testing as early as  practical.    Inform  the  OT&E  Certifying  Official  that  the  system  is  ready  for  dedicated operational testing according to paragraph 6.5 and AFMAN 63-119. 2.16.22.  Secure specialized T&E capabilities, resources, and instrumentation, based on ITT recommendations,  to  support  T&E  throughout  the  system  life  cycle.    See  DASD(DT&E)’s guide, Incorporating Test and Evaluation into Department of Defense Acquisition Contracts,  26 AFI99-103  6 APRIL 2017 on how to secure contractor support in RFPs, statements of objectives (SOO), and statements of work (SOW). 2.16.23.  PMs will ensure that environmental reviews have been accomplished as required by AFI  32-7061,  The  Environmental  Impact  Analysis  Process,  and  32  Code  of  Federal Regulations (CFR) Part 989.  Coordinate with the ITT and LDTO to identify required T&E activities for inclusion in the program’s NEPA/E.O. 12114 Compliance Schedule per DoDI 5000.02.  Appropriate parts should be referenced in the test plan. 2.17.  Chief  Developmental  Tester  (CDT),  Test  Manager  (TM).  All  MDAPs  and  MAIS programs are required to have a CDT IAW 10 U.S.C. § 139b.  This person must be appropriately qualified IAW AFI 63-101/20-101, AFI 36-1301, and USD(AT&L) KLP qualification standards. The CDT reports to the PM.  The CDT will be in a Defense Acquisition Workforce Improvement Act  (DAWIA)  T&E  acquisition-coded  position  designated  as  a  KLP.    A  CDT/TM  will  be designated for all ACAT  II programs  and below.   ACAT II and below CDTs/TMs will be in  a DAWIA T&E Coded position but are not required to be designated as a KLP. For non-MDAP or MAIS programs, a TM can fulfill the functions of a CDT.  The CDT, or TM as applicable will: 2.17.1.  Coordinate the planning, management,  and oversight  of all DT&E activities for the program. 2.17.2.  Maintain  oversight  of  program  contractor  T&E  activities  and  the  T&E  activities  of PTO supporting the program. 2.17.2.1.  Work with the LDTO to determine when contractors require LDTO oversight. 2.17.3.  Advise the PM on test issues and responsibilities listed in paragraph 2.16 and help the  PM  make  technically  informed,  objective  judgments  about  government  and  contractor DT&E results. 2.17.4.  Provide program guidance to the LDTO and the ITT. 2.17.5.  Inform  the  PM  if  the  program  is  placed  on  the  OSD  T&E  Oversight  for  DT&E, OT&E, or LFT&E. 2.17.6.  Participate  with  LDTO  in  the  Preliminary  Design  Review  (PDR),  Critical  Design Review  (CDR),  Operational  Test  Readiness  Review  (OTRR),  and  Test  Readiness  Review (TRR). The CDT/TM chairs the government DT&E TRR. 2.17.7.  Chair the ITT with the OTO. 2.17.8.  Coordinate the development of the strategy for T&E, TEMP, cyber test strategy, and other  T&E  documentation  IAW  the  DoD  5000-series,  DoDI  8500.01,  AFI  63-101/20-101, and this AFI. 2.17.9.  Ensure  the  TEMP  incorporates  cyber  test  requirements  as  derived  from  the Cybersecurity Strategy throughout all phases of program development. 2.17.9.1.  Ensure the test requirements for system cybersecurity and cyber resiliency are complete and testable. 2.17.10.  Develop  and  collaborate  Critical  Technical  Parameters  (CTPs)  with  the  Chief Engineer, and coordinate with the ITT for inclusion in the TEMP.  AFI99-103  6 APRIL 2017 27 2.17.11.  Review  and  approve  Contractor  Developmental  Test  Plans  with  the  assistance  of the LDTO and ITT. 2.17.12.  Assist the Chief Engineer when assessing the technological maturity and integration risk of critical technologies. 2.17.13.  Coordinate  with  the  program  Chief  Engineer  and  test  organizations  to  identify required technical and safety reviews. 2.18.  Lead  Developmental  Test  and  Evaluation  Organization  (LDTO).  The  LDTO functions as the lead integrator for a program’s DT&E activities.  The LDTO (or alternate LDTO described in paragraph 4.5.4) is separate from the program office, but supports the PM and ITT through  the  CDT/TM  in  a  provider-customer  relationship  with  regard  to  the  scope,  type,  and conduct of required DT&E. The LDTO may designate a sub organization, such as an Executing Test Organization (ETO) or PTO, to conduct the test with LDTO oversight.   Exception:  Due to the  long  established  structure  and  limited  pool  of  highly  specialized  technical  knowledge  in space  systems  acquisition,  a  different  LDTO  construct  is  authorized.    The  PEO  for  Space  may approve the use of an internal LDTO, provided it is within a separate three-letter division from the segment three-letter program offices. The LDTO will: (Note: paragraphs 2.18.1 through 2.18.3 implement 10 U.S.C. §139b and USD(AT&L) guidance specifically for MDAPs and MAIS programs.)    2.18.1.  Provide technical expertise on DT&E matters to the program’s CDT or TM. (T-0) 2.18.1.1.  Assist  the  CDT/TM  and  the  requirements,  acquisition,  and  cyber  test communities in developing studies, analyses, and program documentation IAW AFI 10-601, AFI 63-101/20-101, and AFI 17-101. (T-1) 2.18.1.2.  Participate in ITTs as they are being formed and assist TIPTs as required. (T-1) 2.18.2.  Conduct DT&E activities as directed by the program’s CDT/TM. (T-0) 2.18.2.1.  Plan,  manage,  and/or  conduct  government  DT&E,  LFT&E,  and  integrated testing according to the strategy for T&E, the TEMP, and DT&E and LFT&E strategies and plans. (T-1) 2.18.2.2.  Collaborate  with  the  CDT/TM  to  establish,  coordinate,  and  oversee  a confederation  of  government  DT&E  organizations  that  plan  and  conduct  DT&E according to the TEMP. (T-1) 2.18.2.3.  Oversee contractor DT&E as directed by the CDT/TM. (T-0) 2.18.2.4.  Assist the CDT/TM in reaching technically informed and objective judgments about contractor DT&E results. (T-0) 2.18.2.5.  Conduct or oversee cyber tests in support of the cyber test strategy as directed by the CDT/TM. (T-1) 2.18.2.6.  Accomplish  independent  Technical,  Environmental  and  Safety  Reviews.    All test  organizations  must  establish  procedures  for  when  and  how  these  reviews  are accomplished. (T-1) 2.18.3.  Provide reports and assessments with objective, accurate and defensible information to make informed acquisition decisions. (T-0)  28 AFI99-103  6 APRIL 2017 2.18.3.1.  Report, validate, and initially prioritize DRs IAW TO 00-35D-54. (T-1) 2.18.3.2.  Provide government DT&E results and final reports to the PM, PEO, and other stakeholders  in  support  of  decision  reviews  and  certification  of  readiness  for  dedicated operational testing.  Provide results and reports to the program’s common T&E database (see paragraph 5.18).  (T-0) 2.18.3.3.  Provide the CDT/TM an LDTO Quarterly Assessment Report. This assessment is  intended  to  provide  a  snapshot  of  a  program’s  progress  based  on  available  data  and open  a  direct  avenue  to  communicate  a  program’s  progress  towards  delivering  the required  capability.    Report  instructions  and  format  are  found  in  the  AF  T&E  Guide.  Provide  an  informational  copy  to  AF/TEP  and  the  appropriate  MAJCOM  elements (AFMC/A3F, AFSPC/A5X and test functional leaders as directed). (T-0) 2.19.  Participating Test Organizations (PTO).  PTOs will: 2.19.1.  Participate in ITTs and TIPTs as requested by the CDT/TM, LDTO, ETO, OTO, and other ITT members. (T-1) 2.19.2.  Assist other test organizations as described in TEMPs, test plans, and other program documentation. (T-1) 2.19.3.  Mark  and  handle  cybersecurity  vulnerabilities  according  to  appropriate  security classification. (T-1) 2.20.  Integrated Test Team (ITT).  The ITT will: 2.20.1.  Develop  and  manage  the  strategy  for  T&E  and  test  plans  that  are  integrated  to effectively support the requirements, acquisition, cyber, and sustainment strategies.  A single ITT may cover multiple related programs such as systems of systems.  PMs should not have multiple project-level ITTs within a program, but should create focused subgroups that report to the ITT.  New programs should consider using an existing ITT’s expertise to ensure more efficient startup. 2.20.2.  Develop and implement an ITT charter according to paragraph 4.4.  Recommended member  organizations  are  listed  in  paragraph  4.4.4.    Coordinate  updates  to  the  charter  as program changes warrant.  Note: During Material Solution Analysis or early TMRR phase, provisional or temporary ITT representatives may be required to initiate the processes cited in paragraph 4.4. 2.20.3.  Recommend an LDTO to the PM for PEO approval according to paragraph 4.5. 2.20.4.  Direct  formation  of  subgroups  (e.g.,  integrated  product  teams  (IPT))  as  needed  to address T&E data analysis, problem solving, test planning, and to coordinate test, execution, and reporting. 2.20.5.  Assist  in  establishing  test  teams  to  conduct  integrated  testing,  to  include  integrated warfighting and cross-domain T&E. 2.20.6.  Develop  the  strategy  for  T&E,  TEMP,  LCSP,  and  other  T&E  documentation  IAW the DoD 5000-series, AFI 63-101/20-101, and this AFI. 2.20.7.  Assist in  developing  applicable  requirements documents,  enabling  and  operating  concepts,  and  architectures  as  described  in  CJCSI the  requirements  community  AFI99-103  6 APRIL 2017 29 3170.01, the  AF/A5R  Requirements  Development  Guidebooks,  and  AFI  17-140, Implementing Air Force Architectures.  For DBS programs, also reference AFMAN 63-144, Defense  Business  System  Life  Cycle  Management,  and  AFMAN  33-402,  Service Development and Delivery Process (SDDP). 2.20.8.  Develop  cyber  test  strategy  IAW  DoDI  8500.01,  DoDI  8510.01,  Risk  Management Framework  (RMF)  for  DoD  Information  Technology  (IT),  DoDI  5000.02,  AFI  17-101,  and this  AFI.    For  information  systems  containing  SAP  information,  refer  to  Department  of Defense (DoD) Joint Special Access Program (SAP) Implementation Guide (JSIG). 2.20.9.  Ensure interoperability testing is planned IAW DoDI 8330.01 CJCSI 5123.01G, and Air  Force  Chief  Information  Officer  (CIO)  Guidance  Memo  (AFGM2015-33-03),  for Interoperability  and  Supportability  of  Information  Technology  (IT)  and  National  Security Systems (NSS). 2.20.10.  Review  program’s  Information  Support  Plan  (ISP)  via  the  formal  ISP  staffing process,  to  ensure  T&E  data  is  consistent  with  the  TEMP  and  other  applicable  T&E documentation. 2.20.11.  Plan for a common T&E database for the program according to paragraph 5.18. 2.20.12.  Assist  the  acquisition  community  in  developing  studies,  analyses,  documentation, strategies, contractual documents, and plans. 2.20.13.  Ensure test teams report, validate, and prioritize DRs IAW TO 00-35D-54, AFI 63-145, DoDI 8510.01, and AFIs 17-101 and 63-101/20-101.  See paragraphs 5.19 and  5.20. 2.20.14.  Review  and  provide  inputs  to  contractual  documents  to  ensure  they  address government testing needs according to  paragraph 5.3; additional information can be found in  DASD(DT&E)’s  guide,  Incorporating  Test  and  Evaluation  into  Department  of  Defense Acquisition Contracts.   2.20.15.  Monitor contractor DT&E and the activities of all T&E members. 2.20.16.  Identify T&E resource requirements,  including acquisition of test items,  necessary facility upgrades, and personnel. 2.20.17.  Ensure that all T&E activities comply with AFPD 16-6, International Arms Control and  Non-Proliferation  Agreements  and  the  DoD  Foreign  Clearance  Program.    If  required, coordinate with SAF/GCI and AF/A3S. 2.20.18.  Outline  which  T&E-related  records  will  be  retained  and/or  forwarded  to  the Defense  Technical  Information  Center  (DTIC)  and  other  repositories  according  to paragraph 5.18.2, AFMAN 33-363, and AFRIMS. 2.20.19.  Develop a distribution list for all DT&E reports which includes operational testers, PTOs, PEO, applicable MAJCOMs, Center Test Functional Leaders, AF/TE, and DTIC.  30 AFI99-103  6 APRIL 2017 Chapter 3 TYPES OF TEST AND EVALUATION 3.1.  Major  Categories  of  Test  &  Evaluation.  Air  Force  testing  falls  into  two  overarching categories,  developmental  testing  and  operational  testing.    If  a  specific  T&E  requirement  does not fall precisely into one of the following discrete categories of testing, consult with AF/TEP to select and tailor the type of testing that best fits the need. 3.2.  Developmental  Test  &  Evaluation.  Developmental  testing  is  conducted  throughout  the acquisition and sustainment processes to assist engineering design and development, and verify that  CTPs  have  been  achieved.    DT&E  supports  the  development  and  demonstration  of  new materiel  solutions  or  operational  capabilities  as  early  as  possible  in  the  acquisition  life  cycle.  After FRP/Full Deployment (FD) or fielding, DT&E supports the sustainment and modernization of  systems.    To  support  integrated  testing,  as  many  test  activities  as  practical  are  conducted  in operationally  relevant  environments  without  compromising  engineering  integrity,  safety,  or security.  Developmental testing leads to and supports a certification that the system is ready for dedicated  operational  testing  IAW  DoDI  5000.02,  Enclosure  5,  and  AFMAN  63-119, Certification  of  System  Readiness   In  addition, developmental testing: for  Dedicated  Operational  Testing. 3.2.1.  Assesses  the  technological  capabilities  of  systems  or  concepts  in  support  of requirements  activities  described  in  the  AF/A5R  Requirements  Development  Guidebooks (e.g.,  courses  of  action  (COA)).    Conducts  research,  development,  test,  and  evaluation (RDT&E)  to  investigate  new  concepts  and  technologies  and  collect  basic  scientific  and engineering data. 3.2.2.  Provides empirical data for cost, schedule, and performance trade-offs. 3.2.3.  Uses  M&S  tools  and  digital  system  models  (DSM);  evaluates  M&S  tools  for applicability;  and  performs  verification  and  validation  with  actual  test  data  to  support accreditation of M&S tools. 3.2.4.  Identifies and helps resolve deficiencies and vulnerabilities as early as possible. 3.2.5.  Verifies the extent to which design risks have been minimized. 3.2.6.  Verifies compliance with specifications, standards, and contracts. 3.2.7.  Characterizes system performance and military utility. 3.2.8.  Assesses quality and reliability of systems. 3.2.9.  Quantifies manufacturing quality and contract technical performance. 3.2.10.  Determines  fielded  system  performance  against  changing  operational  requirements and threats. 3.2.11.  Ensures  all  new  developments,  modifications,  upgrades,  sustainment  equipment, support equipment,  commodity replacement studies and demonstrations address  operational safety,  suitability,  and  effectiveness  (OSS&E);  security;  cybersecurity  and  cyber  resiliency; environment, safety, and occupational health integration; and HSI IAW AFI 63-101/20-101 and AFMCI 63-1201.  AFI99-103  6 APRIL 2017 31 3.2.12.  Supports  aging  and surveillance programs, value engineering projects,  productivity, reliability,  availability  and  maintainability  projects,  technology  insertions,  and  other modifications  IAW  AFI  63-101/20-101,  and  Air  Force  Pamphlet  (AFPAM)  63-128, Integrated Life Cycle Management. 3.2.13.  Uses various appropriations of funding  depending on the nature and purpose of the work and the type of testing required.   For specific funding  guidance, see DoD 7000.14-R, Department of Defense Financial Management Regulation (FMR), Vol 2A and AFI 65-601, Budget Guidance and Procedures, Vol 1. 3.3.  Types  of Developmental Testing.  This AFI does not attempt to prescribe an all-inclusive list of developmental test types.  The potential exists for several developmental testing types to overlap.  The types of DT&E must be described in the TEMP and test plans to facilitate planning and  coordination  for  integrated  testing.    The  following  general  DT&E  types  exist  for  many acquisition programs: 3.3.1.  Qualification  Test  and  Evaluation  (QT&E).  QT&E  is  a  tailored  type  of  DT&E performed  by  the  LDTO  primarily  for  commercial-off-the-shelf  (COTS)  items,  non-developmental  items  (NDI),  and  government  furnished  equipment  (GFE).    For  Defense Business  Systems  (DBS)  and  IT  systems,  QT&E  validates  the  product  integrates  into  the intended  environment,  meets  documented  functional,  non-functional,  and  cybersecurity assurance  requirements  and  performance  standards.  QT&E  includes  the  following  test segments:    System  Integration  Test  (SIT),  Data  Management  Evaluation  (DME),  System Operability Evaluation (SOE), Performance Evaluation Test (PET), Cybersecurity Evaluation (CSE),  Regression  Test  (RT),  and  User  Evaluation  Test  (UET).    Depending  on  user requirements, these and other items may require little or no government funded research and development (R&D), engineering, design, or integration efforts.   PMs  plan for and conduct T&E  of  COTS,  NDI,  and  GFE  even  when  these  items  come  from  pre-established  sources.  See paragraph 5.12 for more information on COTS, NDI, and GFE.  Note: QT&E generally uses  procurement  (e.g.,  3010  [aircraft],  3020  [missiles],  or  3080 [other]),  or  operations  and maintenance (O&M) funds (i.e., 3400) IAW DoD 7000.14-R, Vol 2A, and AFI 65-601, Vol I. 3.3.2.  Production-Related  Testing.  The  PM  ensures  T&E  is  conducted  on  production items  to  demonstrate  that  specifications  and  performance-based  requirements  of  the procuring  contracts  have  been  fulfilled.    Defense  Contract  Management  Agency  (DCMA) personnel normally oversee this testing at the contractor’s facility.  Typical tests (defined in Attachment 1) include:  first article tests (FAT);  lot acceptance tests (LAT); pre-production qualification  tests  (PPQT);  production  qualification  tests  (PQT);  and  production  acceptance test  and  evaluation  (PAT&E).    Developmental  and  operational  testers  may  observe,  collect data, or participate during these tests as needed. 3.3.3.  Live Fire Test and Evaluation (LFT&E).  LFT&E is a type of DT&E that provides timely,  rigorous,  and  credible  vulnerability  or  lethality  test  and  evaluation  of  “covered” systems  as  they  progress  through  the  Engineering  and  Manufacturing  Development  (EMD) Phase  and  early  Production  and  Deployment  Phase  prior  to  FRP/FD,  or  a  major  system modification  that  affects  survivability.    Survivability  information  from  LFT&E  consists  of susceptibility, vulnerability, and recoverability information derived from the firing of actual weapons  (or  surrogates  if  actual  threat  weapons  are  not  available)  at  components,  sub- 32 AFI99-103  6 APRIL 2017 systems,  sub-assemblies,  and/or  full  up,  system-level  targets.    Modeling,  simulation,  and analysis must be an integral part of the LFT&E process.  The Air Force must initiate LFT&E programs sufficiently early to allow test results to impact system design prior to FRP/FD or major modification decisions.  See paragraph 5.8 for more information; Attachment 1 for key definitions; and 10 U.S.C. §2366.  The Air Force accomplishes LFT&E to: 3.3.3.1.  Provide  information  to  decision  makers  on  potential  user  casualties,  system vulnerabilities, lethality, and system recoverability while taking into equal consideration the susceptibility to attack and combat performance of the system. 3.3.3.2.  Ensure  system  fielding  decisions  include  an  evaluation  of  vulnerability  and lethality data under conditions that are as realistic as possible. 3.3.3.3.  Assess battle damage repair capabilities and issues.  While assessment of battle damage  repair  is  not  a  statutory  requirement  of  LFT&E,  test  officials  should  exploit opportunities to assess such capabilities whenever prudent and affordable. 3.4.  Operational Test.  Operational test determines the operational effectiveness and suitability of  the  systems  under  test.    It  determines  if  operational  capability  requirements  have  been satisfied and assesses system impacts to both peacetime and combat operations.  It identifies and helps resolve deficiencies as early as possible, identifies enhancements, and evaluates changes in system configurations that alter system performance.  Operational test includes a determination of  the  operational  impacts  of  fielding  and/or  employing  a  system  across  the  full  spectrum  of military operations and may be conducted throughout the system life cycle.  Operational test may also  evaluate  or  assess  doctrine,  organization,  training,  materiel,  leadership  and  education, personnel and facilities and the policy that affects the other seven elements (DOTMLPF-P). 3.5.  Types  of OT&E.  OT&E is the formal field test, under realistic combat conditions, of any item of (or key component of) weapons, equipment, or munitions for the purpose of determining the effectiveness  and suitability of that system  for use in  combat by typical  military users,  and the  evaluation  of  the  results  of  such  test.    The  types  of  operational  testing  listed  below  offer operational  testers  a  range  of  options  for  completing  their  mission.    “Evaluations”  collect, analyze, and report data against stated criteria with a high degree of analytical rigor and are used to inform FRP/FD decisions.  “Assessments” usually collect and analyze data with less analytical rigor,  need  not  report  against  stated  criteria,  and  cannot  be  the  sole  source  of  T&E  data  for FRP/FD decisions.  All programs that result in a FRP/FD decision require an appropriate type of operational testing supported by sufficient independent evaluation to inform that decision.  The ITT  recommends  an  appropriate  level  of  operational  T&E  to  the  MDA  and  T&E  oversight organizations (if applicable) for approval.  Operational testing of COTS, NDI, and GFE cannot be  omitted  simply  because  these  items  came  from  pre-established  sources.    Acquisitions  that support  sustainment,  to  include  acquisition  of  support  equipment  and  form,  fit,  function,  and interface  (F3I)  replacements,  require  FRP/FD  decisions  and  an  appropriate  type  of  operational testing.    Operational  testing  must  be  based  on  approved  operational  requirements  documents specifically for the capabilities being fielded; however, the OTO has the authority to test against expanded  operational  requirements  based  on  real-world  developments.    See  the  definition  of OT&E in Attachment 1 for further information. 3.5.1.  Initial  Operational  Test  and  Evaluation  (IOT&E).  IOT&E  is  the  primary dedicated  OT&E  of  a  system  before  FRP/FD  as  directed  by  DoDI  5000.02.    IOT&E determines  if  operational  requirements  and  critical  operational  issues  (COI)  have  been  AFI99-103  6 APRIL 2017 33 satisfied  and  assesses  system  impacts  to  peacetime  and  combat  operations.    Tests  are conducted  under  operational  conditions,  including  combat  mission  scenarios  that  are  as operationally realistic as possible.  A dedicated phase of IOT&E is required for new ACAT I and  II  programs  and  DOT&E  oversight  programs  IAW  DoDI  5000.02.    For  programs  on DOT&E oversight, IOT&E shall be conducted only by AFOTEC.  AFOTEC determines the operational effectiveness and operational suitability of the items under test using production or  production-representative  articles  with  stabilized  performance  and  operationally representative  personnel.   The  determination  of  appropriate  OTO  for  subsequent modifications  and  upgrades,  as  well  as  applicability  to  other  types  of  programs,  will  be accomplished according to paragraph 4.6 and Figure 4.3. 3.5.2.  Qualification  Operational  Test  and  Evaluation  (QOT&E).  QOT&E  is  a  tailored type  of  IOT&E  performed  on  systems  for  which  there  is  little  to  no  RDT&E-funded development  effort.    Conducted  only  by  AFOTEC,  QOT&E  is  used  to  evaluate  military-unique portions and applications of COTS, NDI, and GFE for military use in an operational environment.    QOT&E  supports  the  same  kinds  of  decisions  as  IOT&E.    See  paragraph 5.12 for more information on COTS, NDI, and GFE. 3.5.3.  Follow-on Operational Test and Evaluation (FOT&E).  FOT&E is the continuation of OT&E after IOT&E, QOT&E, or Multi-Service OT&E (MOT&E) and is conducted only by AFOTEC.    It  answers specific questions about  unresolved COIs and  test issues;  verifies the resolution of deficiencies or shortfalls determined to have substantial or severe impact(s) on mission operations; or completes T&E of those areas not finished during previous OT&E.  AFOTEC reports document known requirements for FOT&E.  More than one FOT&E may be  required.    Note:  FOT&E  that  follows  a  QOT&E  as  described  in  paragraph  3.5.2  is generally funded with procurement (3010, 3011, 3020, or 3080) or O&M (3400) funds, not RDT&E 3600 funds.  See paragraph 5.2 for T&E funding sources, and paragraph 5.22 for test deferrals, limitations, and waivers. 3.5.4.  Force  Development  Evaluation  (FDE).  FDE  is  a  type  of  dedicated  OT&E performed by MAJCOM OTOs in support of MAJCOM-managed system acquisition-related decisions  and  milestones  prior  to  initial  fielding,  or  for  subsequent  system  sustainment  or upgrade activities.  An FDE may be used for multiple purposes to include: 3.5.4.1.  Evaluate  and  verify  the  resolution  of  previously  identified  deficiencies  or shortfalls, including those rated in AFOTEC reports as not having a substantial or severe impact on mission operations. 3.5.4.2.  Evaluate  routine  software  modifications  (e.g.,  operational  flight  programs (OFP)),  subsequent  releases,  upgrades,  and  other  improvements  or  changes  made  to sustain or enhance the system. 3.5.4.3.  Evaluate  and  verify  correction  of  new  performance  shortfalls  discovered  after fielding of the system. 3.5.4.4.  Evaluate operational systems against foreign equipment. 3.5.4.5.  Evaluate operational systems against new or modified threats. 3.5.4.6.  Evaluate military-unique portions and applications of COTS, NDI, and GFE for military use.  34 AFI99-103  6 APRIL 2017 3.5.5.  Multi-Service  Operational  Test  and  Evaluation  (MOT&E).  MOT&E  is  OT&E (IOT&E, QOT&E, FOT&E, or FDE) conducted by two or more Service OTOs for systems acquired by more than one Service.  MOT&E is  conducted  IAW the T&E directives of the lead OTO, or as agreed in a memorandum of agreement between the participants.  Refer to the:    Memorandum  of  Agreement  on  Multi-Service  Operational  Test  and  Evaluation (MOT&E) and Operational Suitability Terminology and Definitions, April 2015 for guidance on  conduct,  execution,  and  reporting  of  an  MOT&E.    A  copy  of  the  MOT&E  MOA  is available by email if a request is sent to: "AFOTEC.A5A8.Workflow@us.af.mil.”  Also see paragraphs 4.6.6.4,  4.8 and  7.4.4 of this Instruction.  If MAJCOMs are involved in multi-Service testing without AFOTEC, they should use this MOA as a guide. 3.5.6.  Tactics  Development  and  Evaluation  (TD&E).  TD&E  is  a  type  of  operational testing conducted by MAJCOMs to refine doctrine, system capabilities, and TTPs throughout a  system’s  life  cycle  IAW  AFI  11-260,  Tactics  Development  Program.    TD&Es  normally identify  non-materiel  solutions  to  problems  or  evaluate  better  ways  to  use  new  or  existing systems. 3.5.7.  Operational  Utility  Evaluation  (OUE).  An  OUE  is  an  operational  test  which  may be  conducted  by  AFOTEC  or  MAJCOM  OTOs  whenever  a  dedicated  OT&E  event  is required,  but  the  full  scope  and  rigor  of  formal  IOT&E,  QOT&E,  FOT&E,  or  FDE  is  not appropriate or required IAW this AFI.  OUEs may be used to support operational decisions (e.g., fielding a system with less than full capability, to include but not limited to integrated testing  of  releases  and  increments  of  IT  capabilities)  or  acquisition-related  decisions  (e.g., low-rate  initial  production  (LRIP))  when  appropriate  throughout  the  system  life  cycle.    An OUE cannot support FRP or FD decisions for ACAT I, II, or oversight programs.  OTOs may establish their supplemental internal guidance on when and how to use OUEs.  Use of OUE or  FDE  to  support  MAJCOM-managed  acquisition  decisions  is  at  the  discretion  of  the appropriate MAJCOM staff or test organization. 3.5.8.  Operational  Assessment  (OA).  OAs  are  conducted  by  AFOTEC  or  MAJCOM OTOs  in  preparation  for  dedicated  operational  testing  and  typically  support  MS  C  or  LRIP decisions.  They are designed to be progress reports and not intended to determine the overall effectiveness  or  suitability  of  a  system.    They  provide  early  operational  data  and  feedback from actual testing to developers, users, and decision makers.  OAs also provide a progress report  on  the  system’s  readiness  for  IOT&E  or  FDE,  or  support  the  assessment  of  new technologies.    OAs  will  not  be  used  as  substitutes  for  IOT&E,  QOT&E,  FOT&E,  FDE,  or OUE.  OAs may be integrated with DT&E to: 3.5.8.1.  Assess  and  report  on  a  system’s  maturity  and  potential  to  meet  operational requirements during dedicated operational testing. 3.5.8.2.  Support long-lead, LRIP, or increments of acquisition programs. 3.5.8.3.  Identify  deficiencies  or  design  problems  that  can  impact  system  capability  to meet concepts of employment, concepts of operation or operational requirements. 3.5.8.4.  Uncover potential system changes needed which in turn may impact operational requirements, COIs, or the Acquisition Strategy.  AFI99-103  6 APRIL 2017 35 3.5.8.5.  Support the demonstration of prototypes, new technologies, or new applications of existing technologies, and demonstrate how well these systems meet mission needs or satisfy operational capability requirements. 3.5.8.6.  Support proof of concept initiatives. 3.5.8.7.  Augment or reduce the scope of dedicated operational testing. 3.5.9.  Early  Operational  Assessment  (EOA).  EOAs  are  similar  to  OAs,  except  they  are performed  prior  to  MS  B  to  provide  very  early  assessments  of  system  capabilities  and programmatic  risks.    Most  EOAs  are  reviews  of  existing  documentation,  but  some  may require hands-on involvement with prototype hardware and/or software. 3.5.10.  Military  Utility  Assessment  (MUA).  An  MUA  is  useful  for  MAJCOM  OT assessment  of  a  new  capability  and  how  well  it  addresses  the  stated  military  need  when  a formal  OA  or  OT&E  is  not  warranted  (non-oversight,  not  a  program  of  record,  etc.).    The assessment  should  characterize  the  military  utility  considering  all  operational  factors including maintainability. 3.5.11.  Sufficiency  of  Operational  Test  Review  (SOTR).  For  some  programs  of  limited scope  and  complexity,  system  development  testing  or  integrated  developmental  and operational  test  events  may  provide  adequate  test  data  to  support  MAJCOM  production  or fielding  decisions.    In  these  situations,  the  lowest  appropriate  level  of  required  operational testing may consist of a review of existing data rather than a separate, dedicated operational test event.  The ITT should recommend a SOTR when collected test data can address all test measures and result in effectiveness and suitability ratings.  A SOTR is not intended to be a cost or schedule-driven solution. 3.5.11.1.  The  SOTR  must  be  approved  by  MAJCOM  T&E  staff.    The  SOTR  may  be used  as  the  source  of  operational  test  information  for  supporting  fielding,  acquisition milestone, or production decisions.  See also paragraph 4.6.6.3.  The SOTR may not be used  for  milestone  decisions  associated  with  OSD  OT&E  Oversight  programs  unless approved by the Director, Operational Test and Evaluation (DOT&E). 3.5.11.2.  See  paragraph  7.4.5  for  reporting  SOTR  results,  and  the  Air  Force  T&E Guidebook for a comparison with the Capabilities and Limitations (C&L) report. 3.5.12.  Summary  of  Operational  Testing.  The  key  distinctions  between  types  of operational testing and the decisions they support are shown in Table 3.1.  Note: Table 3.1 is  intended  as  a  summary  and  may  not  cover  all  possible  T&E  situations;  refer  to  the descriptions in paragraph 3.5 or consult with AF/TEP for final guidance of any issues.    36 AFI99-103  6 APRIL 2017 Table 3.1.  Summary of Operational Testing Options. Types of          Operational Tests Assessments EOA OA MUA Evaluations  IOT&E  QOT&E  FOT&E  MOT&E Decisions Supported Who Conducts Types of Programs  MS B, CDD Validation, Development RFP Release Decision Point MS C/LRIP/LD   AFOTEC or MAJCOM OTO All (ACAT I-III, OSD T&E Oversight, Non-Oversight)  Note 1 New S&T application  MAJCOM OTO Non-Oversight, non-program of   record    FRP/FD AFOTEC ACAT I, IA, II, OSD T&E Oversight FRP/FD AFOTEC or MAJCOM OTO All FDE FRP/FD MAJCOM OTO All Note 2 All Note 3 Non-Oversight Note 3 OUE SOTR TD&E FRP/FD FRP/FD AFOTEC or MAJCOM OTO MAJCOM OTO TTP Documentation  MAJCOM OTO All Notes: 1. Cannot be substituted for I/Q/FOT&E, FDE, or OUE. 2. Do not use when I/Q/FOT&E are more appropriate. 3. Do not use when I/Q/FOT&E or FDE are more appropriate. 3.6.  Testing  of  Training  Devices.  Training devices should be considered part of the SUT and must also undergo DT and OT.  To ensure crew training devices provide accurate and credible training throughout their life cycles, AFI 36-2251, Management of Air Force Training Systems, gives  direction  and  guidance  for  using  the  simulator  certification  (SIMCERT)  and  simulator validation  (SIMVAL)  processes.    Specifically,  SIMCERT  and  SIMVAL  are  assessments  of training device effectiveness in accomplishing allocated tasks and provide a comparison of crew training device performance with the prime mission system.  SIMCERTs and SIMVALs support and complement the test of the training devices.  In addition, PMs must include training system concepts  and  requirements  in  all  acquisition  strategies.    They  must  ensure  training  systems  are fielded  concurrently  with  initial  prime  mission  system  fielding,  and  remain  current  throughout the weapon system life cycle IAW AFI 63-101/20-101.  See definitions in Attachment 1. 3.7.  Specialized  Types  of  Test  and  Evaluation.  Certain  types  of  T&E  require  test organizations to use specialized processes, techniques, requirements, and formats in addition to those prescribed in this AFI.  These specialized types of T&E must be integrated with other T&E activities as early as possible.  These tests often occur during DT&E and OT&E and may have the  characteristics  of  both.    They  are  often  done  concurrently  with  other  testing  to  conserve resources  and  shorten  schedules,  but  may  also  be  conducted  as  stand-alone  test  activities  if necessary.    These  tests  are  usually  conducted  in  operationally  relevant  environments  which  AFI99-103  6 APRIL 2017 37 include  end-to-end  scenarios.    Table  3.2  identifies  guidance  for  the  PM  to  use  in  planning, conducting, and reporting these specialized types of T&E. Table 3.2.  Specialized Types of T&E. Type of Testing Advanced Technology Demonstration (ATD) (Note 1) Technical Assurance Standards Testing Electronic Warfare Integrated Reprogramming (EWIR) Emission Security (EMSEC) Assessment Foreign Comparative Testing (FCT) (Note 1) Joint Capability Technology Demonstrations (JCTD)  (Note 1) Joint Interoperability Test and Certification Joint Test & Evaluation (JT&E) (Note 1) Testing of Urgent Needs  (Note 1) Unified Capabilities (UC) Certification  Description References Air Force Research Laboratory-funded, MAJCOM-sponsored development efforts that demonstrate the maturity and potential of advanced technologies for enhancing military operational capabilities. Evaluates offensive cyberspace operations capabilities against technical assurance standards. Process intended to produce and deliver software/hardware changes to electronic equipment used to provide awareness and response capability within the EM spectrum. May require changes in TTP, equipment employment guidance, aircrew training and training devices (threat simulators and emitters).  Provides guidance for test / fielding of mission data (MD) changes, OFP changes, or minor hardware changes that comply with the guidance in AFI 63-101/20-101 concerning modifications.  AFI 61-101, Management of Science and Technology DoDI O-3600.03, Technical Assurance Standard (TAS) for Computer Network Attack (CNA) Capabilities  AFI 10-703, Electronic Warfare (EW) Integrated Reprogramming Assesses against the requirement to control the compromise of classified electronic emissions. AFSSI 7700, Emissions Security,  AFSSI 7702, EMSEC Countermeasures Reviews FCT is an OSD-sponsored program for T&E of foreign nations’ systems, equipment, and technologies to determine their potential to satisfy validated United States operational requirements. Exploits maturing technologies to solve important military problems and to concurrently develop the associated CONOPS to permit the technologies to be fully exploited.  Emphasis is on tech assessment and integration rather than development.   Required certification for net-readiness prior to a system being placed into operation. Must be preceded by Air Force System Interoperability Testing (AFSIT), formal service-level testing to determine the degree to which AF systems which employ tactical data links conform to appropriate DoD MIL-STDs. Evaluates non-materiel capabilities and potential options for increasing joint military effectiveness.  Focus is on evaluating current equipment, organizations, threats, and doctrine in realistic environments.  JT&E projects are not acquisition programs. Quick reaction capability for satisfying near-term urgent warfighter needs. Certifies interoperability and information assurance for Unified Capabilities (defined as integration of voice, video, and/or data services delivered ubiquitously across a secure and highly available network infrastructure, independent of technology).  AFSPC appoints the Air Force UC test organization responsible for testing technologies meeting the definition. 10 U.S.C. § 2350a(g) OSD Comparative Technology Office Handbook (https://cto.acqcenter.com/) DoDI 5000.02, Operation of the Defense Acquisition System AFI 63-101/20-101, Integrated Life Cycle Management CJCSI 5123.01G, Charter of the Joint Requirements Oversight Council (JROC)  DoDI 8330.01, Interoperability of Information Technology (IT) and National Security Systems (NSS) DoDI 5010.41, Joint Test and Evaluation (JT&E) Program  AFI 99-106, Joint Test and Evaluation Program DoDI 5000.02, Operation of the Defense Acquisition System DoDI 8100.04, DoD Unified Capabilities AFMAN 17-1202, Collaboration Services and Voice Systems Management Notes:  1. Activity falls outside the traditional acquisition process; however, Air Force testers may be required to support the activity by providing T&E expertise in assessing the military utility of new technologies. 3.8.  Weapons  System  Evaluation  Program  (WSEP).  WSEP  is  a  MAJCOM-conducted  test program  that  provides  a  tailored  end-to-end  operational  evaluation  of  fielded  weapons  systems and their support systems using realistic combat  scenarios.  The evaluation should characterize system performance and TTPs against changing operational requirements and threats to support the  requirements  development  process.    WSEP  also  conducts  investigative  firings  to  revalidate capabilities or better understand munitions malfunctions.  38 AFI99-103  6 APRIL 2017 3.9.  Other Test Considerations. 3.9.1.  Test for Foreign Military Sales (FMS). 3.9.1.1.  IAW  Defense  Security  Cooperation  Agency  (DSCA)  5105.38-M  Security Assistance  Management  Manual  (SAMM)  and  AFI  63-101/20-101,  testing  associated with FMS acquisition shall meet the intent of  DoD regulations and other applicable USG procedures for conducting test  and  evaluation  activities, affording the foreign purchaser the same benefits and protection that apply to all DoD procurement efforts.  Per AFI 63-101/20-101, the government-to-government  agreement should specify any tailored  FMS implementation. 3.9.1.2.  Upon  receipt  of  a  Letter  of  Request  (LOR)  from  a  Foreign  Partner  (FP), AFLCMC's  or  SMC’s  Test  Functional  leaders  will  develop  and/or  oversee,  in consultation  with  an  LDTO,  the  early  case  Test  and  Evaluation  planning  for  DoD  and non-DoD  systems,  system  configurations,  or  system  integrations  in  support  of  FMS programs.    This  strategy  should,  at  a  minimum,  consider  any  necessary  developmental test  (flight  test,  M&S),  test  range(s),  infrastructure,  test  manpower,  resources,  and certifications  needed  for  appropriate  testing  of  the  system  to  be  delivered.    This preliminary test strategy should have sufficient technical fidelity to produce a rough order of magnitude estimated cost and period of performance to support a dedicated "Test" line on the Letter of Offer and Acceptance (LOA), if warranted.  The LOA is the government-to-government  agreement  that  identifies  the  defense  articles  and  services  the  USG proposes to sell to the FP. 3.9.1.3.  The purpose of AFLCMC's or SMC’s Test Functional leaders’ oversight of the early  Test  and  Evaluation  plan  is  to  help  ensure  system  performance  meets  customer expectations  of  military  utility  per  written  agreement.    A  detailed  test  plan  will  be required once the case is established to refine the actual test requirement and cost.  The “Test”  line  on  the  LOA  would  be  managed  by  the  TM  located  in  the  System  Program Office. 3.9.1.4.  Additional  T&E  should  be  planned  and  conducted  on  a  system  or  a  subsystem with  Defense  Exportability  Feature  (DEF)  to  ensure  AT  protection  measures  and  other CPI  or  technology  protection  measures  work  as  expected  per  DoDD  5200.47E,  Anti-Tamper (AT), and DoDI 5200.39. 3.9.2.  Cyber  Test.  Cyber  test  evaluates  and  characterizes  systems  and  sub-systems operating in the cyberspace domain, and the access pathways of such systems.  Cyberspace is defined as a domain characterized by the use of electronics and the electromagnetic spectrum to  store,  modify,  and  exchange  data  via  networked  systems  and  associated  physical infrastructures.  The primary objectives of cyber test are to evaluate a system’s cybersecurity and resilience to cyber threats to ultimately verify mission capability. 3.9.2.1.  Cyber  test  should  be  integrated  throughout  contractor  and  government  DT&E and  OT&E  and  executed  in  operationally  representative  cyberspace  environments.  DT&E and OT&E plans must be developed considering system architecture and all attack surfaces  (interfacing  and  embedded  systems,  services,  and  data  exchanges  that  may expose the system to potential cyber threats) through all applicable domains.  AFI99-103  6 APRIL 2017 39 3.9.2.2.  Cyberspace is a contested domain and provides the opportunity for asymmetric actions  that  generate  effects  across  the  physical  domains.  As  such,  an  adequate  test program must identify the risks to accomplishing the mission in a non-permissive cyber environment  based  on  inherent  vulnerabilities  and  known  threats.    Intelligence  support will  be  used  to  develop  requirements,  an  integrated  concept  of  operations  (CONOPS), and cyber test measures. 3.9.2.3.  Cybersecurity  test  focuses  on  identifying  system  cyber  vulnerabilities.    It  is scoped through assessing a system’s cyber boundary and risk to mission assurance.   Risk analysis,  at  a minimum,  should consider the threat  and threat  severity, the likelihood of discovery, likelihood of attack, and system impact.  Cybersecurity is evaluated based on the  Security  Assessment  Plan,  Program  Protection  Plan,  Information  Support  Plan,  and Risk  Management  Framework  artifacts.    Cybersecurity  testing  provides  the  data necessary  to  the  Authorizing  Official  (AO)  to  render  a  determination  of  risk  to  DoD operations and assets, individuals, other organizations, and the Nation from the operation and use of the system (DoDI 8510.01). 3.9.2.4.  Cyber  resiliency  testing  evaluates  a  system’s  ability  to  meet  operational requirements  while  under  cyber  attack.    Cyber  attack  is  defined  as  an  attack,  via cyberspace,  designed  to  infiltrate,  disrupt,  disable,  deceive,  destroy,  or  maliciously control a target within cyberspace or a physical system.  Cyber resiliency testing focuses on detection and reaction to a successful cyber attack and the continuity, recovery and the degree of restoration of data and system functionality. 3.9.2.5.  The  ITT  and  test  organizations  must  plan  for  appropriate  cyber  test  to  assess system vulnerabilities and mission impact.  If the ITT or test organization cannot comply with cyber test requirements, the ITT or test organization must document the limitations and rationale in the TEMP and test plans.  40 AFI99-103  6 APRIL 2017 Chapter 4 T&E ACTIVITIES SUPPORTING MILESTONE A DECISIONS 4.1.  Pre-MS A Tester Involvement.  The most important activities prior to and during Materiel Solution Analysis that support a MS A decision are shown in Figure 4.1.  This chapter describes testers’  roles  in  these  activities.    Testers  need  to  be  involved  in  multidisciplinary  teams performing  developmental  planning  activities.    They  must  ensure  that  appropriate  T&E information  is  provided  in  a  timely  manner  to  support  the  requirements  and  acquisition processes.  This chapter focuses on early team building, strategy development, and establishing baselines for managing T&E activities in this phase and beyond. Figure 4.1.  Integration of Requirements, Acquisition, and T&E Events Prior to MS A. 4.2.  Pre-MS A Tester Involvement in Requirements Development.  Tester involvement starts with  participation  in  the  requirements  process  described  in  the  AF/A5R  Requirements Development Guidebook, Volume 1, CJCSI 3170.01, and CJCSI 5123.01G.  As HPT members, the  CDT  along  with  the  developmental  and  operational  testers  support  development  of  the Requirements  Strategy  and  appropriate  requirements  documents  with  technical  and  operational expertise.  Air Force T&E organizations working with the CDT provide support to HPTs. (T-1)  Testers review Air Force operating and enabling concepts to fully understand how new systems   AFI99-103  6 APRIL 2017 41 will be employed and supported.  Testers use these documents to support the development of a strategy  for  T&E  and  development  of  test  inputs  to  RFPs.    Critically,  they  also  ensure  that capability  requirements  are  testable.    AF/TE,  AFOTEC,  and  MAJCOM  representatives participate in the Air Force requirements process. 4.3.  Pre-MS A Tester Involvement in the Acquisition Process.  At this time, a PM should be assigned  to  lead  and  fund  early  studies  and  collaborate  with  the  CDT  on  a  strategy  for  T&E.  Early  tester  involvement  helps  identify  planning  and  other  shortfalls  that  could  result  in increased  development,  operations,  and/or  life  cycle  costs.    The  CDT  must  ensure  that developmental  and  operational  testers  are  involved  in  the  collaborative  work  that  produces  the AoA  Study  Plan,  COAs,  AoA  Final  Report,  PPP,  Acquisition  Strategy,  Technology Development  Strategy  (TDS),  strategy  for  T&E,  TEMP,  LCSP,  cyber  test  strategy,  and  the definition  of  entrance  and  exit  criteria  for  developmental  and  operational  testing.    Pre-MS  A project or program documentation must address which test organizations will conduct DT&E and operational testing as determined from paragraphs 4.4,  4.5, and  4.6. 4.4.  Formation  of  the  ITT.  The  PM  establishes  an  ITT  immediately  after  the  MDD  to  help shape  the  acquisition  strategy  and  determine  test  requirements  for  T&E.    The  PM  assigns  a CDT/TM to chair and form the ITT.  See Figure 4.2 for notional ITT membership.  The ITT is a decision making body and its members must be empowered to speak for their organizations.  The ITT works together as a cross-functional team to map out the strategy for testing and evaluating a system.  All programs must have an ITT, but a single ITT can cover a number of closely related programs such as the modifications and upgrades embedded in a legacy aircraft program. 4.4.1.  ITT  Quick  Start.  Identifying  appropriate  ITT  organizational  membership  is  critical to  ensure  program  stability.    During  early  program  phases  (e.g.,  immediately  after  MDD), ITT member organizations must send empowered representatives to assist with requirements development, designing the strategy for T&E, recommending the LDTO and OTO, reviewing early documentation, developing an initial T&E resources estimate, and other appropriate test planning  activities  as  required.   The  program/project's  anticipated  LDTO  and  OT organizations will participate in such meetings and activities.  A representative from the Air Force  Test  Center  (AFTC)  or  SMC,  dependent  on  SUT,  will  assist  the  ITT  in  the development  of  initial  strategy  for  T&E  and  selection  of  the  most  appropriate  LDTO  to support the program test requirements. 4.4.2.  ITT Leadership.  The program office (or the program's initial cadre) takes the lead in forming  an  ITT  with  representatives  from  all  needed  disciplines.    As  the  program  office forms, the PM selects the CDT or TM to chair the ITT with the lead OTO’s test lead as co-chair.    If  the  CDT  position  is  vacant,  the  PM  will  assume  CDT  responsibilities  until  the position is filled.  Testers should be proactive in supporting ITT initial formation and goals even  though  they  may  not  be  formally  tasked  before  the  initial  MDD  ADM  is  signed.  Testers who contributed to the AoA plan or participated in the HPT should form the nucleus of the initial ITT. 4.4.3.  ITT  Charter.  The CDT/TM produces a formal, charter for approval by the PM and other  stakeholders  that  describes  ITT  membership,  responsibilities,  ITT  resources,  and  the products  for  which  the  ITT  is  responsible.    ITTs  may  function  at  two  levels:  an  Executive Level  consisting  of  O-6s  and  GS-15s  from  key  organizations;  and  a  Working  Group  Level consisting  of  organizations  needed   Organizational to  fulfill  specific  ITT tasks.  42 AFI99-103  6 APRIL 2017 representatives no higher than O-6 or GS-15 coordinate on and sign the ITT charter.  See the recommended ITT charter outline and guidance in the Air Force T&E Guidebook. 4.4.4.  ITT  Membership.  The  ITT  leadership  tailors  the  membership,  structure,  and protocols  as  necessary  to  help  ensure  program  success.    ITT  membership  (at  the  Executive Level  and Working Group  Level) may vary depending on program  needs.  The  ITT should include  expertise  from  organizations  such  as  the  program  office  (or  the  program's  initial cadre),  AFOTEC  and/or  MAJCOM  OTO  as  appropriate,  LDTO,  ETO,  and  other  DT&E organizations,  the  Center  Test  Functional  Leaders  and  engineering  function,  AF/TEP, AF/A3/5,  SAF/A6,  JITC,  OSD,  organizations  responsible  for  cyber  and  interoperability testing  including  Security  Controls  Assessors  (SCA),  System  Security  Engineers  (SSE), system  and  support  contractors,  developers,  lab  and  S&T  organizations,  intelligence, requirements  sponsors,  test  facilities,  and  other  stakeholders  as  needed  during  various  test program  phases.    Include  representatives  from  the  other  Services  if  testing  a  multi-Service program.    Also  include  the  implementing  command  headquarters  and  Air  Education  and Training Command, if required. 4.4.5.  ITTs  for  Interoperable  Systems.  If a system is dependent on the outcome of other acquisition programs, or must provide capabilities to other systems, those dependencies must be  detailed  in  the  acquisition  strategy  and  other  program  documentation.    The  ITT  charter should  reflect  those  dependencies  by  including  representatives  from  the  other  programs  as needed who can address interoperability testing requirements. 4.4.6.  Subgroups.  The  ITT  charter  should  direct  the  formation  of  subgroups  (e.g.,  TIPTs, Test  Data  Scoring  Boards  (TDSBs),  study  groups,  review  boards)  to  write  test  plans  and handle  specific  test  issues  as  needed.    These  subgroups  would  not  require  full  ITT participation.   A “test team” is  a group of testers and other experts who are responsible for specific test issues or carry out integrated testing according to specific test plans.  There may be multiple TIPTs and test teams associated with an ITT.  AFI99-103  6 APRIL 2017 43 Figure 4.2.  Integrated Test Team.  *May be MAJCOM operational test org if AFOTEC not OTO  4.4.7.  Operational  MAJCOM  Roles.  MAJCOM  operational  testers  are  required  to participate in the ITT at program inception when AFOTEC is not the lead OTO.  In this case, they must assume the ITT co-chair position and conduct required operational testing.  When AFOTEC is the lead OTO, MAJCOM operational testers should plan for transition of these responsibilities  according  to  paragraph  4.6.   TEMPs  must  reflect  this  transition.  Additionally, the MAJCOM provides operational users for the conduct of operational testing. 4.4.7.1.  The  MAJCOM  is  responsible  for  informing  the  ITT  how  the  SUT  will  be employed.  This is typically done through a CONOPS. 4.4.8.  Charter  Updates.  ITT  charters  are  reviewed  and  updated  after  each  major  decision review  to  ensure  testing  is  integrated  as  much  as  possible  within  statutory  and  regulatory guidelines.  Changes in  membership should reflect  the skills  required for each phase of the program.  The ITT’s responsibilities are described in paragraph 2.20. 4.4.9.  Integrated Testing and the TEMP.  After MDD, the ITT must begin integrating all T&E activities to include contractor testing.  The TEMP must outline how all testing will be   44 AFI99-103  6 APRIL 2017 integrated,  addressing  the  overall  evaluation  approach,  key  evaluation  measures,  and  the major  risks  or  limitations  to  completing  the  evaluations.    State  justification  for  any  testing that is not integrated.  The TEMP will also include the interfaces and interoperability with all other supporting/supported systems described in the system enabling and operating concepts, and  operational  architectures.    T&E  planners  must  develop  strategies  for  embedded  and stand-alone  IT  sub-systems  to  include  cyber  testing.    The  principles,  guidelines,  and strategies of the TEMP shall be reflected in all supporting documents and contracts with all stakeholders.  Refer to the DAG, for the recommended TEMP format (https://dag.dau.mil/).  For at https://www.my.af.mil/gcss-af/USAF/ep/browse.do?programId=t6925EC2D581C0FB5E044080020E329A9&channelPageId=s6925EC1355030FB5E044080020E329A9  and  the  DOT&E  TEMP  Guidebook:  http://www.dote.osd.mil/tempguide/index.html. the  AF/TE  TEMP  Guide additional found guidance, see:  4.5.  Determining  the  LDTO.  The  LDTO  is  the  lead  government  DT&E  organization responsible  for  a  program’s  DT&E  IAW  paragraph  2.18.    For  complex  programs,  the  LDTO may build a confederation of DT&E organizations with appropriate skill mixes by enlisting the support of other PTOs as needed.  The LDTO serves as the lead integrator and “single-face-to-the-customer,”  working  closely  with  the  program’s  CDT  or  TM  for  purposes  of  planning, executing  and  reporting  DT&E.    For  less  complex  programs,  the  LDTO  may  be  solely responsible for overseeing and/or conducting all or most of the relevant  DT&E.  In accordance with 10 U.S.C. §139b and DoDI 5000.02, all MDAPs and MAIS programs will be supported by a government DT&E organization serving as LDTO.  All other Air Force programs will select a government  DT  organization  as  LDTO  unless  an  alternate  organization  (only  possible  for  low programmatic  risk  ACAT  III  programs)  is  determined  to  be  the  best  course  of  action  and  is approved in  writing by the PEO IAW  paragraph 4.5.4.  DT may be accomplished by an ETO under LDTO oversight. 4.5.1.  LDTO Selection.  The ITT initiates selection of an LDTO when building the strategy for T&E prior to MS A if possible.  LDTO selection must be based on a thorough review of required DT&E skill sets and human and capital resources that are best suited and available for each program. and 4.5.2.  Appropriate LDTO Organizations.  HQ AFMC/A3 and HQ AFSPC/A5 will jointly develop lists of LDTO qualifications and candidate LDTO organizations; current lists can be obtained  by  contacting  the  following  offices  at  AFMC:    AFMC/A3F  LDTO  Workflow <AFMC.A3F.LDTOWorkflow@us.af.mil AFSPC:  AFSPC.A5XR.Workflow.1@us.af.mil.  DTO  candidates  should  have  experience  with  the relevant system domain(s) and in leading other organizations.  During system development, the  skills  of  several  developmental  test  organizations  may  be  needed,  but  only  one  will  be designated  as  the  LDTO.    In  all  cases,  the  confederation  of  DT&E  organizations  must  be qualified  to  oversee  and/or  conduct  the  required  DT&E,  and  be  capable  of  providing objective  analysis  and  judgment.    The  designation  as  an  LDTO  does  not  require  all associated  DT&E  activities  to  be  conducted  by  the  LDTO  itself  or  at  a  single  geographic location.    While  there  are  many  LDTO  organizations,  the  AFTC  has  primary  LDTO capability  and  responsibility  for  aircraft,  air  armament,  avionics,  and  electronic  warfare testing.    SMC  has  primary  LDTO  capability  and  responsibility  for  test  of  space  and  space-borne systems.  AFI99-103  6 APRIL 2017 45 4.5.3.  LDTO  Selection  Process.  The  ITT  submits  their  selection  to  the  PM  along  with  a capabilities  and  resource  analysis.    LDTO  nominations  will  be  coordinated  with  HQ AFMC/A3 and/or HQ AFSPC/A5, as appropriate, before submission to the PEO.  After the PEO  approves  the  selection,  the  PM  notifies  HQ  AFMC/A3  and/or  HQ  AFSPC/A5,  as appropriate, and the PEM within 30 days.  Note: The PEM is the person from the Secretariat or  Air  Staff  who  has  overall  responsibility  for  the  program  element  and  who  harmonizes program documentation. 4.5.4.  Alternate  LDTO  Option.  Referred  to  as  an  “alternate-LDTO,”  this  designated option  is  by  exception  and  only  authorized  for  low  programmatic  risk  ACAT  III  programs not on any oversight list.  An alternate organization may be designated in lieu of an LDTO to perform  and/or  oversee  the  functions  described  in  paragraph  2.18.    Alternate  LDTO nominations  will  be  coordinated  with  HQ  AFMC/A3  and/or  HQ  AFSPC/A5/8/9X  before submission  to  the  PEO.    After  the  PEO  approves  the  selection,  the  PM  notifies  HQ AFMC/A3 and/or HQ AFSPC/A5, as appropriate, AF/TE, and the program element monitor (PEM) within 30 days. 4.6.  Determining  the  OTO.  The OTO for all programs and projects will be determined using the  three-column  flowchart  in  Figure  4.3.    The  flowchart  identifies  the  responsible  (default) OTO  for  Air  Force  acquisition  programs  based  on  program  ACAT,  OSD  OT&E  Oversight status,  and  multi-Service  applicability.    The  flowchart  also  identifies  a  process  to  transfer operational  test responsibilities from  MAJCOM test organizations to  AFOTEC  when requested by  the  MAJCOM  and  accepted  by  AFOTEC.    Any  such  change  must  be  coordinated  with  the PM.    The  flowchart  will  be  used  according  to  the  following  paragraphs  (references  cited  in Figure 4.3). 4.6.1.  Programs  Requiring  AFOTEC  Conduct.  As  the  Air  Force  OTA,  AFOTEC conducts  operational  testing  for  ACAT  I,  IA,  II,  OSD  OT&E  Oversight,  and  multi-Service acquisition programs as shown in Column 1 of Figure 4.3.  AFOTEC also conducts FOT&E for  programs  as  described  in  paragraph  3.5.3  and  as  shown  in  Column  2.    AFOTEC involvement  will  end  at  the  completion  of  FOT&E  (or  I/Q/MOT&E  if  no  FOT&E  is required) unless AFOTEC and the user MAJCOM otherwise mutually agree and document in the TEMP or other program documentation. 4.6.1.1.  If  a  program  has  completed  I/Q/MOT&E  with  deficiencies  or  shortfalls  having severe  or  substantial  mission  impacts,  as  identified  in  the  AFOTEC  final  report, AFOTEC  normally  conducts  FOT&E  for  those  deficiencies  as  shown  at  the  top  of Column  2.    AFOTEC  and  the  appropriate  MAJCOM  may  mutually  agree  to  allow  the MAJCOM to  conduct  further testing for mission impacts  rated substantial.   When these post-I/Q/MOT&E  programs  have  no  deficiencies  with  severe  or  substantial  mission impacts, the MAJCOM is responsible for continued operational testing. 4.6.1.2.  If  a  program  has  modifications,  upgrades,  etc.,  that  are  large  enough  to  be considered new acquisition programs, required operational testing will be conducted for the  new  program  by  the  appropriate  OTO  in  accordance  with  Figure  4.3.    In  these instances, systems normally re-enter the acquisition process at a milestone commensurate with  the  Acquisition  Strategy.    An  additional  indicator  that  a  program  may  warrant AFOTEC  involvement  is  the  presence  of  new  or  revised  operational  Capability Requirements Document (CRD) validated by the Joint Requirements Oversight  Council  46 AFI99-103  6 APRIL 2017 (JROC).  Multi-Service FDE may be assigned to a MAJCOM by mutual agreement with AFOTEC. Figure 4.3.  Determining the Operational Test Organization.  4.6.2.  Programs  Requiring  MAJCOM  Conduct.  As  shown  in  Column  3,  MAJCOM OTOs  conduct  required  operational  testing  for  ACAT  III  programs.    MAJCOMs  continue conducting  operational  testing  for  all  routine  post-I/Q/F/MOT&E  fielded  system  upgrades, deficiency  corrections,  and  sustainment  programs  as  required.    See  paragraph  2.11.1  for lead command designation.  MAJCOMs may request AFOTEC to assume responsibility for operational  testing  (see  paragraph    4.6.3)  and/or  may  request  support  according  to paragraphs 2.11.16 and  4.6.6.1. 4.6.3.  MAJCOM  Requests  for  AFOTEC  Re-Involvement.  Post-I/Q/MOT&E  and  post-FOT&E, MAJCOMs may request that AFOTEC remain involved (or become re-involved) in programs that are normally a MAJCOM responsibility (see right side of Column 2).  These requests  must  include  required  documentation  (i.e.,  Joint  Capabilities  Integration  and Development System (JCIDS) documents, enabling and operating concepts, and Acquisition Strategy) needed for AFOTEC to make an informed involvement decision.  AFOTEC uses a repeatable, documented process with clearly defined criteria to determine post-I/Q/MOT&E or  post-FOT&E  involvement.    AFOTEC  documents  their  decision  and  provide  timely notification to the HQ MAJCOM T&E OPR and AF/TEP.  If the response time exceeds 30 days,  AFOTEC  informs  the  MAJCOM  on  the  reason  for  delay.    Acceptance  of  test responsibility  also  means  providing  funds  for  test  execution  according  to  operational  test funding guidance in AFI 65-601, Vol I.  AFI99-103  6 APRIL 2017 47 4.6.4.  Some  acquisition  program  schedules  may  require  MAJCOM  testing  of  follow-on modifications, preplanned product improvements, and upgrades simultaneously with planned AFOTEC  FOT&E.    In  these  instances,  AFOTEC  and  operational  MAJCOM  testers coordinate through the ITT on the most efficient strategy for completing the required testing. 4.6.5.  AFOTEC Requests to Transfer OT&E Responsibilities. 4.6.5.1.  AFOTEC  requests  to  transfer  any  operational  test  responsibilities  should  be coordinated and resolved not later than 18 months prior to the first scheduled or required operational  test  event.    Transfer  of  operational  test  responsibilities  less  than  18  months prior  to  test  start  may  only  be  done  by  mutual  agreement  of  all  parties  and  AF/TE concurrence. 4.6.5.2.  In some cases, operational testing for an AFOTEC-supported program in Figure 4.3, Column 1, may be more appropriately executed by a MAJCOM OTO.  If AFOTEC and  the  MAJCOM(s)  mutually  agree,  AFOTEC  requests  an  exception  to  policy  from AF/TEP.    The  request  must  include  whether  the  program  is  on  OSD  OT&E  Oversight, the  ACAT  level,  phase  of  program  development,  rationale  for  the  change,  any  special conditions, and written MAJCOM concurrence. 4.6.6.  Miscellaneous Provisions. 4.6.6.1.  Despite  having  a  designated  lead  command  per  AFPD  10-9,  some  ACAT  III, non-OSD Oversight  programs  support multiple users with  differing  requirements  across an entire AF-wide enterprise area.  The lead MAJCOM and AFOTEC will negotiate an OT&E involvement role per Column 3 of Figure 4.3, or coordinate with appropriate HQ MAJCOM T&E OPR for a multi-MAJCOM/AFOTEC test approach. 4.6.6.2.  Some programs may not be clearly “owned” by a MAJCOM or sponsor with an organic operational test function.  In these cases, the program’s sponsor coordinates with AFOTEC  to  identify  an  appropriate  OTO,  with  respective  MAJCOM  concurrence,  to complete  any  required  operational  testing.    If  an  appropriate  OTO  cannot  be  identified, the sponsor contacts AF/TE for guidance. 4.6.6.3.  If the OTO and lead HQ MAJCOM T&E OPR jointly agree that no operational testing is necessary, the LDTO provides relevant DT&E data that supports the option to not conduct operational testing.  The OTO reviews the LDTO’s work, assesses the risk of accepting  that  work,  and  documents  their  assessment  with  a  SOTR  according  to paragraphs 3.5.11 and  7.4.5. 4.6.6.4.  Multiple  OTOs.  If  multiple  OTOs  within  the  Air  Force  are  tasked  to  conduct testing concurrently, the ITT must be notified before planning begins and a lead OTO is designated.  All operational test plans must be reviewed by, and reports coordinated with, the  lead  OTO  to  ensure  continuity  of  effort.    This  information  must  be  updated  in  the TEMP,  test  plans,  and  other  documentation  when  appropriate.    For  OSD  OT&E Oversight programs, the lead OTO complies with all Oversight requirements according to Attachment 2. 4.7.  OSD  T&E  Oversight  and  Approval.  DOT&E  publishes  a  list  of  acquisition  and sustainment programs requiring OSD T&E Oversight and monitoring.  The master list has sub- 48 AFI99-103  6 APRIL 2017 parts for LFT&E and OT&E.  PMs and CDTs/TMs must contact AF/TE as early as possible to determine if their program is on this list due to additional workload and reporting requirements. 4.7.1.  Additional  Workload  and  Reporting.  Continuous  coordination  with  AF/TEP  and the  assigned  DASD(DT&E)  and  DOT&E  action  officers  is  required  for  programs  on  OSD T&E Oversight.    ITTs should  invite AF/TEP and OSD action officers to  ITT meetings and decision  reviews,  and  coordinate  draft  TEMPs,  test  plans,  and  other  program-related documentation  as  the  program  unfolds.    Attachment  2  contains  a  succinct  summary  of information requirements. 4.7.1.1.  Selected  DT&E  plans  and  acquisition  documents  for  programs  on  OSD  DT&E Oversight  may  require  DASD(DT&E)  review  and/or  approval.    DOT&E  may  require  a test concept briefing for selected test programs.  PMs and LDTOs will respond promptly to requests for DT&E plans, test concept briefings, or other T&E documentation. 4.7.1.2.  When  LFT&E is required for  “covered  systems”  IAW 10 U.S.C. § 2366, these programs  are  placed  on  the  LFT&E  part  of  the  OSD  T&E  Oversight  list.    PEOs  must continually review their portfolios for any programs “covered” under 10 U.S.C. § 2366.  The PM is responsible to help identify these programs.  DOT&E approval of the LFT&E plan  is  required  before  commencing  tests.    In  certain  cases,  LFT&E  waivers  are appropriate and must be obtained before MS B.  See details in paragraph 5.8.4. 4.7.1.3.  Operational  testing  for  programs  on  OSD  OT&E  Oversight  may  not  start  until DOT&E approves the adequacy of the test plans in writing.  DOT&E requires approval of EOAs, OAs, OUEs, FDE, and OT&E plans, and requires a test  concept  briefing 180 days prior to test start for each of these plans.  For test plans that are integrated, DOT&E approval is only required on the operational test portions prior to the start of operational testing.  See paragraphs 6.6 and  6.7 for more details about DOT&E’s requirements. 4.7.2.  Coordination  Prior  to  Approval.  Program  offices  and  OTOs  should  endeavor  to coordinate  test  plans  and  concepts  with  all  ITT  stakeholders  as  early  as  possible.  Program offices, LDTOs, and OTOs (as appropriate) will route DT&E, LFT&E, operational test plans (e.g.,  EOA,  OA,  and  IOT&E),  and  test  concepts  requiring  OSD  approval  through  AF/TE before  submission  to  OSD.    AF/TEP  will  assist  with  the  review,  coordination,  and submission of these documents. 4.7.3.  OSD  Oversight  Programs  with  Multiple  Subparts.  Some  T&E  Oversight programs, although listed as a single entity, have multiple subparts, each with its own set of test  planning  and  reporting  requirements  to  satisfy  OSD’s  statutory  obligations.    OSD representatives to the ITT should identify which subparts are relieved of these requirements.  In  addition,  some  OSD  Oversight  programs  may  use  or  consist  of  components  from  non-OSD  Oversight  programs.    As  a  result,  these  components  may  be  subject  to  OSD  test  plan approval and reporting.  The ITT co-chairs document the subcomponents that are under OSD Oversight and notify AF/TE, the PM and the PEO. 4.7.4.  OSD  T&E  Oversight  List  Updates.  The  most  current  lists  are  maintained  at https://extranet.dote.osd.mil/oversight/index.html.  This list is frequently updated and new programs are added without official notice.  Contact AF/TEP for more information about the most current list.  All test organizations should forward recommended additions or deletions to AF/TEP.  AFI99-103  6 APRIL 2017 49 4.7.5.  Interoperability.  Interoperability  testing  must  be  comprehensive,  cost  effective, completed, and interoperability certification granted, before fielding of a new IT capability or upgrade.  An  interoperability  DT  plan  must  be  included  in  the  TEMP  and  interoperability demonstrated  by  MS  C  to  support  interoperability  certification  during  IOT&E.    PMs  and ITTs must coordinate closely with JITC under Defense Information Systems Agency (DISA) to  review the  NR KPPs and ensure test plan  adequacy to  verify the system  meets  NR KPP requirements, TEMPs, test criteria, and associated developmental and operational  test plans for  interoperability.    This  same  review  must  be  accomplished  for  IT  programs  with  joint, multinational,  or   AF/A2  must  ensure interoperability  test,  evaluation  and  certification  of  ISR  NSS  before  connection  to  an  IC network.    JITC  must  ensure  interoperability  test,  evaluation,  and  certification  of  IT  before connection to a DoD network.  PMs must also submit an ISP along with the TEMP prior to each milestone or CDR, or when significant modifications to the program occur. See DoDI 8330.01  and  AF  Guidance  Memo  2015-33-03,  Air  Force  Interoperability  &  Supportability IT/NSS.  interoperability interagency requirements. 4.7.5.1.  Operating at Risk List (OARL).  The Air Force representative to the DoD CIO Interoperability  Steering  Group  (ISG)  (tri-chaired  by  JS/J6,  DoD-CIO  and  AT&L)  may track  and  place  any  IT  or  NSS  with  significant  interoperability  deficiencies,  or  is  not making significant progress toward achieving Joint Interoperability Test Certification, on the  OARL.    Listed  programs  may  transition  to  the  OSD  T&E  Oversight  List.    DISA maintains  the  OARL  listing  all  IT  systems  denied  an  Interim  Certificate  to  Operate (ICTO) and have not received a waiver.  See DoDI 8330.01. 4.8.  Lead  Service  Considerations.  When  the  Air  Force  is  designated  the  lead  Service  for multi-Service T&E, the  ITT will document the other Services’ T&E responsibilities, resources, and methods to eliminate conflicts and duplication.  When the Air Force is not the lead Service, Air  Force  testers  follow  the  lead  Service’s  T&E  policies.    See  the  DAG  and  the  MOA  on MOT&E and JTE http://www.dote.osd.mil/pub/index.html for more information. 4.9.  Tester Inputs during Materiel Solution Analysis (MSA).  Developmental and operational testers with input from the CDT/TM shall assist requirements sponsors, acquisition planners, and systems  engineers  in  developing  AoAs  and  COAs.    Testers  provide  T&E  inputs  for  each alternative  developed.    Criteria,  issues,  COIs,  CTPs,  measures  of  effectiveness  (MOEs),  and measures of suitability (MOSs) developed for these documents are later used for developing the strategy for T&E and subsequent T&E plans. 4.10.  Developing  Test  Measures.  During  the  MSA  phase,  developmental  and  operational testers  should  begin  drafting  clear,  realistic,  and  testable  measures  to  support  the  strategy  for T&E, the MS A decision, and future test plans.  The feasibility of applying STAT methodologies (as defined in Par. 5.13) to these measures should be carefully considered to facilitate testability. These measures are refined and evolve as more information becomes available during and after the  MSA  phase.    DT&E  practitioners  assist  systems  engineers  in  developing  critical  system characteristics (i.e., CTPs) that when achieved, allow the attainment of operational performance requirements.  Operational testers draft COIs, MOEs, and MOSs for operational testing purposes.  The  goal  is  to  ensure  all  measures  are  traceable  to  key  system  requirements  and  architectures, and  correlate  to  the  KPPs  and  KSAs.    These  measures  guide  the  PM  when  writing  system specifications  for  contractual  purposes.    The  best  way  to  ensure  complete  coverage  and correlation is to list them in the DEF that becomes part of the MS A TEMP.  50 AFI99-103  6 APRIL 2017 4.11.  Test and Evaluation Master Plan (TEMP).  The TEMP documents the overall structure and objectives of the program’s T&E activities as well as test resource requirements to support acquisition  milestones  or  decision  points,  and  ultimately,  a  full-rate  production  or  full deployment  decision.    The  TEMP  integrates  the  requirements,  acquisition,  T&E,  systems engineering, and sustainment strategies with  all T&E schedules, funding, and resources into an efficient continuum of integrated testing.  The PM, working through the ITT, is responsible for preparing  TEMPs  for  MS  A,  RFP,  MS  B,  MS  C,  and  FRP/FD  decisions  for  all  acquisition programs IAW Table 2 in DoDI 5000.02.  All AF acquisition or sustainment programs requiring DT and/or OT to support a production or fielding decision require a TEMP regardless of where the  program  enters  the  acquisition  life  cycle.    PMs  may  tailor  the  content  of  the  TEMP  within regulatory guidelines to fit individual program needs and satisfy MDA requirements. 4.11.1.  The  TEMP  must  describe  feasible  test  approaches  for  the  selected  COA  option(s) based on the ICD, PPP, and enabling & operating concepts.  The TEMP outlines initial T&E designs,  objectives,  and  T&E  resource  requirements.    The  CDT/TM  with  developmental testers  assist  systems  engineers  in  drafting  CTPs  that  are  testable.    Operational  testers,  in conjunction  with  MAJCOM  requirements  and  T&E  offices,  develop  COIs  in  the  form  of questions  to  be  answered  during  evaluation  of  a  system’s  overall  effectiveness  and suitability.  They also draft the MOEs and MOSs.  A series of OAs should be integrated into the T&E continuum to reduce program risk and minimize the overall number of test events. 4.11.2.  TEMP  Organization.  The  TEMP  should  be  written  following  the  format  in  the DAG.    Any  type  of  testing  (as  described  in  Chapter  3)  used  by  the  program  will  be integrated into Part III (“Test and Evaluation Strategy”) of the TEMP.  The completed TEMP conveys such information as: 4.11.2.1.  The  linkage  between  the  requirements,  acquisition,  T&E,  and  sustainment strategies. 4.11.2.2.  The  linkage  between  operating  and  enabling  concepts,  the  SEP,  operational requirements  and  architectures,  system  characteristics,  threat  documents,  test  design information, CTPs, COIs, MOEs, MOSs, and increments of capability. 4.11.2.3.  Organizational  responsibilities  for  the  contractor(s),  PM,  LDTO,  PTO(s),  and operational testers. 4.11.2.4.  Integrated test methodologies and designs. 4.11.2.5.  Test resources, including M&S and cyber resources. 4.11.2.6.  Test limitations and test deferrals (see paragraphs 5.22 and  6.4.3). 4.11.2.7.  The  LFT&E  strategy  and  plans,  and  the  strategy  for  system  certification  of readiness for dedicated operational testing. 4.11.2.8.  MAJCOM testing, to include operational testing for follow-on increments. 4.11.3.  MS  A  TEMP  Requirements.  The  MS  A  TEMP  should  address  major  sections  of the  TEMP  outline  in  the  DAG,  understandably  with  limited  detail  available  at  MS  A.    A feasible test approach that supports the requirements, acquisition, cyber test strategies, and to a  limited  extent,  the  production  and  sustainment  strategy,  must  be  projected  within  the TEMP.  The TEMP must plan to take maximum advantage of existing investments in DoD ranges and facilities.  The MS A TEMP should include the following:  AFI99-103  6 APRIL 2017 51 4.11.3.1.  A  developmental  evaluation  methodology  providing  essential  programmatic information, technical risks and information required for major programmatic decisions. 4.11.3.2.  Estimate and plan for required resources to support adequate T&E. 4.11.3.3.  A summary of and working link to CDD or equivalent capability requirements document providing rationale for requirements. 4.11.3.4.  For software or software-intensive acquisitions, the OTO or OTA will conduct an  analysis  of  operational  risk  to  mission  accomplishment  covering  all  planned capabilities. (T-1)  Analysis will include an evaluation of operational risk of COTS and NDI integration. 4.11.3.5.  All planned T&E for phase completion including test entrance and exit criteria. 4.11.3.6.  A  table  of  independent  variables  (or  conditions,  parameters,  factors)  having  a significant effect on operational performance. 4.11.3.7.  The  OTO  or  OTA  for  the  program  will  provide  an  assessment  of  the  T&E implications of the initial CONOPS provided by the user no later than the MS A TEMP. (T-0)  The CONOPS/Operational Mode Summary/Mission Profile (CONOPS/OMS/MP) describes  the  operational  tasks,  events,  durations,  frequency,  operating  conditions  and environment in which the materiel solution is expected to perform each mission and each phase of the mission. 4.11.3.8.  Strategy and resources for cyber test and evaluation.  See also paragraphs 4.14 and  5.14. 4.11.4.  TEMP  Submittal  and  Coordination.  Obtain  the  required  TEMP  signatures  as shown in the TEMP Signature Page Format in the DAG.  All Air Force TEMPs will include a signature block for the LDTO next to the OTO. 4.11.4.1.  The  ITT  forwards  a  TEMP  draft  “in  parallel”  to  all  stakeholder  organizations represented on the ITT for pre-coordination review.  ITT representatives are expected to verify  concurrence  or  identify  outstanding  issues  within  30  days.   Dissenting organizations  must  provide  a  position  statement,  to  include  alternatives,  or  formal  non-concurrence on the draft TEMP  within this timeframe.   Following this pre-coordination period,  the  PM  signs  the  TEMP  and  staffs  in  parallel  to  all  required  “concurrence signature”  organizations  below  the  Air  Staff  level.    After  “concurrence  signatures”  are obtained, the TEMP  will be forwarded to  the Air Staff, through the PEO, for Air Force and OSD coordination and approval. 4.11.4.2.  For  all  OSD  T&E  Oversight  programs,  the  PEO  will  submit  the  TEMP  to SAF/AQE for HAF staffing.  The PEO will coordinate through required Air Staff offices (to include AF/TE and the SAE, in that order) for formal Service-level approval.  After SAE signature, the PEO will submit the TEMP to DASD(DT&E) and DOT&E. 4.11.4.3.  For  all  other  programs  not  requiring  OSD  approval,  the  PEM  will  ensure  the SAE (or designated representative) signs as the final Service approval authority.  AF/TE will sign prior to the SAE as the “DoD Component Test and Evaluation Director.”  If the SAE  is  not  a  signatory,  no  signature  is  required  for  the  “DoD  Component  Test  and Evaluation Director.”  52 AFI99-103  6 APRIL 2017 4.11.5.  Schedule.  TEMPs  requiring  OSD  approval  should  be  submitted  to  the  PEO  for review and signature 120 days prior to the decision review.  The PEO signs and submits the TEMP  via  SAF/AQ  Workflow  not  later  than  90  days  prior  to  the  decision  review  for  HQ USAF  (i.e.,  Service-level)  coordination  and  AF/TE  and  SAE  approval/signature.    Not  later than 45 days prior to the decision review, the SAE sends the TEMP to OSD for review and approval.  If OSD has issues, they may send the TEMP back to the PEM for changes.  After OSD’s changes are incorporated, the SAE submits the final Service-approved TEMP 10 days prior  to  the  decision  review  for  final  OSD  approval.    See  Attachment  2  for  a  summary  of coordination requirements. 4.11.6.  Multi-Service  TEMPs.  The  lead  Service  is  responsible  for  coordinating  multi-Service  TEMPs.    Signatures  from  the  “concurrence  signature”  organizations  in  the  other participating Services must be obtained before TEMP submission to the PEO, who submits in turn  to  the  Service  T&E  executives,  the  SAEs  (or  MDA  if  appropriate),  and  OSD.    PMs should consider additional time required for other Service coordination. 4.11.7.  TEMP Updates and Administrative Changes.  The PM and ITT will: 4.11.7.1.  Make updates to the TEMP whenever significant revisions impact the program or T&E execution as defined by the PM, DOT&E, DASD(DT&E), or AF/TE.  Updates are  required  prior  to  major  milestones  IAW  DoDI  5000.02,  and  will  be  staffed  as described  in  paragraph  4.11.5.    Note:  Updates  are  any  revisions  that  alter  the substantive  basis  of  the  MDA  certification  or  otherwise  cause  the  program  to  deviate significantly from the material previously presented, or if the conditions that formed the basis  for  the  original  agreement  have  changed.    (DoDI  5000.02,  Enclosure  1,  Table  4, contains general guidance from 10 U.S.C. § 2445(c) about what constitutes an update.) 4.11.7.2.  Make  administrative  changes  for  small  corrections  or  modifications  to  the TEMP.    Administrative  changes  do  not  impact  T&E  execution  and  do  not  require  full coordination  as  described  in  paragraph  4.11.5.    Provide  an  errata  page  listing  these changes. 4.11.8.  When a TEMP is No Longer Required.  Once a program’s acquisition is complete and COIs are satisfactorily resolved, a TEMP may no longer be required.  For programs on OSD  T&E  Oversight,  the  ITT  should  initiate  requests  to  cancel  the  TEMP.    Submit  such requests  and  justification  through  AF/TE  to  OSD.    For  non-oversight  programs,  TEMP cancellation is at the discretion of the ITT. 4.12.  Lead  DT&E  Integrator.  The  CDT  or  TM  functions  as  the  "lead  DT&E  integrator," interfacing  as  needed  with  all  other  representatives  on  the  ITT  and  maintaining  insight  into contractor  activities.    The  CDT  ensures  all  necessary  organizations  with  specialized  skills contribute to  TEMP  development.   The integrated test planning process  culminates in  a TEMP that  includes  an  initial  description  of  test  scenarios,  test  measures  (e.g.,  CTPs,  MOEs,  and MOSs), test locations, exercises, T&E methodologies, operational impacts and issues, contractor contributions, and projections for future capabilities. 4.13.  Reliability  Growth  Planning.  Planning  for  reliability  starts  with  testers  participating  in HPTs  to  help  ensure  operational  reliability  requirements  are  correctly  written,  reflect  realistic conditions, and are testable.  Testers work with the program's systems engineers in the allocation of  reliability  among  critical  components,  determining  the  amount  of  testing  and  resources  AFI99-103  6 APRIL 2017 53 required,  and  developing  the  plan  for  improving  reliability  as  development  progresses.    These items,  among others, are necessary when designing the system and the test  program.   They  are outlined  in  the  TEMP,  SEP,  and  LCSP.    Also  see  AFI  63-101/20-101  and  the  DoD  Guide  for Achieving Reliability, Availability, and Maintainability. 4.14.  Program  Protection.  The  PM  is  responsible  for  ensuring  sufficient  efforts  are  taken  to prevent  technology  transfer  to  adversaries  as  well  as  assessing  risks  to  the  supply  chain.  Program  protection  measures  will  be  employed  throughout  the  acquisition  life  cycle  to  include cybersecurity and AT and documented in the PPP and RMF Security Plan.  These measures will be assessed and evaluated through a comprehensive T&E program.  The PPP will be submitted with  the  MS  A  TEMP  and  included  with  each  subsequent  TEMP.    See  DoDI  5000.02,  DoDD 5200.47E, and DoDI 5200.39, Critical Program Information (CPI) Identification and Protection Within Research, Development, Test, and Evaluation (RDT&E).  4.14.1.  Cybersecurity Strategy.  The Cybersecurity Strategy outlines the implementation of cybersecurity  risk  management  throughout  the  program  acquisition  life  cycle.  The Cybersecurity  Strategy  must  indicate  the  most  recent  approval  status  of  the  RMF  Security Plan.    The  Cybersecurity  Strategy  should  describe  how  mission  critical  components identified  in  the  PPP  will  be  protected.    Cyber  test  planning,  to  include  cybersecurity  and cyber  resiliency  testing,  will  be  based  on  the  information  provided  by  the  Cybersecurity Strategy and will be included in the TEMP. 4.14.2.  Development of systems designed to operate in a contested cyber domain.  Testing of systems  that  operate  in  cyberspace  should  evaluate  the  system’s  ability  to  protect (cybersecurity  testing),  detect,  and  react  (cyber  resiliency  testing)  to  a  cyber  attack  and continue the mission. 4.14.3.  Anti-Tamper  (AT).  AT  is  documented  as  an  appendix  to  the  PPP  and  is  updated prior to each milestone.  The AT V&V plan and testing of the AT design will be coordinated with SAF/AQLS and completed before prior to FRP/FD decision. 4.15.  Pre-Milestone A Planning for T&E Resources. 4.15.1.  Securing  T&E  Ranges  and  Facilities.  Test  planners  must  contact  potential  test sites  early  to  obtain  estimates  of  costs,  availability,  and  test  priority.    Test  planners  should ascertain  how  each  range  or  site  establishes  priorities  among  programs  on  that  range,  and what to submit to gain access.  HQ AFMC A3, HQ AFSPC A5/8/9, or HQ ACC/A3 and the range or facility points of contact (POC) will provide information and assistance on using the MRTFB  and  other  government  test  facilities.    See  AFI  99-109,  Major  Range  and  Test Facility  Base  (MRTFB)  Test  and  Evaluation  Resource  Planning.    See  AFI  13-212,  Range Planning and Operations, for information on the use of test and training ranges.  The USAF T&E  Organizations  and  Facilities  Database  on  the  AF/TEP  page  of  the  Air  Force  Portal (https://www.my.af.mil/gcss-af/USAF/ep/contentView.do?contentType=EDITORIAL&contentId=cA4057E1F3C49EABC013C8CEA6BD714C5&channelPageId=s6925EC1351550FB5E044080020E329A9&programId=t88B4F00B39C57917013A794B7A081E45 ) provides information about the capabilities of available Air Force test facilities, capabilities, and other resources. 4.15.2.  Use  of  Government  Test  Facilities.  The  ITT  will  plan  to  take  full  advantage  of existing  investments  in  DoD  ranges,  facilities,  and  other  resources,  including  the  use  of  54 AFI99-103  6 APRIL 2017 embedded instrumentation.  For Air Force programs, test teams should plan to use Air Force test capabilities first, followed by other MRTFB facilities, followed by other military Service and non-DoD government facilities (including Federally Funded Research and Development Corporation  (FFRDC)  test  resources),  and  finally  contractor  facilities.    This  hierarchy  does not  mean  that  all  T&E  facilities  used  by  a  program  must  be  from  a  single  category; combinations of contractor and government facilities may provide the best business case and should be considered. 4.15.3.  Use  of  Non-Government  Facilities.  During  test  planning  development,  the  ITT should  consider  contractor  test  facilities  only  when  government  facilities  are  not  available, cannot  be  modified,  or  are  too  expensive.    If  the  strategy  for  T&E  calls  for  testing  at  non-government facilities, the PM must conduct a business case analysis that includes facility life cycle  sustainment  costs  for  all  COAs.    Analyze  COAs  that  include  teaming  arrangements with other programs using the same facilities on a cost-sharing basis.  Include these facility requirements  in  the  EMD  RFP  and  document  the  final  choice  with  rationale  in  the  TEMP.  The  T&E  resource  strategy  must  be  cost-efficient  as  well  as  flexible  while  also  providing consideration for security of the asset(s). 4.15.4.  Use  of  Exercises  and  Experiments.  To  the  maximum  practical  extent,  the USAFWC assists Air Force test organizations in gaining access to exercises and experiments to  take  advantage  of  operationally  realistic  environments,  high  threat  densities,  massed forces,  and  other  efficiencies.    Test  organizations  should  plan  to  participate  in  joint  and Service experiments and war games, as appropriate.  The goals of the exercise, experiment, or  T&E  activity  must  be  compatible;  some  tailoring  may  be  required  to  ensure  all stakeholders benefit from the activity. 4.15.5.  Planning  for  Testing  in  a  Joint  Environment.  All  planning  for  testing  must  be structured to reflect the joint environment and missions in which the system will operate. for  Target  and 4.15.6.  Planning Instrumented  Munitions  Expenditures.  Test organizations,  in  consultation  with  PMs,  will  plan  for  aerial  target  requirements  IAW  AFI 99-108,  Programming  and  Reporting  Aerial  Target  and  Missile  Expenditures  in  Test  and Evaluation.    Test  organizations  and  PMs  must  forecast  their  requirements  for  munitions flight  termination  and  telemetry  kits  IAW  AFI  99-120,  Forecasting  and  Programming Munitions Telemetry and Flight Termination Systems. 4.15.7.  Planning  for  Cyber  Test  Resources.  Cyber  test  assets  needed  to  support  testing must  be  included  in  the  first  TEMP  of  a  program  and  updated  in  subsequent  TEMPs.  Resource  requirements  must  reflect  use  of  operationally  representative  test  articles  in  an operationally representative cyber environment. 4.15.8.  Planning  for  Foreign  Materiel  Resources.  ITT  members  should  consult  with requirements,  acquisition,  and  intelligence  organizations  to  determine  the  need  for  foreign materiel resources. 4.16.  Testing  Defense  Business  Systems  (DBS).  The  DoDI  5000.02  software  intensive acquisition model mentioned in paragraph 1.2.1.1 is well-suited to DBS acquisition.  AFMAN 63-144, Defense Business System Life Cycle Management, states “DBS should be delivered using a  portfolio  approach.”    The  tailored  portfolio  approach  allows  some  common  (test)  processes, documents,  and  resources  be  applied  to  numerous  programs  in  a  portfolio.    Programs  on  OSD  AFI99-103  6 APRIL 2017 55 oversight  must  have  a  standalone  TEMP.    The  PEO  will  ensure  program-specific  or  tailored processes,  documents  and  resources  are  documented.    DBS  programs  including  limited deployments or software releases will require OT&E readiness certification per AFMAN 63-119 followed  by  OT&E.  The  PM  must  ensure  that  any  specialized  tests  (e.g.,  cyber  and interoperability), and correction of any deficiencies with mission impacts, are addressed as early as  possible  prior  to  cyber  and  interoperability  certification  decision  milestone  dates.    Once fielded,  cybersecurity  capability  will  be  monitored  using  an  AO-approved  system-level continuous monitoring strategy. 4.16.1.  Non-MDAP  DBS.  A  new  acquisition  approach  for  non-MDAP  defense  business systems  is  outlined  in  the  2  Feb  2017  release  of  DoDI  5000.75.    Acquisition  for  these systems  follows  a  Business  Capability  Acquisition  Cycle  (BCAC)  that  encourages  tailored procedures for capability being acquired and application of commercial best practices.  The BCAC  introduces  new  terminology  that  doesn’t  correlate  directly  with  the  traditional acquisition life cycle depicted in DoDI 5000.02 and AFI 63-101.  Milestones A, B and C are replaced  by  phase-specific  Authority  To  Proceed  (ATP)  decision  points.    The  term “Implementation Plan” captures DT and OT requirements traditionally codified in a TEMP. 4.17.  Testing  of  Urgent  Needs.  Expedited  testing  and  reporting  is  required  for  urgent  needs (e.g.,  UON,  Joint  Emergent  Operational  Need  (JEON),  or  Joint  Urgent  Operational  Need (JUON)) using the QRC guidance in CJCSI 3170.01 (and the associated JCIDS Manual) and the AF/A5R  Requirements  Development  Guidebook,  Volume  2  (Urgent  Needs)  along  with  the acquisition  guidance  in  DoDD  5000.71,  and  DoDI  5000.02.    Levels  of  risk  acceptance  will  be higher  and  timelines  much  shorter  than  normal  in  order  to  satisfy  urgent  needs.    Tailoring  and streamlining  is  required  for  rapid  acquisition  programs.    Per  DoDI  5000.02,  the  document requirement is the minimal amount necessary to define and execute the program.  A TEMP may be waived for accelerated or urgent programs on DOT&E oversight; the PM should prepare an operational  and/or live fire test plan for DOT&E  approval.   T&E results are generally reported with a C&L Report according to paragraph 7.5.  After initial system fielding, if the QRC will be further  developed  as  an  enduring  program,  the  PEO  may  require  the  program  to  complete  the traditional  acquisition,  requirements,  T&E,  and  C&A  processes  for  any  unfinished  areas.    For urgent need systems being added to existing capability, testing must ensure that the addition did no harm to the existing system, including cybersecurity. 4.18.  Additional  Early  Planning  Considerations.  PMs  and  T&E  practitioners  need  to consider the topics in Table 4.1 prior to MS A.  Although details are not required until after MS A,  early  strategic  planning  for  these  items  streamlines  later  activities.    The  ITT  should  locate qualified personnel to develop and manage these future topics.  Chapter 5 contains the details.    56 AFI99-103  6 APRIL 2017 Table 4.1.  Topics for Early Test Planning Consideration. Topic Common T&E Database Critical Technical Parameters (CTP) Data Archiving Deficiency Reporting Foreign Disclosure Integrated Technical, Environmental, and Safety Reviews Joint Reliability and Maintainability Evaluation Team (JRMET) Scientific Test and Analysis Techniques (STAT) Description Single repository for all T&E data for the system under test.  Note:  official government Deficiency Reports must be input into the Joint Deficiency Reporting System. Measurable, critical system characteristics that, when achieved, allow the attainment of operational performance requirements. Retention of test plans, analyses, annexes and related studies to maintain historical perspective Processes and procedures established by the PM to report, screen, validate, evaluate, track, prioritize, and resolve deficiencies Recommending test data or materials for release to foreign nationals For More Information Para 5.18 Para 5.11 Para 5.18.9 Para 5.19 Para 5.18.8 Procedures for scheduling and conducting technical, environmental, and safety reviews  Para 5.21 Collects, analyzes, verifies, categorizes, and scores reliability, availability, and maintainability (RAM) data Para 5.18.5 Scientifically-based test and analysis techniques and methodologies for designing, executing, and reporting on tests Para 5.13  AFI99-103  6 APRIL 2017 57 Chapter 5 T&E ACTIVITIES SUPPORTING MILESTONE B DECISIONS 5.1.  Post  MS  A.  The  most  important  activities  after  the  MS  A  decision  and  during  the Technology  Maturation  &  Risk  Reduction  phase  are  shown  in  Figure  5.1.    Sustained,  high quality tester involvement and collaboration with requirements sponsors and system developers must continue throughout the Technology Maturation & Risk Reduction phase in preparation for the  next  phase,  EMD.    T&E  practitioners  continue  expanding  and  developing  the  topics described in Chapter 4.  They must address new topics added in this chapter, continue refining the  strategy  for  T&E,  and  begin  building  specific,  executable  T&E  plans  that  support  the requirements, acquisition, and cyber test. Figure 5.1.  Integration of Requirements, Acquisition, and T&E Events Prior to MS B. 5.2.  T&E Funding Sources.  The funding sources for T&E depend on the nature and purpose of the work and the type of testing.  Funding is not based on the organization conducting the test or the name of the test.  Detailed guidance is in DoD 7000.14-R, Vol 2A, and AFI 65-601, Vol 1.  Funding requirements for Joint Interoperability Certification Tests must be coordinated directly with JITC in accordance with the JITC  Interoperability Process Guide v2.0 and DoDI 8330.01.  Test  resource  advisors  must  ensure  compliance  with  these  documents  before  requesting  and committing funds.  Direct assistance is available from SAF/FMBI, SAF/AQXR, and AF/TEP.   58 AFI99-103  6 APRIL 2017 5.3.  Formal  Contractual  Documents.  The  CDT/TM  working  with  developmental  testers review the System Requirements Document (SRD) to ensure it correctly links and translates the CDD (draft or final, as appropriate) into system specifications that can be put on contract.  MIL-HDBK-520,  Systems  Requirements  Document  Guidance,  provides  guidance  on  translating capability based  requirements into system requirements.   ITT members review the RFP, SOW, and DD Form 254 (as appropriate) for EMD to ensure contractor support to government T&E is included and properly described.  For guidance, use DASD(DT&E)’s guide, Incorporating Test and Contracts.  https://acc.dau.mil/CommunityBrowser.aspx?id=650755.  The ITT reviews the Contract Data Requirements List (CDRL) to ensure it describes the content, format, delivery instructions, and approval and acceptance criteria for all deliverable T&E data.  The ITT confirms that sufficient funding is provided for all T&E-related resources.  The ITT also reviews these drafts to ensure user-defined  capabilities  have  been  accurately  translated  into  system  specifications  and provisions are made for the following: Department Evaluation into of Defense Acquisition 5.3.1.  Government review and approval of contractor test plans and procedures before tests commence. 5.3.2.  Government insight into contractor testing to ensure systems are maturing as planned, to include government observation of contractor testing. 5.3.3.  Proper  interface  of  the  contractor’s  DR  system  with  the  government’s  DR  system, including  TO  00-35D-54,  USAF  Deficiency  Reporting,  Investigation,  and  Resolution, compliant processes and methodologies, and portability of data into government information management systems. 5.3.4.  Contractor  T&E  support  such  as  failure  analyses,  T&E  data  collection  data  sharing and data management, operation of unique test equipment, provision of product support, and test reports. 5.3.5.  Contractor participation in government test planning forums such as the ITT. 5.3.6.  Contractor provision of training to testers and provision of long-lead items as well as contractor support of instrumentation necessary to collect data needed by other stakeholders. 5.4.  Limitations  on  Contractor  Involvement  in  Operational  Testing.  DoDI  5000.02  places limits  on  contractor  involvement  in  IOT&E  of  MDAPs.    Air  Force  policy  applies  these limitations  to  all  OT&E  programs,  projects,  and  activities  regardless  of  ACAT.    This  does  not prohibit  contractor  observation  of  OT&E  events  if  the  program  office  provides  justification  to the OTO or OTA for approval and it does not influence the event. 5.4.1.  System  Contractors.  Operational testers must strictly avoid situations where system contractors could reduce the credibility of operational test results or compromise the realistic accomplishment of operational test scenarios.  Contractor personnel may only participate in OT&E of Air Force programs to the extent they are planned to be involved in the operation, maintenance, and other support of the system when deployed in combat. 5.4.2.  System  Contractor  Support  to  Operational  Testing.  System  contractors  may  be beneficial  in  providing  logistic  support  and  training,  test  failure  analyses,  test  data,  and unique software and instrumentation support that could increase the value of operational test  AFI99-103  6 APRIL 2017 59 data.  Explanations of how this contractor support will be used and the mitigation of possible adverse effects must be described in the TEMP and developmental and operational test plans. 5.4.3.  Contractors.  According  to  DoDI  5000.02  and  Air  Force  policy,  contractors  who have  been  involved  in  the  development,  production,  or  testing  of  a  system  may  not  be involved  in  the  establishment  of  criteria  for  data  collection,  performance  assessment,  or evaluation  activities  for  operational  testing.    This  limitation  does  not  apply  to  a  support contractor that has participated in such development, production, or testing solely in test on behalf of the government. 5.5.  Testing  IT  and  DBS.  As  agile  development  concepts  and  methods  are  incorporated  into DoD  policy,  the  ITT  must  tailor  the  strategy  for  T&E  to  suit  program  needs.    Agile  methods break  tasks  into  small  increments,  use  minimal  documentation,  are  tolerant  of  changing requirements,  and  have  iterations  typically  lasting  from  a  few  weeks  to  a  few  months.    The emphasis  is  on  software  that  works  as  the  primary  measure  of  progress.    The  strategy  for developmental  T&E  on  software  intensive  systems  should  likewise  test  small  increments, consolidating  test  planning  into  an  overarching  test  plan  of  the  entire  capability,  with  focused annexes  for  tests  of  incremental  capability.  Testers  must  maintain  early  and  recurring involvement with the program office, developer, and users to manage requirements, and should minimize reporting to focus on the incremental progress.  While efforts should be made during developmental testing to approximate an operational environment, no formal operational testing should  be  performed  until  the  deployable  or  final  release  or  increment  is  complete  to  deliver  a usable capability in the operational environment. 5.5.1.  The ITT ensures cyber testing described in paragraph 3.9.2 is integrated into the ISP, SEP, TEMP, contracts, and relevant test plans where and when appropriate. 5.5.2.  Use DODI 5000.02 Enclosure 5 to determine the risk assessment level of operational test for software acquisitions in these systems. 5.6.  Modeling and Simulation (M&S) in Support of T&E.  Increasingly complex battlespace environments,  cross-domain  systems  interdependencies  and  increasingly  capable  and  dynamic threats  are  effectively  making  modeling  and  simulation  essential  in  developing,  testing,  and assessing  system  capability  and  performance.    Early  requirements  definition,  research,  and detailed planning are essential in ensuring that modeling efforts are timely, adequately resourced and  fully  address  programmatic  needs.      T&E  planning  for  M&S  needs  to  look  across  the  full breadth of the program to avoid duplication, identify and leverage synergies, and to ensure that long lead requirements such as intelligence community support are identified and resourced in a timely fashion and will meet schedule requirements. Additional M&S direction, guidance, and resources are available across the Department and the Services and should be reviewed for applicability.  The DoD Modeling and Simulation Coordination Office (MSCO):  https://www.msco.mil/ provides a code repository and tools for M&S discovery metadata search to identify existing verified, validated, accredited, and reusable M&S tools and DSMs prior to initiating development of M&S assets.  This review reduces duplication of existing technology and products. DODI 5000.02 mandates that every distinct use of a model or simulation in support of an operational evaluation must be accredited by the OTA.  Additionally, for programs under DOT&E Oversight, the use M&S for operational evaluation/test must also be approved by DOT&E.  Additional guidelines can be found in the AF/TE TEMP Guide at https://www.my.af.mil/gcss- 60 AFI99-103  6 APRIL 2017 afbvpcp/USAF/ep/globalTab.do?channelPageId=s6925EC1355030FB5E044080020E329A9 and the DOT&E TEMP Guidebook at http://www.dote.osd.mil/tempguide/index.html. It should be noted that Accreditation of an M&S application for one program does not mean accreditation is valid for use on another program. Check the Air Force Agency for Modeling and Simulation (AFAMS) website at http://www.afams.af.mil/ for shared Live, Virtual, Constructive LVC - Operational Training (OT) foundations (infrastructure, standards, security, knowledge management and workforce development) and interoperability to identify potential synergies and prevent unnecessary duplication.   M&S tools must also undergo cyber testing to identify cyber vulnerabilities and to prevent or mitigate cyber threats prior to use in test of other systems.   The PM must document how M&S supports integrated testing in the Modeling and Simulation Support Plan and the TEMP to include schedule planning for VV&A completion prior to formal requirement verification.  For additional policies on using M&S, refer to AFI 63-101/20-101, AFI 16-1001, Verification, Validation and Accreditation (VV&A); AFI 16-1005. Modeling & Simulation Management. 5.7.  Pre-MS B DT&E Planning. 5.7.1.  Planning  for  Integrated  Testing.  Integrated  testing,  as  described  in  paragraph 1.3.4, is the expected approach unless it can be shown that it adds unacceptable costs, delays, or  technical  risks.    The  ITT  and  test  teams  continue  refining  the  ITC  initially  developed  in the MS A TEMP.   The  ITC  supports development  of test plans that  are integrated  and that cover as many developmental, and operational  test  objectives as possible prior to  dedicated operational testing.  The ITT integrates operationally relevant test events throughout DT&E to  provide  additional  test  realism,  decrease  overall  duplication  of  effort,  increase  test efficiency,  and  identify  performance  shortfalls  that  could  result  in  increased  development costs.    Multiple  sets  of  test  objectives  must  be  accomplished  together  within  statutory  and regulatory  guidelines.    DT&E  activities  can  overlap  and  share  T&E  resources  with  OAs  to conserve resources and extract maximum amounts of data. 5.7.1.1.  Use  the  systems  engineering  approach  in  the  SEP  to  break  down,  identify,  and integrate  the  COIs,  CTPs,  test  objectives,  MOEs,  MOSs,  measures  of  performance (MOP),  resources,  and  schedules,  which  are  documented  as  part  of  the  ITC.    When appropriate,  scientific  test  and  analysis  techniques  (STAT)  and  methodologies  (as described  in  paragraph  5.13)  will  also  be  used.    Existing  safety  review  processes  will not be compromised.  See paragraphs 1.3 and  6.2 through  6.4. 5.7.1.2.  Test  approaches  must  be  flexible  and  efficient,  especially  in  areas  long  held  to require  rigid  structural  control.    Traditional  limits  such  as  frozen  baselines  for  the duration of OT&E, concurrent development, data merging, using other testers’ validated data,  and  statistical  confidence  when  using  small  sample  sizes  should  be  carefully reviewed  so  they  do  not  become  impediments.    However,  the  overarching  goals  of  any test  should  not  be  compromised.    After  thorough  analysis,  test  planners  may  conclude that some test activities (e.g., the dedicated portions of OT&E) should not be combined. 5.7.1.3.  While planning for integrated testing, both operational suitability and operational effectiveness  should  be  given  commensurate  consideration.    See  AFPAM  63-128, Attachment  6,  and  DoD  Guide for  Achieving  Reliability,  Availability,  and Maintainability.  AFI99-103  6 APRIL 2017 61 5.7.1.4.  Any  test  limitations  or  deferrals  resulting  from  integrating  test  events  must  be explained in test plans and the TEMP.  See paragraph 5.21. 5.7.1.5.  Update  TEMP  and  operational  test  plans  prior  to  each  milestone  with  latest validated threat assessment.  Any elevated classification resulting from inclusion of threat information  will  require  addition  of  classified  annex  to  TEMP  and/or  classified requirements document. 5.7.2.  Requesting  Operational  MAJCOM  Support  for  DT&E.  Requests for operational MAJCOM  test  support  for  DT&E  must  be  vetted  through  the  appropriate  MAJCOM headquarters  T&E  office  before  they  may  be  accepted.    Operational  and/or  implementing MAJCOM  headquarters’  review  and  approval  is  required  depending  on  the  nature  of  the request. 5.7.2.1.  Air Force program offices and/or developmental test organizations may request operational  MAJCOM  (i.e.,  non-test  coded  unit)  support  for  DT&E  activities  only  after obtaining  concurrence  from  that  organization's  MAJCOM  headquarters  T&E  office.  Such test support will be restricted to low-risk military utility evaluations under the direct supervision of an LDTO.  These activities will be called "DT&E Assists" to indicate they are not operational testing. 5.7.2.2.  Air  Force  program  offices  and  developmental  test  organizations  may  request MAJCOM  OTO  support  for  DT&E  activities  (including  acquisition/sustainment programs  or  proof-of-concept  activities  where  no  formal  DT&E  is  planned)  only  after obtaining  concurrence  from  the  operational  MAJCOM  headquarters  T&E  office.    Such test  support  should  normally  be  restricted  to  low-risk  (technical  and  safety)  DT&E activities.    OTOs  must  accomplish  independent  technical  and  safety  reviews.    Any previously  accomplished  technical  and  safety  reviews  and  approval  documentation  will be provided to the OTO for their independent analysis. 5.7.2.3.  Requests  for  operational  MAJCOM test  support  from  non-Air  Force organizations  (e.g.,  Defense  Advanced  Research  Projects  Agency)  must  first  be forwarded  to  the  operational  MAJCOM  headquarters  T&E  office  for  feasibility  review and  approval.    Requests  rejected  by  an  operational  MAJCOM  may  be  submitted  to  an implementing  MAJCOM  headquarters  T&E  office  (AFMC/A3  or  AFSPC/A5  as appropriate)  for  potential  sponsorship,  program  initiation  and  subsequent  assignment  of an  LDTO.    If  a  program  office  or  LDTO  is  associated  with  the  non-Air  Force  agency request, forward all applicable technical and safety data to the OTO for their independent reviews. 5.7.2.4.  Information on test resources and ranges can be found in the AF/TE Guidebook. 5.8.  LFT&E Planning.  The following paragraphs supplement statutory direction in 10 U.S.C. § 2366.    The  DAG  provides  additional  guidance  for  implementing  LFT&E  legislation  and  OSD requirements. 5.8.1.  Implementation.  LFT&E  results  must  support  system  design  and  production decisions for covered systems.  The focus and funding for LFT&E should be on the system components  immediately  related  to  the  development  or  modification  program,  but  the resultant evaluation must be at the system level.  PMs should contact the appropriate LFT&E test  organization  in  Arnold  Engineering  Development  Center  (AEDC)  (i.e.,  780th  Test  62 AFI99-103  6 APRIL 2017 Squadron for munitions and 704th Test Group/OL-AC for survivability of covered systems) for assistance with development of LFT&E strategies, plans, waivers, and alternative plans. 5.8.2.  Determining Covered System or Major Munitions Program Status.  The PM and ITT must first determine if their system is a “covered system,” “major munitions program,” or “covered product improvement program.”  PEOs must continually review their portfolios for any programs “covered” under 10 U.S.C. § 2366.  When a potential LFT&E candidate is identified, the  ITT, PM,  appropriate  LFT&E organization, and AF/TEP must  be notified  as early  as  possible  to  start  the  LFT&E  Strategy  Approval  process.    The  appropriate  LFT&E organization can facilitate discussions to  help  determine a corporate  Air  Force position and develop a recommendation to DOT&E. 5.8.3.  LFT&E  Strategy  Approval.  As  soon  as  an  affirmative  determination  of  covered status  is  made,  the  PM  develops  a  LFT&E  strategy  with  the  assistance  of  the  appropriate LFT&E  organization.    The  PM  is  responsible  for  communicating  and  coordinating  the LFT&E strategy with DOT&E and determining the appropriate method.  The strategy must be  structured  so  design  deficiencies  uncovered  during  EMD  may  be  corrected  before proceeding  beyond  LRIP.    Technology  projects  meeting  the  statutory  criteria  are  also required to undergo LFT&E.  The ITT describes the LFT&E strategy and plans in the TEMP.  LFT&E  must  be  fully  integrated  into  the  continuum  of  testing.    AF/TE  will  coordinate  the LFT&E strategy with SAF/AQ before it is forwarded to DOT&E for final approval. 5.8.4.  Requests for LFT&E Waivers.  The Secretary of Defense may waive the application of the survivability and lethality tests of this section to a covered system, munitions program, missile program,  or covered product  improvement  program  if the Secretary  determines that live-fire  testing  of  such  system  or  program  would  be  “unreasonably  expensive  and impractical”  and  submits  a  certification  of  that  determination  to  Congress  either  (a)  before MS B approval for the system or program; or (b) in the case of a system or program initiated at  (i)  MS  B,  as  soon  as is  practicable  after  the  MS  B  approval;  or  (ii)  MS  C,  as  soon  as  is practicable after the MS C approval.  To support this determination, the PM will submit the LFT&E waiver request and alternative strategy to AF/TE & SAF/AQ prior to Service-level approval.    After  SAF/AQ  approval,  the  LFT&E  waiver  request  and  alternative  strategy  are forwarded to DOT&E for alternative strategy approval, and then together to USD(AT&L) for waiver approval.  Upon final OSD approval, DOT&E issues a report and formal certification to Congress.  Document the LFT&E waiver and alternative LFT&E strategy in an annex to the TEMP. 5.8.5.  Alternative  LFT&E  Strategy.  The  alternative  strategy  does  not  alleviate  the statutory  requirement  for  survivability  or  lethality  testing.    The  alternative  strategy  must include  LFT&E  of  components,  subassemblies,  and/or  subsystems  which,  when  combined with accredited M&S and combat data analysis, will result in confidence in the survivability (or lethality) of the system. 5.8.6.  Alternative  Strategy  and  Testing  for  Major  Modifications.  In  the  case  of  major modifications or new production variants, the alternative LFT&E strategy and detailed plans must focus on configuration changes that could significantly affect survivability or lethality.  Potential interactions between portions  of the configuration that are  changed and those that are  not  changed  must  be  assessed.    The  assessment  results  must  include  a  whole  system analysis  of  the  survivability  and  vulnerability  impacts  on  the  total  system.    Alternative  AFI99-103  6 APRIL 2017 63 LFT&E  are  not  required  on  components  or  subsystems  unrelated  to  the  modification program. 5.8.7.  Detailed  LFT&E  Plans.  DOT&E  reviews  and  approves  all  LFT&E  plans  prior  to commencement  of  LFT&E.    All  LFT&E  must  be  completed  and  test  reports  submitted  45 calendar  days  before  the  beyond-LRIP  decision  review.    The  DAG  lists  the  mandatory contents of LFT&E plans. 5.8.8.  Personnel Survivability.  An assessment of force protection equipment and personnel survivability will also be conducted as required by DoDI 5000.02. 5.9.  Early Operational Assessment (EOA) Planning and Execution.  During the Technology Maturation  &  Risk  Reduction  phase,  EOAs  are  conducted  as  required  to  provide  operational inputs to requirements and system developers prior to MS B.  The EOA supports development of the  Capability  Development  Document  (CDD),  test  concepts  and  plans,  the  RFP  Release Decision Point, and the MS B decision.  The scope and content of  EOAs should be tailored to ascertain  if  the  program  is  on  track  using  any  available  data.    For  programs  on  DOT&E oversight, EOAs will require DOT&E approval before they can start.  EOAs can be collaborative efforts conducted concurrently with DT&E, and need not be independently conducted; however, results must be independently assessed. 5.10.  Tester  Involvement  in  Requirements  Documentation.  Testers  must  continue  assisting requirements  sponsors  in  refining  capability  requirements  (e.g.,  CDD,  CPD)  and  enabling  and operating concepts as described in the AF/A5R Requirements Development Guidebook, Volume 1.    Developmental  and  operational  testers  participate  in  HPTs  by  providing  technical  and operational  expertise,  lessons  learned,  and  data  from  EOAs,  prototypes,  and  integrated  testing.  Testers  help  ensure  system  performance  attributes  (KPPs,  KSAs,  and  APAs)  and  CTPs  are attainable, testable, and accurately expressed in SRDs, RFPs, and SOWs. that,  when  achieved,  allow 5.11.  Critical Technical Parameters (CTP).  The CDT and the systems engineers, assisted by DT&E practitioners, are responsible for developing CTPs.  CTPs are measurable, critical system characteristics the  attainment  of  operational  performance requirements.  They are selected from the technical performance measures on the critical path to achieving  the  system’s  technical  goals.    Failure  to  achieve  a  CTP  during  DT&E  should  be considered a reliable indicator that the system is behind in the planned development schedule, or will  likely  not  achieve  an  operational  requirement.    By  contrast,  a  KPP  is  a  system  attribute considered  essential  for  mission  accomplishment.      KPPs  are  expressed  in  term  of  parameters which reflect Measures of Performance (MOPs) using a threshold/objective format. 5.11.1.  Developmental testers must help ensure CTPs are measurable and testable, traceable to  key  system  requirements  and  architectures,  and  help  the  PM  translate  them  into  system specifications for contractual purposes. 5.11.2.  CTPs  must  reflect  the  system’s  definition  and  design  for  all  elements  such  as hardware  components,  software,  architectures,  information  assurance,  personnel,  facilities, support equipment, reliability and maintainability, and data.  CTPs will be correlated to COIs and  OT&E  test  objectives  (i.e.,  MOEs  and  MOSs)  in  the  TEMP.    Testers  must  ensure complete coverage and correlation by listing them in the DEF in the TEMP.  Guidance and examples for the DEF can be found in the DAG.  64 AFI99-103  6 APRIL 2017 5.12.  Testing COTS, NDI, and GFE.  PMs shall plan for and conduct T&E of COTS, NDI, and GFE  even  when  these  items  come  from  pre-established  sources.    The  operational  effectiveness and suitability of these items and any military-unique applications must be tested and evaluated before  a  FRP/FD  decision.    The  ITT  should  plan  to  take  maximum  advantage  of  pre-existing T&E data to reduce the scope and cost of government testing.  More information is available in USD(AT&L)’s handbook SD-2, Buying Commercial & Non-developmental Items: A Handbook, available at http://www.dsp.dla.mil/.  IT and NSS should be tested IAW DoDI 8500.2, CJCSI 5123.01G, and the Joint SAP Implementation Guide (JSIG), (if applicable). 5.13.  Scientific  Test  and  Analysis  Techniques  (STAT).  Whenever  feasible  and  consistent with available resources, STAT will be used for designing and executing tests (DT & OT), and for analyzing the subsequent test data.  The top-level approach must be described in the TEMP and  the  SEP  at  Milestone  A,  and  in  more  detail  in  subsequent  test  plans  as  appropriate.    The conceptual  test  designs  themselves  need  not  be  part  of  the  TEMP  or  the  SEP,  but  shall  be available for review during coordination of those documents.  The ITT should consult  a STAT practitioner  (systems  engineer  experienced  in  applying  STAT  methodologies  to  optimize  test) whenever test designs are considered. 5.13.1.  The selected approach must address the following areas as a minimum: 5.13.1.1.  Define the objective(s) of the test (or series of tests, when appropriate). 5.13.1.2.  Identify the information required from the test to meet the test objective(s). 5.13.1.3.  Identify  the  important  variables  that  must  be  measured  to  obtain  the  data required  for  analysis.    Identify  how  those  variables  will  be  measured  and  controlled.  Identify the analysis technique(s) to be used. 5.13.1.4.  Identify the test points required and justify their placement in the test space to maximize the information obtained from the test. 5.13.1.5.  If  using  a  traditional  hypothesis  test  for  data  analysis,  calculate  statistical measures of merit (power and confidence level) for the relevant response variables for the selected  number  of  test  events.    If  using  another  statistical  analysis  technique,  indicate what  statistical  measures of merit will be used.   If a statistical  analysis technique is  not being used, discuss the analysis technique that is being used and provide rationale. 5.13.1.6.  State  whether  sampling  error  is  expected,  and  identify  the  plan  to  deal  with sampling  error  in  the  measurements’  uncertainty  and  its  inclusion  in  the  overall uncertainty of derived parameters. 5.13.2.  The selected test design(s) should help ensure smoother, more efficient integration of all types of testing up to and including FOT&E.  In all cases, the PM is responsible for the adequacy of the planned series of tests  and reports  on the expected decision risk remaining after test completion. 5.14.  Cyber  Test.  All  aspects  of  cyber  test  including  required  resources,  manpower,  and infrastructure  must  be  planned  for  and  documented  in  the  TEMP.    Cyber-related  TEMP requirements  should  support  cyber  test  considerations  in  paragraph  3.3.5.    TEMPs  should explain  what  will  be  accomplished,  including  scope  and  expected  outcomes  for  cybersecurity and  cyber  resilience  testing.    Planned  testing  should  explain  the  scope  of  “detect,  react,  and restore”  activities  that  will  be  performed  during  cyber  test.    It  is  understood  that  many  system  AFI99-103  6 APRIL 2017 65 and  subsystem  architectures  were  established  without  cybersecurity  and  cyber  resiliency requirements.    The  TEMP  should  acknowledge  these  system  limitations  and  explain  those aspects of cybersecurity and cyber resiliency that can be tested.   For some weapon systems, any cybersecurity  vulnerability  is  SECRET,  at  a  minimum;  thus,  classification  of  this  data  is pertinent  to  handling  and  reporting  procedures.  The  security  classification  of  known  or discovered  cybersecurity  vulnerabilities  should  be  conveyed  to  the  test  organization  prior  to testing and documented in the TEMP.  Create a classified annex if needed. 5.14.1.  The CDT or TM, LDTO, OTO, or OTA with cooperation from the prime contractor, will  analyze  the  SUT  design  and  security  implementation  throughout  the  acquisition  life cycle.  Cyber vulnerabilities are not exclusively defined by the RMF process.  Subject matter experts will analyze and test the attack surface to identify issues related to cybersecurity and resilience  of  military  capabilities  from  cyber  threats.    The  TEMP  should  convey  which portions  of  the  potential  attack  surface  are  being  assessed  during  DT  and  OT.    The  TEMP should provide the plan to assess user ability to detect threat activity, react to threat activity, and  sustain  mission  capability  after  degradation  or  loss.    Security  classification  of vulnerabilities must be determined and documented in the TEMP. 5.15.  RFP TEMP.  The MS A TEMP must be updated prior to release of the RFP.  The TEMP must  reflect  a  test  program  commensurate  with  system  requirements.    The  RFP  TEMP  should also  include  a  draft  CONOPS.    Director  AF/TE  will  sign  RFP  TEMPs  for  all  programs  on oversight.    If  the  program  enters  post-MS  A,  an  RFP  TEMP  must  be  created  and  staffed  for AF/TE signature. 5.16.  MS  B  TEMP  .    At  MS  B  the  TEMP  must  be  updated  to  reflect  revised  T&E  strategy developed in MS A to include emphasis on LFT&E, OT&E, cyber test, expanded use of STAT, human  machine  interface  (HMI)  testing  and  updated  Reliability  Growth  Curves  (RGC)  with  a working link to Failure Modes, Effects and Criticality Analysis (FMECA).  The PM must ensure the following are included in the MS B TEMP: 5.16.1.  DoDI  5000.02  requires  a  DEF  be  included  with  the  MS  B  TEMP.    The  DEF identifies key areas to assess progress toward achieving KPPs, CTPs, KSAs, interoperability requirements,  cybersecurity  and  cyber reliability  growth, maintainability attributes, DT objectives, and others as needed.  The DEF also correlates test events, resources, and decision supported.  See DAG TEMP format for details. requirements, resiliency 5.16.2.  An  Operational  Evaluation  Framework  (OEF)  linking  operational  test  strategy,  test events, independent variables, and test resources (traceable to test events) to ensure a robust approach in evaluating mission capability; see DOT&E TEMP Guidebook and DAG TEMP format.  The OEF table must include: 5.16.2.1.  Mission-oriented  measures  to  assess  operational  effectiveness,  suitability,  and survivability. 5.16.2.2.  Resources, schedule, and cost drivers of the test program. 5.16.3.  An  updated  table  of  variables,  range  of  applicable  values,  effects  of  the  variable, method of controlling the variable including anticipated effects on operational performance. 5.16.4.  An M&S and Verification, Validation, and Accreditation (VV&A) plan if required. 5.16.5.  An updated cyber test plan addressing the requirements set forth in paragraph 3.3.5.  66 AFI99-103  6 APRIL 2017 5.16.6.  Updated CONOPS and/or Employment Concepts. 5.16.7.  Complete test resource requirements traceable to  test events  ensuring adequacy  and availability. 5.17.  Tailored  Integrated  Documentation.  AFI  63-101/20-101  and  AFPAM  63-128 encourage  the  PM  to  tailor,  combine,  and  streamline  program  documentation  to  meet  program needs as long as specified document content, formats, and templates are followed. 5.17.1.  The  Air  Force  tailoring  concept  permits  consolidation  of  multiple  documents  (e.g., the  Acquisition  Strategy  and  acquisition  plan,  TEMP,  and  SEP)  into  fewer  documents, perhaps a single document if justifiable.  The MDA retains the authority to tailor and make the final determination of what information is covered. 5.17.2.  For  programs  not  on  the  OSD  T&E  Oversight  List,  the  PM  may  tailor  the  TEMP outline in the DAG to include critical T&E planning information from Parts II, III, and IV of the TEMP format with approval of the MDA.  The PM must include all ITT members when preparing the T&E portions of this document.  As signatories of the TEMP, the LDTO and OTO must reach concurrence on the T&E portions.  PMs may use attachments, annexes, or a web-based  site  to  ensure  all  information  is  covered.    See  AFI  63-101/20-101  and  AFPAM 63-128 for details. 5.18.  Management of T&E Data.  Accurate and efficient data collection is essential in all T&E efforts and must be planned before any testing starts.  Integrated testing requires use of common test  parameters  across  test  boundaries  for  uniform  data  collection,  scoring,  analysis,  and reporting purposes.  Testers must have a  clear understanding of their actual  data needs  and the required instrumentation to collect the data because data collection can be a major expense.  PMs and testers must safeguard classified information resulting from system development or test such as vulnerabilities identified through cyber test.  This  includes safeguarding physical  and digital data  as  well  as  communications  and  datalinks  even  when  shared  or  provided  to  other organizations. 5.18.1.  Common  T&E  Data  Management.  The  PM  will  establish  a  common  T&E database  as  early  as  practical  for  all  T&E  data  for  the  system  under  test.    The  goal  is  to leverage all available T&E knowledge about the system.  A statement about data validity and a point of contact must be attached to each data batch.  All program stakeholders will have access to T&E data on a need-to-know basis.  Classified, proprietary, competition sensitive, and  government-only  data  require  restricted  access.    The  ITT  will  ensure  that  any  RFP  or SOW supports inclusion of contractor T&E data as part of this database, as well as all T&E data from previous increments and real world operations.  To the maximum extent possible, all testers must allow open data sharing and non-interference observation by other testers, the system developer, contractor, users, DOT&E, DASD(DT&E), and the PM . 5.18.2.  Tracking  T&E  Data.  All  test  teams  establish  rigorous  data  collection,  control, accountability, and security procedures for T&E data.  To avoid using questionable test data, test teams must only use authorized databases for storing data, verify the origin and integrity of  any  data  used  in  final  reports,  i.e.,  whether  the  data  came  from  contractors,  DT&E, integrated  testing,  other  Service  OTAs,  deployed  assets  used  in  real  world  operations,  or dedicated  Air  Force  operational  tests.    T&E  data  from  deployed  early  prototypes  used  and  AFI99-103  6 APRIL 2017 67 evaluated in real world operations should be properly archived.  See paragraphs 5.17,  5.18, and  6.10 for more information. 5.18.3.  Contractor T&E Data.  Test teams and TIPTs should use as much contractor T&E data as possible if its accuracy can be verified.  Contractor T&E data should be visible and shall be clearly identifiable in the common T&E database. 5.18.4.  Operational Testers.  Operational testers may use data from sources such as DT&E, integrated testing, and OAs to augment or reduce the scope of dedicated operational testing if the data can be verified as accurate and applicable.  Per DoDI 5000.02, DOT&E reviews and approves data sources for programs on Oversight. 5.18.5.  Joint  Reliability  and  Maintainability  Evaluation  Team  (JRMET).  The PM will establish a JRMET (or similar TIPT) to assist in the reliability growth process and reliability growth  planning  and  the  collection,  analysis,  verification,  and  categorization  of  reliability, availability,  and  maintainability  (RAM)  data.    JRMET  may  also  include  Prognostics  and Health  Management  (PHM)  data.    Categorizing  is  defined  as  assignment  of  relevancy  and chargeability  of  the  data.  Scoring  is  defined  as  officially  accepting  JRMET  data  as  useable for  R&M  calculations.  A  clear,  unequivocal  definition  of  “failure”  must  be  established  for the equipment or system in relation to its performance parameters.  The JRMET also review applicable DRs and recommend whether or not they should be closed.  The PM or designated representative  chairs  the  JRMET  during  DT&E;  an  operational  test  representative  chairs during  dedicated  operational  testing.    Note:  A  Failure  Reporting  Analysis  and  Corrective Action  (FRACAS)  report  or  a  Deficiency  Review  Board  (DRB)  can  be  used  for  re-categorization of hardware and software deficiencies identified in JRMET.  See  paragraph 5.18.4 and TO 00-35D-54. 5.18.6.  Periodic Review of Test Data.  The PM and testers describe in the TEMP how they will jointly review T&E data during the system development and sustainment phases.  These should  be  periodic  government-only  reviews.    For  programs  where  AFOTEC  is  the  lead operational tester, a Test Data Scoring Board may also be used. 5.18.7.  Timely  Release  of  T&E  Data.  All  test  teams  will  release  validated  test  data  and factual  information  as  soon  as  practical  to  other  testers  and  stakeholders.    Preliminary  data may also be released, but must be clearly identified as such. 5.18.8.  Disclosing  Test  Data  to  Foreign  Nationals.  The  PM is  responsible  for recommending what test data or materials may be disclosed to foreign nationals.  Use AFPD 16-2,  Operations  Support,  Disclosure  of  Military  Information  to  Foreign  Governments  and International  Organizations,  and  AFI  61-204,  Disseminating  Scientific  and  Technical Information.    See  paragraphs  7.9  and    7.10  about  the  release  and  protection  of  test information. 5.18.9.  Data  Archiving  Strategy.  The  ITT  must  develop  a  strategy  for  collecting  and archiving  key  T&E  information  and  data  that  have  significant  record  value  for  permanent retention.   Consider the system’s importance and potential for future inquiries into baseline performance, performance variance, test design, conduct,  and how results were determined.  Retain  baseline  performance  data,  pertinent  statistical  information,  test  plans,  TEMPs, analyses,  annexes,  and  related  studies,  in  addition  to  final  reports,  to  maintain  a  complete historical picture.  68 AFI99-103  6 APRIL 2017 5.19.  Deficiency  Reporting  (DR)  Process.  All  testers  must  plan  for  identifying  deficiencies and enhancements and submitting DRs IAW AFI 63-145.  All Government testers will use JDRS for  weapon  systems  deficiency  reporting  as  described  in  TO  00-35D-54  unless  a  waiver  is approved IAW that TO.  Directions for technical data deficiencies are in TO 00-5-1, Air Force Technical Order System.  See additional information in paragraphs 6.8 and  6.10. 5.19.1.  Responsible  Agent.  The  PM  has  overall  responsibility  for  establishing  and administering  a  DR  process  and  tailored  procedures  for  reporting,  screening,  validating, evaluating, tracking, prioritizing, and resolving DRs originating from all sources.  A waiver must  be  obtained  from  HQ  AFMC/A4F  if  the  required  DR  system  is  not  used.    If  a contractor-based  DR  system  is  planned  as  the  system  of  record,  the  RFP  and  SOW  must require  the  contractor’s  DR  system  to  satisfy  the  purpose  and  intent  of  the  TO,  provide visibility to MAJCOM Functionals, cross service components, HQ AFMC, and describe how the process will remain under Government cognizance. 5.19.2.  When  to  Start  Reporting  DRs.  The  ITT  determines  the  optimum  time  to  begin submitting DRs to the program’s DR system.  The program’s DR system must be populated in advance of any OT&E readiness certification or fielding decision to allow the user, OTO, and  Operational  Accepting  Authority  sufficient  time  to  assess  the  impact  of  known deficiencies on system performance.  DRs should be promptly reported once formal reporting begins; however, a Watch Item (WIT) tracking system may be used to ensure sufficient data are  collected  for  accurate  reporting.    The  contractor-based  DR  system  may  suffice  for  the early stages of development, but the government-based DR system must become the primary method of reporting and tracking DRs during government-conducted T&E. 5.19.3.  Accurate  Categorization  of  DRs.  When  submitting  or  screening  DRs,  all  testers must ensure the DR’s severity is accurately represented by assigning the proper category as defined in TO 00-35D-54.  Government testers must clearly distinguish between DRs which cite deficiencies and those which cite enhancements going beyond the scope of the system’s operational requirements. 5.19.4.  DR  Tracking  and  Management.  DT&E  and  OT&E  test  directors  periodically convene  a  local  DRB  to  review  the  prioritization,  resolution,  and  tracking  of  all  open  DRs and WITs.  The DT&E test director chairs the DRB during DT&E phases, and the OT&E test director chairs the DRB during OT&E phases.  Both test directors, plus representatives from the  PTOs  and  using  MAJCOMs  are  members  of  the  PM’s  MIPRB  which  provides  final resolution  of all DRs.  The  ITT periodically  convenes a JRMET to  review DRs  focused on reliability, maintainability, and availability. 5.19.5.  Prioritizing  DRs.  Prioritized  DRs  are  used  in  preparation  for  certification  of readiness for dedicated operational testing.  If the PM cannot correct or resolve all Category I and  II  DRs  before  dedicated  operational  testing  begins,  or  defers  fixes  for  these  DRs, operational  testers  and  users  must  assess  the  impacts.    The  PM  and  ITT  must  reach agreement  prior  to  certification  of  readiness  for  operational  testing  and  develop  a  plan  for resolution and subsequent testing. 5.19.6.  Classified DRs.  Since JDRS lacks capability to handle classified DRs, an alternative DR  system  may  be  necessary.    The  PM  will  establish  and  maintain  procedures  to  manage classified  or  sensitive  DRs  IAW  AFI  31-401,  Information  Security  Program  Management.  Coordinate  with  the  applicable  program  office  representative  before  handling.    Produce,  AFI99-103  6 APRIL 2017 69 handle, store, transmit and destroy classified documents according to the applicable program security classification guide. 5.20.  DRs  for  Cyber  Vulnerabilities.  When addressing  cyber vulnerabilities for systems,  use the  impact  codes  and  severity  categories  in  DoDI  8510.01.    Severity  categories  expressed  as category  (CAT)  I,  CAT  II,  and  CAT  III  indicate  the  risk  level  associated  with  each  security weakness and the urgency of completing corrective action.  Severity categories are assigned after considering  the  architecture  limitations  and  mitigation  measures  that  have  been  implemented within  the  system  design  (Residual  Risk).    Mission  critical  components  containing  exploitable cyber  vulnerabilities  should  receive  priority  in  remediation  or  mitigation  regardless  of  severity category.  Deficiencies discovered during cyber test should be marked and handled according to the  security  classification  of  the  data.    Also  see  DoDI  8500.2  for  details  about  selecting  and implementing security requirements, controls, protection mechanisms, and standards. 5.20.1.  DoDI  8510.01  assumes  vulnerabilities  (i.e.,  deficiencies)  will  be  present  and addressed on a continuing basis.  These items are maintained in the program Plan of Action and  Milestones  (POA&M)  that  supports  the  RMF  process.    These  vulnerabilities  are  not necessarily reported using the TO 00-35D-54 reporting system. logically  grouped react, (e.g.,  protect,  detect, 5.20.2.  When assessing cyber vulnerabilities as potential DRs, a separate DR is not needed for  every  identified  control,  shortfall,  or  finding.    Depending  on  the  severity,  cyber vulnerabilities  should  be restore, confidentiality,  integrity,  or  availability).    A  standard  way  of  reporting  vulnerabilities  and when  they  qualify  as  a  DR  should  be  developed  and  described  in  the  TEMP.    One  way  of doing this is described in AFPAM 63-128, Guide to Acquisition and Sustainment Life Cycle Management,  Table  A6.8.1,  Software  Severity  Levels  and  Weights.    Alternatively,  use  the following  documents  to  assess  risk  for  proper  DR  and  vulnerability  categorization: Committee on National Security Systems Instruction (CNSSI) 1253, Security Categorization and  Control  Selection  for  National  Security  Systems;  NIST  SP  800-30  rev  1,  Guide  for Conducting  Risk  Assessments;  NIST  SP  800-39,  Managing  Information  Security  Risk;  and NIST SP 800-53A rev 1,  Guide for  Assessing the Security Controls  in Federal  Information Systems and Organizations. 5.20.3.  Cyber  vulnerabilities  identified  during  DT&E  and  OT&E  will  be  reported  as observed potential vulnerabilities to the confidentiality, availability, integrity, authentication, and non-repudiation  of a system.   Some vulnerabilities that rise to  the level  of a deficiency will  equate  to  materiel  solution  defects  (design  and/or  documentation)  when  they demonstrate or have potential for definitive mission impact.  Ensure these vulnerabilities are documented,  assigned  appropriate  security  classification,  vetted,  and  tracked  as  a  DR according to TO 00-35D-54, as well as in the Plan of Actions and Milestones (POA&M). 5.21.  Independent Technical, Environmental and Safety Reviews.  Independent government technical,  environment,  and  safety  personnel  examine  the  technical,  environment,  and  safety aspects  of  T&E  plans  that  involve  government  resources  prior  to  commencement  of  test activities.  All test organizations must establish procedures for when and how these reviews are accomplished.    These  groups  function  as  necessary  throughout  the  acquisition  and  sustainment process until the system is demilitarized. 5.21.1.  Technical  Reviews.  Technical reviews assess the soundness of system designs and test  plans  to  reduce  test  risk.    Technically  qualified  personnel  with  test  management  70 AFI99-103  6 APRIL 2017 experience, but  who are independent of the test program, will perform these reviews.  As  a minimum,  technical  reviews  will  assess  test  requirements,  techniques,  approaches,  and objectives. 5.21.2.  Environmental  Reviews.  Environmental  reviews  assess  the  requirement  for  an environmental  impact  analysis  per  32  CFR  Part  989  based  on  planned  test  activities  and assess the impacts of previous reviews on current activities.  Documents generated by these reviews (e.g. Environmental Impact Statements) must be forwarded to the Program Office to facilitate the Program Office’s NEPA/E.O. 12114 Compliance Schedule and provided to test personnel for hazard evaluation. 5.21.3.  Safety  Reviews.  Safety  reviews  assess  whether  the  T&E  project's  safety  plan  has identified  and  mitigated  all  health  and  safety  risks.    Safety  review  members  must  be technically  qualified  and  independent  of  the  test  program.    Test  organizations  will  identify risks.    All  test  organizations  will  set  up  procedures  for  controlling  and  supervising  tests consistent with the risk involved and according to local range safety criteria.  In addition, the PM  will  provide  a  Safety  Release  to  the  LDTO  or  OTO  prior  to  any  testing  involving personnel  IAW  DoDI  5000.02,  Enclosure  4.    Mishap  accountability  must  be  clearly established IAW AFI 91-204, Safety Investigations and Reports, prior to conducting tests. 5.21.4.  Nonnuclear  Munitions  Safety  Board  (NNMSB).  The  NNMSB  reviews  and assesses all newly developed live, uncertified munitions, fuses, and initiating devices prior to airborne testing or release IAW AFI 91-205, Nonnuclear Munitions Safety Board. 5.21.5.  Directed Energy Weapons Certification Board (DEWCB).  The DEWCB reviews and  certifies  all  directed  energy  weapons  prior  to  operational  assessment,  test  and  training use IAW AFI 91-401, Directed Energy Weapons Safety. 5.22.  Test  Deferrals,  Limitations,  and  Waivers.  A  test  deferral  is  the  movement  of  testing and/or evaluation of a specific CTP, operational requirement, or COI to a follow-on increment or test activity (e.g., FOT&E).  A test limitation is any condition that hampers but does not preclude adequate  test  and/or  evaluation  of  a  CTP,  operational  requirement,  or  COI  during  a  T&E program.  The ITT documents test deferrals and test limitations in the TEMP and test plans.  Test limitations and test deferrals do not require waivers, but must be described in the TEMP and test plans, to include, in the case of a deferral, a revised timeline for decisions and reports.  These test limitations  and  deferrals  are  considered  approved  when  the  TEMP  or  test  plan  is  approved.  Waivers are the deletion of specific mandatory items; waivers for not conducting OT&E will not be approved when OT&E is mandated by statute or this AFI.  See Attachment 1 for definitions and paragraph 6.4.3 for more details.  AFI99-103  6 APRIL 2017 71 Chapter 6 T&E ACTIVITIES IN SUPPORT OF MILESTONE C AND BEYOND 6.1.  Post MS B.  The most important activities after the MS B decision and during the EMD and Production  and  Deployment  phases  are  shown  in  Figure  6.1.    This  chapter  focuses  on  test execution  supporting  the  MS  C,  FRP/FD  decisions.    Sustained,  high  quality  tester  activity  and collaboration  with  all  program  stakeholders  must  continue.    The  ITT  and  individual  test  teams implement integrated test plans and activities and report T&E results to decision makers. Figure 6.1.  Integration of Requirements, Acquisition, and T&E Events Supporting MS C and Beyond. 6.2.  Refining  the  ITC  in  the  TEMP.  The  ITT  should  continue  refining  the  ITC  within  the TEMP to support the development of test plans that are integrated.  Building on the work done in previous  TEMPs,  continue  refining  the  COIs,  CTPs,  test  objectives,  MOEs,  MOSs,  MOPs, resources,  schedules  as  necessary,  and  update  the  OEF.    The  TEMP  and  operational  test  plans must  incorporate  any  new  validated  threats  and,  or  environments  that  may  impact  operational effectiveness.    Test  teams  continue  planning  for  execution  of  test  plans  that  are  integrated, covering  as  many  DT&E,  and  operational  test  objectives  as  possible  prior  to  dedicated operational  testing.    A  series  of  OAs  should  be  integrated  into  the  test  program  to  reduce program risk. T&E and systems engineering practitioners use STAT methodologies to optimize   72 AFI99-103  6 APRIL 2017 the overall number of test events and test articles without compromising test objectives.  Tester activities during the EMD phase and beyond help identify performance shortfalls and other areas that could cause unintended increases in development, operations, and life cycle costs.  The ITC should describe M&S tools and DSMs for test design, systems engineering, and data evaluation, and  how  these  supplement,  augment,  and  extrapolate  empirical  T&E  data  wherever  practical.  Description of the VV&A of all models should be included. 6.3.  Developing Test Plans That Are Integrated.  The ITC integrates all individual contractor and government test plans into a linked series of evaluations compatible in objectives, schedule, and  resources.    These  plans  are  focused  on  the  current  increment,  with  follow-on  increments described  in  lesser  detail.    A  single  program  test  plan  is  not  required.    The  ITT  must  plan  for OAs  intermingled  with  operationally  relevant  DT&E  to  produce  increasing  amounts  of operationally relevant data within each increment. 6.3.1.  Operational  Assessments.  One or more OAs, if appropriate, should be planned and conducted early enough in the EMD phase to provide operational inputs to requirements and system development prior to MS C.  OAs must be tailored to emphasize an integrated testing approach for assessing system capabilities in preparation for dedicated operational testing. 6.3.2.  Integrated Testing.  Integrated test plans should support each increment with DT&E and  one  or  more  OAs  if  appropriate.    These  plans  should  address  as  many  of  the  COIs, MOEs, and MOSs as possible to provide timely, credible, and continuous feedback must be provided  to  developers,  users,  and  decision  makers  before  dedicated  operational  testing begins. 6.3.3.  Specialized Testing.  Specialized types of T&E described in Table 3.2 required to be completed  by  MS  C  should  be  designed  to  support  dedicated  operational  testing  that concentrates  on  mission  impacts  and  unanswered  COIs,  MOEs,  MOSs,  and  MOPs.    The dedicated operational test plan may use operationally relevant data collected during previous testing to verify capability requirements in the approved CPD for the fielded item. 6.4.  Realistic  Testing.  This  AFI  implements  DoDI  5000.02  and  10  U.S.C.  §  2399  which require  the  conduct  of  realistic  operational  tests  in  a  realistic  operational  environment,  using production  representative  articles,  to  evaluate  a  system’s  overall  effectiveness  and  suitability, and  to  assess  impacts  to  wartime  and  peacetime  operations.    See  descriptions  of  operational testing in the DAG. 6.4.1.  Threats  and  Capabilities.  To  support  MS  C  or  any  deployment  decisions,  the  ITT must  ensure  test  plans  are  updated  to  include  new  validated  threats,  enemy  TTPs, environments as well as any added capability requirements. 6.4.2.  Virtual  Test  Environment  .    Systems  with  large  IT  content  and  DBS  should  use  a "virtual" environment whenever possible that emulates real-world networks and threats. 6.4.3.  Deferment of Operational  Testing.  Operational testers will not defer testing of any KPPs,  COIs,  or  operational  requirements  to  future  increments  unless  planned  for  in  the Acquisition Strategy and ITC portion of the TEMP.  If an unplanned deferral is unavoidable at  the  MS  C  or  FRP/FD  decision,  the  PM  will  consult  with  the  using  command  and requirements  authorities  to  decide  on  the  best  strategy  for  completing  the  deferred  testing.  The decision is  documented in  an  approved ADM and TEMP,  and an  OT&E  waiver is  not required. See paragraphs 4.11 and  5.22.  AFI99-103  6 APRIL 2017 73 6.4.4.  Support  of  AFOTEC-Conducted  Operational  Testing.  MAJCOM  operational units,  test  centers,  complexes,  and  other  DT&E  organizations  may  be  requested  to  support AFOTEC-conducted operational testing.  This support is documented in TEMPs, TRPs, ITT charters, test plans, MOAs, and directed in MAJCOM test project orders.  AFOTEC prepares TRPs in time to budget during the POM cycle. 6.4.5.  Tests Involving Personnel.  If personnel are used as test subjects, the level of risk to the  person  must  be  documented  IAW  DoDI  3216.02,  Protection  of  Human  Subjects  and Adherence to Ethical Standards in DoD-Supported Research.  The PM will provide a Safety Release  to  the  LDTO  and/or  OTO  prior  to  any  testing  involving  personnel.    Aircrew  or maintenance  personnel  used  to  evaluate  a  SUT  are  not  considered  test  subjects  unless  the SUT  is  aircrew  flight/maintenance  equipment/apparel  or  similar,  potentially  impacting human performance.  See AFMAN 63-119, Attachment 23, for additional information. 6.5.  Certification  of  System  Readiness  for  Dedicated  Operational  Testing.  The  PM  will implement the Certification of System Readiness for Dedicated Operational Test review process described in  AFMAN 63-119 as early  as practical  during the EMD phase.  Developmental and operational  testers  participate  and  assist  the  PM  in  preparation  for  OT&E,  and  carrying  out responsibilities  as  agreed.    The  readiness  certification  is  mandatory  for  all  ACAT  programs, including  in  the  operations  and  maintenance  phase  (e.g.  modifications  and  sustainment)  where OT&E  will  support  a  deployment  of  FRP/FD  decision.    Non-ACAT  programs  or  projects  are highly encouraged to follow this process; however, the OT&E certification can be tailored to suit the scope and criticality of the program.  The process and reporting of results may be tailored to suit program objectives as long as they comply with the requirements of AFMAN 63-119. 6.5.1.  OT&E  Certification.  For  programs  on  the  DOT&E  Oversight  List,  the  SAE determines  system  readiness  for  IOT&E.    The  DOT&E  Oversight  List  is  found  at  the following  link;  https://extranet.dote.osd.mil/oversight/index.html.    For  other  programs, the MDA is the OT&E Certification official.  The SAE or MDA may delegate this authority (via  Acquisition  Decision  Memorandum)  to  the  responsible  PEO.    OT&E  Certification Officials for smaller programs originating at MAJCOM or Center levels may be delegated by MDA to a subordinate level as appropriate.  Under no circumstance shall a PM be the OT&E Certification Official for his/her own program.  The OT&E Certification Official determines the  overall  scope  and  schedule  for  the  operational  test  readiness  review  and  certification process IAW AFMAN 63-119.  The Certification Official and the planned implementation of the certification process will be identified in the TEMP. 6.5.2.  The Readiness Certification Process.  To be certified ready for dedicated operational testing, the system must be mature, production and operationally representative; demonstrate stabilized  performance  in  an  operationally  relevant  environment;  and  all  necessary  test support must be available as planned.  The certification process must be a continuous effort, not a single event in time.  Multiple reviews at logical waypoints in a program are strongly encouraged such as prior to each OA and milestone decision point.  COIs, MOEs, MOPs, and MOSs  must  be  reviewed  for  relevance  and  achievability  before  entering  dedicated  OT&E.  The system must have a high likelihood of a successful operational test.  Identified shortfalls or DRs will be remedied before dedicated operational testing starts or work-around solutions will be developed, negotiated and documented between the PM, user, and operational testers.  Automated certification process tracking tools for all templates found in AFMAN 63-119 are available  on  the  AF/TE  portion  of  the  Air  Force  Portal  at:  https://www.my.af.mil/gcss- 74 AFI99-103  6 APRIL 2017 af/USAF/ep/browse.do?programId=t6925EC2D581C0FB5E044080020E329A9&channelPageId=s6925EC1355030FB5E044080020E329A9.    Modify  these  tools  as  needed  to match any changes made to the templates. 6.5.3.  Program  Assessment.  DASD(DT&E)  provides  program  assessments  for  decision points including RFP, MS B, and MS C.  This assessment is updated to support the OTRR or if requested by the MDA or PM for MDAPs, MAIS programs and USD(AT&L)-designated special  interest  programs.    The  PM  should  work  with  the  DASD(DT&E)  representative  on the  ITT  to  synchronize  conduct  of  the  final  AFMAN  63-119  certification  review  and program assessment to avoid duplication of effort. 6.5.4.  Final  Certification  of  Readiness  for  Dedicated  Operational  Testing.  Final certification  review  and  briefing  of  system  readiness  must  be  completed  45  calendar  days prior  to  the  planned  start  of  dedicated  operational  testing  to  allow  time  for  last  minute program  adjustments  or  deficiency  corrections.    This  time  may  be  shorter  if  the  PM  and operational  testers  mutually  agree.    Certification  requires  a  formal  briefing  (or  less,  if justified  by  program  scope,  OSD  interest,  etc.)  to  the  OT&E  Certification  Official.    The briefing  shall  address  DT&E  results,  conclusions,  recommendations,  identified  deficiencies and  workarounds,  and  an  assessment  of  the  system’s  capability  to  meet  operational requirements.  Workarounds will be vetted by the OTO or requirements sponsor.  AFMAN 63-119  will  be  used  as  a  guide  to  structure  the  briefing  and  demonstrate  readiness.    Both operational  testers  and  developmental  testers  are  represented  at  the  briefing.    The  briefing shall inform the OT&E Certification Official of any outstanding disagreements between the OTO,  user,  and  the  PM.    The  OT&E  Certification  Official  forwards  a  certification  of readiness  memo  to  the  OTO  commander  at  least  15  days  prior  to  the  start  of  dedicated operational testing, or as agreed. 6.5.5.  OT&E  Readiness  Agreement.  The  PM,  user,  and  operational  testers  must coordinate  regularly  throughout  the  system’s  development  to  address  OT&E  readiness shortfalls.  PMs, jointly with their OT&E counterparts, shall provide the OT&E Certification Official  detailed  mitigation  strategies  for  open  shortfalls  found  during  DT&E,  and  will identify  outstanding  disagreements  on  OT&E  readiness  between  the  OTO,  user,  and  the program office prior to the formal certification briefing.  The OT&E Certification Official is responsible  for  weighing  all  factors  before  certifying  readiness,  and  it  is  the  PM’s responsibility to  ensure the OT&E Certification Official is  made fully  aware of  all areas of OTO,  user,  and  program  office  concern.    In  all  cases,  identified  shortfalls  or  DRs  must  be either  remedied  before  dedicated  operational  testing  starts,  or  mitigated  via  agreement  or workarounds  negotiated  between  the  PM,  user,  and  operational  testers.    If  necessary,  the OT&E  Certification  Official  and  OTO  equivalent  counterpart  shall  negotiate  and  plan  the OT&E  way  forward  before  formalizing  the  certification  of  readiness  memo.    If  agreement cannot  be reached at  this point, outstanding issues may be elevated to  SAF/AQ and AF/TE for final resolution. 6.5.6.  Considerations  for  Early  Deployment  of  Prototypes  .    Use  the  applicable certification  templates  in  AFMAN  63-119  to  review  the  system’s  capabilities,  limitations, and readiness prior to early operational deployment of prototypes, UONs, JEONs, QRCs, and JCTDs.  AFI99-103  6 APRIL 2017 75 6.5.7.  Certification  for  Systems  with  Multiple  Increments  or  Releases.  If  a  system  is fielded in multiple releases or increments (common with IT and software intensive systems), then the PM ensures the OT&E Certification Official provides a certification of readiness to the  OTO  commander  prior  to  the  decision  to  commence  operational  testing  of  each individual  release.    The  certification  should  be  tailored  to  and  pertain  specifically  to  the planned  release  of  capability.    For  example,  IT  systems  using  rapid  release  methodologies may  substantially  compress  their  certification  schedule  and  reduce  the  number  of certifications and templates reviewed.  Releases may require substantially less time and effort than an increment. 6.6.  Plans  and  Briefings  for  Operational  Testing.  DOT&E  requires  operational  testers  (i.e., the OTO) to submit written plans and present briefings as discussed below for programs on OSD OT&E  Oversight.    The  information  requirements  below  apply  in  full  to  AFOTEC  and MAJCOMs unless DOT&E relief is documented.  See Attachment 2 for a summary. 6.6.1.  Operational  Test  Concept  Briefings.  DOT&E  requires  a  test  concept  briefing  a minimum  of  180  days  before  the  start  of  dedicated  operational  tests  and  assessments  for programs  on  OSD  OT&E  Oversight  IAW  DoDI  5000.02.    AF/TEP  should  arrange  for corporate  Air  Force-level  reviews  of  test  concept  briefings.   User  and  developer representatives are required to attend these briefings.  For multi-Service programs, the other Services  will  also  be  invited.    A  pre-brief  to  the Air  Staff  may  be  required  before  going  to DOT&E.  Coordinate  with  AF/TEP  for  pre-brief  requirements.    DOT&E  may  elect  to  defer this requirement and accept a later briefing of the final operational test plan in lieu of the test concept briefing. 6.6.2.  Operational Test Plans and Test Plan Briefings.  An operational test plan should be delivered  with  sufficient  time  for  development  testers  to  ensure  the  system  is  ready  for operational test and is due to DOT&E a minimum of 60 days prior to test start.  DOT&E may request, or the OTO may elect, to present a briefing to accompany the final test plan.  This briefing will be coordinated the same way as an operational test concept briefing. 6.7.  OSD  Involvement.  Programs  on  DT&E,  LFT&E,  and/or  OT&E  Oversight  remain  under continuous OSD surveillance through fielding and into sustainment until removal from the OSD T&E  Oversight  List.    The  ITT  must  be  prepared  for  additional  briefings  to  OSD  and  test  plan approvals as described in paragraph 4.7.  Additional briefings requested by DOT&E should be routed  through  AF/TEP  before  submission  to  OSD.    The  information  required  for  OSD  T&E Oversight programs is summarized in Attachment 2. 6.8.  Operational  Tester  DR  Responsibilities.  Prior the  FRP/FD  decision  review, operational  testers  and  users  complete  a  final  prioritization  of  all  open  DRs  for  resolution  and funding.    The  MAJCOM’s  priorities  must  be  used  for  rank-ordering  these  DRs.    The  final priorities are forwarded to the PM to help direct corrective actions and will be listed in the final report. to 6.9.  Interoperability  Certification  Testing.  Comprehensive  interoperability  testing  which involves  system  testing  in  an  operationally  realistic  environment  must  be  completed  and interoperability  certification  granted  by  JITC  or  comparable  authority  before  an  IT  system, upgrade, or capability can be fielded.  76 AFI99-103  6 APRIL 2017 6.10.  Tracking and Closing DRs.  Not all open DRs may receive funding or be corrected after a  system  is  accepted  for  operational  use.    The  database  of  open  DRs  may  provide  the  only documentation of unsatisfactory conditions or worthwhile system enhancements.  At no time will the  program  office  unilaterally  close  or  downgrade  DRs  without  formal  consultation  with  the originating  test  organization  and  MAJCOM  project  officer.    MAJCOM  project  officers  must continue to track open DRs until they are corrected, or the MAJCOM concurs with closing them.  DRs  closed  due  to  lack  of  resources  should  be  annotated  and  tracked  until  the  root  cause  or deficiency is corrected or mitigated. 6.11.  Modifications.  Modifications change the  form, fit, function, and/or interface (F3I) of an in-service,  configuration-managed  AF  asset.    Modifications  may  be  temporary  or  permanent.  See AFI 63-101/20-101 for more detail on modification types. 6.11.1.  Temporary-1  (T-1)  modifications  change  the  configuration  to  enable  short-term operational  mission  accomplishment.    T-1  modifications  typically  use  COTS  or  NDI.    The PM for the system has configuration control of the system and is responsible to evaluating, integrating,  and  installing  these  modifications.    In  conjunction  with  PM,  the  MAJCOMs conduct testing of the installed modification to ensure OSS&E is not compromised. 6.11.2.  Temporary-2  (T-2)  modifications  may  involve  installation  of  T&E  support equipment to obtain data for DT&E and OT&E.  Test organizations and the PM must ensure OSS&E of T-2 modified assets. 6.11.3.  Permanent  modifications  that  change  the  configuration  of  an  asset/software  for operational  effectiveness,  suitability,  survivability,  safety,  service  life  extension,  and/or reduce ownership costs of a fielded weapon system, subsystem, or item and must follow the AF  Form  1067,  Modification  Proposal  Process  found  in  Attachment  2  of  AFI  63-101/20-101, and the AF/A5R Requirements Development Guidebook and may require an additional amount of DT/OT testing prior to fielding. 6.12.  Integrated  Testing  During  Sustainment  and  Follow-on  Increments.  Follow-on increments and modifications continue in parallel with and subsequent to acquisition of the first increment.  OT&E is required for each increment of capability prior to release to the user.  This testing  is  structured  according  to  the  program’s  Acquisition  Strategy,  TEMP,  and  updated requirements documents. 6.12.1.  The existing ITT should continue functioning to ensure continuity of acquisition and T&E operations.  All areas of the ITT charter should be carefully reviewed and modified as necessary. 6.12.2.  The  T&E  activities  described  in  Chapter  4,  Chapter  5,  and  Chapter  6  must  be tailored  for  risk,  new  or  revised  JCIDS  requirements,  and  other  factors,  and  repeated  as needed  during  the  Operations  and  Support  phase.    Testers  should  capitalize  on  previously completed  work  products,  TTP,  analyses,  results,  and  lessons  learned,  thus  eliminating redundant  testing  and  work.    Sustainment  acquisitions,  to  include  support  equipment  and Form,  Fit,  Function,  and  Interface  (F3I)  replacements,  require  FRP/FD  decisions  and  an appropriate type of operational testing. 6.13.  Disposing  of  Test  Assets.  Test  assets  (e.g.,  instrumentation  and  test  articles)  from canceled  or  completed  tests  are  catalogued  and  returned  to  government  T&E  organizations  or  AFI99-103  6 APRIL 2017 77 acquisition  or  sustainment  programs,  or  refurbished  and  reassigned  to  owning  MAJCOMs.  Surplus or unusable items are sent to the applicable Defense Reutilization Management Office. 6.14.  OT  Reporting  on  Fielding  of  Prototypes  or  Pre-Production  Systems.  Warfighter operational needs may require rapid and/or early fielding of new capabilities.  This may result in early  operational  use  of  prototypes,  technology  demonstration  systems,  test  articles,  or  pre-production systems prior to the completion of required dedicated operational testing and formal production decisions.  In these situations, the OTO (as determined in paragraph 4.6) may opt to produce  a  C&L  Report  to  inform  the  warfighter  and  fielding  decision  authorities.    The  C&L Report  provides  the  most  current  operational  test  perspective  on  developmental  system capabilities  and  limitations  based  on  testing  done  to  date.    See  paragraph  7.5  for  more information about C&L Reports.  78 AFI99-103  6 APRIL 2017 Chapter 7 TEST AND EVALUATION REPORTING 7.1.  General  Reporting  Policy.  Test  reports  must  be  timely,  factual,  concise,  and  tailored  to the  needs  of  decision  makers.    They  should  be  delivered  in  time  to  support  the  designated milestone or decision review including addressing the need for having approved test reports prior to  advancing  to  the  next  tier  of  formal  testing  during  DT&E.    All  T&E  plans  describe  which kinds of reports are required, their contents, and when and to whom they are submitted.  All test reports contain evaluations of test results and conclusions.  Additional findings, considerations, remaining  risks,  test  limitations  and  recommendations  are  not  required  but  may  be  included  if deemed  appropriate.    All  reports  must  be  properly  archived  and  retrievable  for  future  use.  Reporting requirements for programs on OSD T&E Oversight are summarized in Attachment 2.  All days are “calendar days” unless otherwise stated. 7.2.  DT&E  Reports.  The  types  and  frequency  of  DT&E  reports  are  tailored  to  meet  decision makers’  requirements  as  documented  in  the  TEMP  and  test  plan.    DT&E  data  and  analytic support (i.e., “reports”) must be provided to the program  decision review process  to  certify the system  is  ready  for  dedicated  IOT&E.    LFT&E  reports  must  be  submitted  to  DOT&E  45  days prior to the beyond-LRIP decision review.  The PM documents requirements for contractor test reports in the CDRL.  Formal briefings are generally not required. Quarterly LTDO assessments are required for oversight programs per paragraph 2.18.3.3 using template found in the AF/TE Guide. 7.3.  DT&E Report Distribution.  The ITT will develop a distribution list for all DT&E reports which  includes  operational  testers,  ETOs,  PTOs,  PEO,  applicable  MAJCOMs,  AF/TE,  Center Test  Functional  Leaders,  and  DTIC.    DT&E  reports  are  not  releasable  to  non-government agencies without prior approval and coordination of the PM.  Release of contractor test reports may be subject to restrictions in the contract.  For OSD T&E Oversight programs, the PEM will send a copy through appropriate channels to DASD(DT&E) and DOT&E if required. 7.4.  Operational Test Reports. 7.4.1.  Significant  Test  Event  Reports.  These  reports  briefly  describe  the  results  of significant  test  events  during  operational  test  activities.    Operational  testers  submit  these reports  to  the  appropriate  agencies  (e.g.,  PM,  CDT  or  TM,  LDTO,  PTOs,  operational MAJCOM, PEM, PEO, Center Test Functional leaders, AF/TE, and/or DOT&E, depending upon  level  of  interest  in  the  program)  within  24  hours  of  any  significant  test  event  as described in the test plan. 7.4.2.  Final Reports.  Final reports should normally be delivered not later than 45 days prior to  the  supported  decision  in  order  to  provide  adequate  time  for  review.    Delivery  timelines may  be  tailored  to  accommodate  accelerated  test  schedules  for  specific  user  needs  if coordinated  with  the  decision  review  authority.    Reports  must  address  each  of  the  COIs  as well as the system’s operational effectiveness and suitability.  These reports must strike the proper  balance  between  system  capabilities  and  limitations  while  taking  into  account  how well  the  system  performed  mission  essential  tasks.    When  appropriate,  a  production  or fielding recommendation may be included for IOT&E, QOT&E, FOT&E, OUE, SOTR, and FDE final reports.  All Category I DRs and the most important Category II DRs will be listed  AFI99-103  6 APRIL 2017 79 to  include  a  Risk  Assessment  of  the  overall  state  of  the  DR  issues.    Detailed  technical information  should  be  published  in  separate  data  documents.    Final  report  briefings  are provided to HQ USAF staff and OSD, as requested. 7.4.3.  Interim  Reports.  Decision  makers  may  require  written  information  about  T&E results during execution of an ongoing test plan or prior to the publication of the final report.  Use these types of interim reports depending on the need. 7.4.3.1.  Status  Reports.  A  status  report  provides  updates  and  important  test  findings during operational testing.  Status report format  and content are flexible.  Status reports are normally very short (no more than several pages) and should not be written as a mini final  report.    It  may  be  periodic  (monthly,  quarterly,  or  as  required),  associated  with specific (planned test) events, or in response to an external organization or agency request for  test  status.    Status  reports  may  be  used  to  inform  fielding  decisions  associated  with each release when an OT&E, OUE, FDE, or OA report is not required or applicable.  The operational test plan should document the requirements for a status report to include the frequency and distribution for periodic status reports. 7.4.3.2.  Interim  Summary  Reports.  If  the  final  report  cannot  be  ready  in  time  to support  a  key  decision,  the  decision  authority  may  instead  accept  a  written  summary report  or  a  formal  briefing.    For  oversight  programs,  AF/TE  will  help  establish  a  new final report due date.  If a briefing is used, a separate written interim summary report is not required.  Any additional data collected is added to the final report when available. 7.4.4.  MOT&E  Final  Reports.  The  lead  OTO  prepares  a  single  MOT&E  final  report aggregating  all  OT&E  information  from  the  participating  Services’  inputs.   Each participating  Service  has  the  option  of  preparing  its  own  supplemental  report  as  an attachment  to  the  single  MOT&E  report.    All  significant  differences  between  Service  test results should be explained.  This guidance also applies to testing with other DoD or Federal agencies.  See the Memorandum of Agreement (MOA) on Multiservice Operational Test and Evaluation  (MOT&E)  and  Operational  Suitability  Terminology  and  Definitions.    A  single integrated  multi-Service  report  will  be  submitted  no  later  than  90  calendar  days  after  the official end of test is declared by the Lead OTA but no later than 45 calendar days prior to a milestone decision or the date announced for the final decision to proceed beyond Low-rate Initial Production.  Briefings will be provided to HQ USAF staff and OSD as requested. 7.4.5.  Reporting  SOTR  Results.  Each  MAJCOM  may  develop  its  own  SOTR  report format as needed.  All conclusions and related recommendations based on the SOTR will be formally  documented.    All  data  and  data  sources  used  to  conduct  the  SOTR  should  be identified.  See paragraphs 3.5.11 and  4.6.6.3. 7.5.  Capabilities  and  Limitations  (C&L)  Reports.  While  not  mandatory,  the  C&L  report  is appropriate  when  a  system  or  prototype  is  provided  to  units  for  training  in  preparation  for fielding, or when the system is deployed directly to an operational unit.  A C&L report may also be appropriate to support MAJCOM UON or Joint (JUON, JEON) requests, or combat capability documents  (CCD).    To  ensure  maximum  flexibility,  C&L  reports  have  no  prescribed  format.  The  level  of  detail  provided  varies  depending  on  the  amount  of  pre-existing  information available, the warfighter’s need for technical information, and the amount of time and resources available to conduct additional testing before the fielding decision.  The C&L report should not make  specific  recommendations  concerning  the  system  fielding  decision  or  release  for  training  80 AFI99-103  6 APRIL 2017 purposes.  This report may be provided to DOT&E to support their requirement in 10 U.S.C. § 2399 for an early report to Congress. 7.5.1.  C&L  reports  are  based  on  existing,  verifiable  T&E  data  (contractor,  developmental, and  operational)  derived  from  all  available  system  development,  ground,  and  flight  test activities.    The  goal  is  to  help  warfighters  gain  early  knowledge  of  potential  operational effectiveness  and  suitability  of  systems  that  have  not  yet  completed  dedicated  operational testing.  Release of a C&L report does not obviate the requirement for dedicated OT&E.  Six months  after  publication  of  the  C&L  report,  the  OTO  should  review  program  status  to determine  whether  an  updated  C&L  report  is  necessary.    C&L  reports  will  not  drive  new testing requirements for a system. 7.5.2.  All  relevant  data  sources  used  to  develop  the  C&L  report  should  be  identified.  Include a program description and a summary of the current phase of formal system testing.  The  report  should  identify  observed  system  capabilities  and  limitations  and  describe  any areas  of  untested  or  unknown  capabilities.    Suitability  observations,  interoperability considerations, and cyber issues should also be included.  The type and scope of planned, but not  yet  accomplished, testing should also  be described.   If time is  available for a dedicated operational  test  event  such  as  an  OUE,  then  that  alternative  would  obviate  the  need  for  a C&L report.  If an operational test event is in progress or recently completed, a status report or interim summary report may be more appropriate. 7.6.  AT  Reports.  SAF/AQLS  provides  an  independent  AT  evaluation  report  directly  to  the MDA following AT V&V testing. 7.7.  Operational  Test  Report  Distribution.  Operational  testers  send  reports  to  the  program stakeholders and DTIC as determined by the ITT.  For OSD OT&E Oversight programs, AF/TE will  forward  copies  to  DOT&E  and  DASD(DT&E).    A  summary  of  operational  test  reporting requirements is in Attachment 2. 7.8.  Briefing  Trail.  AF/TE  will  arrange  for  Air  Force-level  review(s)  of  test  report  briefings.  For multi-Service programs, the other participating Services will be invited to the briefing.  The PM must be prepared to address technical questions, program issues, DT&E, and the resolution of deficiencies. Users must be available to answer questions regarding operational requirements and mission impacts of fielding the system. 7.9.  Distributing and Safeguarding Test Information. 7.9.1.  Within the DoD.  Test organization commanders determine release authority for data, reports,  and  information  under  their  control.    DoDI  5230.24  provides  guidance  on  proper distribution  statements  for  Scientific  and  Technical  Information  (STINFO)  documents  and AFI  61-201  provides  guidance  and  procedures  on  creating,  protecting,  disseminating, archiving  or  destroying  Air  Force  STINFO  test  documentation.    AT  security  classification guidance  requires  all  AT  testing  and  reporting  be  conducted  within  US-only  channels.  Classified  test  information  cannot  be  released  except  as  specified  in  DoDI  5200.01,  DoD Information Security Program and Protection of Sensitive Compartmented Information, and associated documents. 7.9.2.  Outside  the  DoD.  Test  directors  do  not  have  release  authority  for  test  information and communications outside DoD channels.  Freedom of Information Act requests should be processed  IAW  DoDD  5400.7,  DoD  Freedom  of  Information  Act  (FOIA)  Program,  and  AFI99-103  6 APRIL 2017 81 AFMAN  33-302,  Freedom  of  Information  Act  Program.    Test  information  released  to Congress, the General Accountability Office, the DoD Inspector General, or similar agencies must  follow  guidance  in  AFI  90-401,  Air  Force  Relations  with  Congress,  and  AFI  65-401, Relations  with  the  Government  Accounting  Office  (GAO).    Info  copy  AF/TE  on  any  test information  released  to  outside  agencies.    SAF/IAPT,  the  Weapons,  Disclosure  and Technology Transfer Division, is the designated Air Force disclosure authority for release of classified  and  controlled  unclassified  weapons  systems,  technologies  and  information  to foreign  governments  and  international  organizations  in  support  of  Air  Force,  DoD  and commercial international programs. 7.10.  Information Collection and Records. 7.10.1.  No information collections are created by this publication. 7.10.2.  Program records created as a result of the processes prescribed in this publication are maintained  according  to  paragraph  5.18.9  and AFMAN  33-363,  and  disposed  of  IAW  the Air Force Records Disposition Schedule (RDS) located in the Air Force Records Information Management System (AFRIMS). DEVIN L. CATE Director, Test and Evaluation   82 AFI99-103  6 APRIL 2017 GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION Attachment 1 References 10 United States Code (U.S.C.), Armed Forces, § 139; § 139b, c, and d, § 2302(5), § 2366, § 2399, § 2400, and § 2350a(g) P.L. 103-160 § 220, National Defense Authorization Act for Fiscal Year 1994 32 Code of Federal Regulations (CFR) Part 989 JP 1-02, Department of Defense Dictionary of Military and Associated Terms, 8 November 2010 (As Amended Through 15 February 2016) CJCSI 3170.01I, Joint Capabilities Integration and Development System (JCIDS), 23 January 2015 CJCSI 5123.01G, Charter of the Joint Requirements Oversight Council (JROC), 12 February 2015 JCIDS Manual, Manual for the Operation of the Joint Capabilities Integration and Development System, 12 February 2015 DoD 5400.7-R_AFMAN 33-302, Freedom Of Information Act Program, 21 October 2010 Incorporating Through Change 3, 16 May 2016 DoD 7000.14-R, Department of Defense Financial Management Regulation (FMR), Vol 2A, September 2014 DoD Joint SAP Implementation Guide (JSIG), 11 April 2016 DoDD 3200.11, Major Range and Test Facility Base (MRTFB), 27 December 2007 DoDD 5000.01, The Defense Acquisition System, 20 November 2007 DoDD 5141.02, Director of Operational Test and Evaluation (DOT&E), 2 February 2009 DoDD 5200.47E, Anti-Tamper (AT), 4 September 2015 DoDD 8500.01, Cybersecurity, 14 March 2014  DoDI 3216.02, Protection of Human Subjects and Adherence to Ethical Standards in DoD-Supported Research, 8 November 2011 DoDI 5000.02, Change 2, Operation of the Defense Acquisition System, 2 February 2017 DoDI 5000.75, Business Systems Requirements and Acquisition, 2 February 2017 DoDI 5010.41, Joint Test and Evaluation (JT&E) Program, 12 September 2005 DoDI 5134.16, Deputy Assistant Secretary of Defense for System Engineering (DASD(SE)), 19 August 2011 DoDI 5134.17, Deputy Assistant Secretary of Defense for Developmental Test and Evaluation (DASD(DT&E)), 25 October 2011, Incorporating Change 1, 15 September 2015  AFI99-103  6 APRIL 2017 83 DoDI 5200.01, DoD Information Security Program and Protection of Sensitive Compartmented Information, 21 April 2016 DoDI 5200.39, Critical Program Information (CPI) Identification and Protection Within RDT&E, 28 May 2015 DODI 5200.44, Protection of Mission Critical Functions to Achieve Trusted Systems and Networks (TSN), 5 November 2012 incorporating Change 1, 25 August 2016 DoDI 5220.22, National Industrial Security Program, 18 March 2011 DoDI 5230.24, Distribution Statements on Technical Documents, 23 August 2012 DoDI 8100.04, DoD Unified Capabilities (UC), 9 December 2010 DoDI 8330.01, Interoperability of Information Technology (IT), Including National Security Systems (NSS), 21 May 2014 DoDI 8510.01, Risk Management Framework (RMF) for DoD Information Technology (IT), 12 March 2014, Incorporating Change 1, Effective May 24, 2016 DoDI 8580.1, Information Assurance (IA) in the Defense Acquisition System, 9 July 2004 DoDI O-3600.3, Technical Assurance Standard (TAS) for Computer Network Attack (CNA) Capabilities and National Security Systems (NSS), 22 April 2010 DoDI S-3100.15, Space Control, 19 January 2001 AFDD 1-02, Air Force Supplement to the Department of Defense Dictionary of Military and Associated Terms, 11 January 2007 AFMD 14, Air Force Operational Test and Evaluation Center (AFOTEC), 7 April 2006 HAFMD 1-52, Director of Test and Evaluation, 8 January 2015 AFPD 10-6, Capability Requirements Development, 6 November 2013 AFPD 10-9, Lead Command Designation and Responsibilities for Weapon Systems, 8 March 2007 AFPD 16-2, Disclosure of Military Information to Foreign Governments and International Organizations, 10 September 1993 AFPD 16-6, International Arms Control and Non-Proliferation Agreements, and the DoD Foreign Clearance Program, 29 December 2010, certified current 20 March 2012 AFPD 99-1, Test and Evaluation Process, 3 June 2014 AFI 10-601, Operational Capability Requirements Development, 6 November 2013 AFI 10-703, Electronic Warfare Integrated Reprogramming, 4 June 2014 AFI 10-1202, Space Test Program (STP) Management, 15 November 2010 AFI 10-2801, Air Force Concept of Operations Development, 23 October 2014 AFI 11-260, Tactics Development Program, 15 September 2011 AFI 13-212, Range Planning and Operations, 23 April 2015  84 AFI99-103  6 APRIL 2017 AFI 16-1001, Verification, Validation and Accreditation (VV&A), 22 June 2016 AFI 16-1005, Modeling and Simulation Management, TBD AFI 16-1404, Air Force Information Security Program, 29 May 2015 AFI 16-1406, Air Force Industrial Security Program, 25 August 2015 AFI 17-101, Risk Management Framework (RMF) for Air Force Information Technology (IT), 2 February 2017 AFI 17-130, Cybersecurity Program Management, 31 August 2015 AFI 17-140, Air Force Architecting, 17 May 2011 AFI 32-7061, The Environmental Impact Analysis Process, 12 March 2003 AFI 33-324, The Air Force Information Collections and Reports Management Program, 6 March 2013 AFI 33-360, Publications and Forms Management, 1 December 2015 AFI 36-1301, Management of Acquisition Key Leadership Positions (KLP), 19 October 2015 AFI 36-2251, Management of Air Force Training Systems, 5 June 2009 AFI 61-201, Management of Scientific and Technical Information (STINFO), 29 January 2016 AFI 61-204, Disseminating Scientific and Technical Information, 30 August 2002 AFI 63-101/20-101, Integrated Life Cycle Management, 7 March 2013, Incorporating Through Change 2, 23 February 2015 AFI 63-103, Joint Air Force-National Nuclear Security Administration (AF-NNSA) Nuclear Weapons Life Cycle Management, 24 September 2008 AFI 63-104, The SEEK EAGLE Program, 21 January 2005 AFI 63-125, Nuclear Certification Program, 8 August 2012 AFI 63-145, Manufacturing and Quality Management, 30 September 2016 AFI 65-401, Relations With the Government Accounting [sic] Office (GAO), 16 February 2016 AFI 65-601, Vol 1, Budget Guidance and Procedures, 16 August 2012 AFI 90-401, Air Force Relations With Congress, 14 January 2012 AFI 91-202, The US Air Force Mishap Prevention Program, 24 June 2015 AFI 91-204, Safety Investigations and Reports, 12 February 2014, Corrective Actions Applied on 10 April 2014 AFI 91-205, Nonnuclear Munitions Safety Board, 12 April 2012 AFI 91-401, Directed Energy Weapons Safety, 5 September 2013, Incorporating Change 1, 27 April 2016 AFI 99-106, Joint Test and Evaluation Program, 26 August 2009 AFI 99-108, Programming and Reporting Missile and Target Expenditures in Test and Evaluation, 1 March 2007  AFI99-103  6 APRIL 2017 85 AFI 99-109, Major Range and Test Facility Base (MRTFB) Test and Evaluation Resource Planning, 5 February 2015 AFI 99-114, Foreign Materiel Program (S), 22 October 2014 AFI 99-120, Forecasting and Programming Munitions Telemetry and Flight Termination Systems, 1 March 2007 AFMAN 17-1202, Collaboration Services and Voice Systems Management, 6 September 2012 AFMAN 33-363, Management of Records, 1 March 2008 AFMAN 63-119, Certification of System Readiness for Dedicated Operational Testing, 19 February 2016 AFMAN 91-201, Explosives Safety Standards, 12 January 2011 AFPAM 63-128, Integrated Life Cycle Management, 10 July 2014 AFSSI 7700, Emissions Security (C1), 14 April 2009 AFSSI 7702, Emissions Security Countermeasures Reviews (C1), 17 October 2008 AFGM 2015-33-03, Interoperability and Supportability of Information Technology and National Security Systems (IT/NSS), 25 July 2016 CNSSI 1253, Security Categorization and Control Selection for National Security Systems, 15 March 2012 CNSSI 4009, Glossary of Key Information Security Terms, 6 April 2015 MIL-HDBK-520A, Systems Requirements Document Guidance, 19 Dec 2011 NIST SP 800-30 Revision 1, Guide for Conducting Risk Assessments, September 2012 NIST SP 800-39, Managing Information Security Risk, March 2011 NIST SP 800-53 Revision 4, Security and Privacy Controls for Federal Information Systems and Organizations, April 2013 NIST SP 800-53A Revision 1, Guide for Assessing the Security Controls in Federal Information Systems and Organizations, December 2014 SD-2, DoD Acquisitions: Buying Commercial & Nondevelopmental Items, 1 January 2010 TO 00-35D-54, USAF Deficiency Reporting, Investigation, and Resolution TO 00-5-1, AF Technical Order System, 15 August 2009 Air Force Test and Evaluation Guidebook, April 2007 AF/A5R Requirements Development Guidebooks, 19 September 2016 OSD Comparative Testing Office Handbook, December 2007 Defense Acquisition Guidebook, 15 May 2013 Glossary of Defense Acquisition Acronyms & Terms, 14th Edition, July 2011 DoD Guide for Achieving Reliability, Availability, and Maintainability, 3 August 2005  86 AFI99-103  6 APRIL 2017 Incorporating Test and Evaluation into Department of Defense Acquisition Contracts, 24 October 2011 International Cooperation in Acquisition, Technology and Logistics (IC in AT&L) Handbook, 7th Edition, May 2012 Memorandum of Agreement on Multi-Service Operational Test and Evaluation (MOT&E) and Operational Suitability Terminology and Definition, April 2015 Product Support Manager Guidebook, April 2016 DoD Cybersecurity Test and Evaluation Guidebook, 1 July 2015 Department of Defense Risk, Issue, and Opportunity Management Guide for Defense Acquisition Programs, July 15 Systems Engineering Guide for Systems of Systems, August 2008 Test and Evaluation Management Guide, 6th edition, December 2012 Testing in a Joint Environment Roadmap, 12 November 2004 USAF Early Systems Engineering Guidebook, 31 March 2009 United States Air Force Report to Congressional Committees: Developmental Test and Evaluation of Space Systems, January 2013 DOT&E Memo, Procedures for Operational Test and Evaluation of Cybersecurity in Acquisition Programs, 1 August 2014 DOT&E memo, Guidelines for Operational Test and Evaluation of Information and Business Systems, 14 September 2010 DOT&E memo, Timelines of Operational Test and Evaluation (OT&E) Plans, 6 March 2008 DOT&E memo, Procedure for Assessment of Reliability Programs by DOT&E Action Officers, 29 May 2009 DOT&E memo, Timelines for Operational Test and Evaluation (OT&E) Plans, 24 June 2011 USD(AT&L) Memo, Transition of the Defense Space Acquisition Board (DSAB) into the Defense Acquisition Board (DAB), 23 March 2009 USD(AT&L) memo, Government Performance of Critical Acquisition Functions, 25 August 2010 USD(AT&L) memo, Key Leadership Positions and Qualification Criteria, 8 November 2013 United States Air Force Warfare Center (USAFWC) Charter, 31 January 2008 Prescribed Forms. AF Form 1067, Modification Proposal Adopted forms AF Form 847, Recommendation for Change of Publication.  AFI99-103  6 APRIL 2017 87 Abbreviations and Acronyms ACAT—Acquisition Category ACC—Air Combat Command ADM—Acquisition Decision Memorandum AF—Air Force AFLCMC—Air Force Life Cycle Management Center AF/TE—Directorate of Air Force Test and Evaluation AF-NNSA—Air Force-National Nuclear Security Administration AFAMS—Air Force Agency for Modeling and Simulation AFCAP—Air Force Certification and Accreditation Program AFDD—Air Force Doctrine Document AFI—Air Force Instruction AFICE—Air Force Integrated Collaborative Environment AFMAN—Air Force Manual AFMC—Air Force Materiel Command AFMD—Air Force Mission Directive AFOTEC—Air Force Operational Test and Evaluation Center AFPAM—Air Force Pamphlet AFPD—Air Force Policy Directive AFRIMS  RDS—Air  Force  Information  Management  System  Records  Disposition  Schedule, https://www.my.af.mil/afrims/afrims/afrims/rims.cfm AFSIT—Air Force System Interoperability Testing AFSPC—Air Force Space Command AFSSI—Air Force Systems Security Instruction AFTC—Air Force Test Center AML—Acquisition Master List Ao—Operational Availability AoA—Analysis of Alternatives AOTR—Assessment of Operational Test Readiness APA—Additional Performance Attribute APDP—Acquisition Professional Development Program ASD—Agile Software Development ATD—Advanced Technology Demonstration  88 AFI99-103  6 APRIL 2017 ATEC—Army Test and Evaluation Command ATO—Authorization to Operate CAPE—Cost Assessment and Program Evaluation CA—Criticality Analysis C&A—Certification and Accreditation C&L—Capabilities and Limitations CAE—Component Acquisition Executive CAT—Category CC—Critical Component CCA—Clinger-Cohen Act CCD—Combat Capability Document CCMD—Combatant Command CDD—Capability Development Document CDR—Critical Design Review CDRL—Contract Data Requirements List CDT—Chief Developmental Tester CFR—Code of Federal Regulations CIO—Chief Information Officer CJCSI—Chairman of the Joint Chiefs of Staff Instruction CNA—Computer Network Attack CNSSI—Committee on National Security Systems Instruction COA—Course of Action COCOM—Combatant Command (Authority) COI—Critical Operational Issue CONOPS—Concept of Operations COTS—Commercial-Off-The-Shelf CPD—Capability Production Document CPI—Critical Program Information CSAF—Chief of Staff of the Air Force CTF—Combined Test Force CTP—Critical Technical Parameter DAG—Defense Acquisition Guidebook  AFI99-103  6 APRIL 2017 89 DASD(DT&E)—Deputy Assistant Secretary of Defense for Developmental Test and Evaluation DASD(SE)—Deputy Assistant Secretary of Defense for Systems Engineering DAU—Defense Acquisition University DBS—Defense Business Systems DEF—Developmental Evaluation Framework DEWCB—Directed Energy Weapons Certification Board DoD—Department of Defense DoDD—Department of Defense Directive DoDI—Department of Defense Instruction DOT&E—Director, Operational Test and Evaluation DOTMLPF-P—Doctrine,  Organization,  Training,  Materiel,  Leadership  and  Education, Personnel, Facilities and Policy DR—Deficiency Report or Deficiency Reporting DRB—Deficiency Review Board DRU—Direct Reporting Unit DSM—Digital System Model DSOR—Depot Source of Repair DT&E—Developmental Test and Evaluation DTIC—Defense Technical Information Center EA—Evolutionary Acquisition e.g—for example etc—Et cetera (meaning “and so forth” or “and the like”) EITDR—Enterprise Information Technology Data Repository ELA—Elevated Level of Assurance EMD—Engineering and Manufacturing Development EMSEC—Emission Security EOA—Early Operational Assessment ETO—Executing Test Organization EW—Electronic Warfare EWIR—Electronic Warfare Integrated Reprogramming FAT—First Article Test FCT—Foreign Comparative Testing FDE—Force Development Evaluation  90 AFI99-103  6 APRIL 2017 FISMA—Federal Information Management Security Act F3I—Form, Fit, Function, and Interface FMECA—Failure Mode, Effects and Criticality Analysis FMP—Foreign Materiel Program FMR—Financial Management Regulation FOA—Field Operating Agency FOC—Full Operational Capability FOT&E—Follow-on Operational Test and Evaluation FRP—Full-Rate Production FRPDR—Full-Rate Production Decision Review FRP/FD—Full-Rate Production or Full Deployment GAO—Government Accountability Office GFE—Government Furnished Equipment HAF—Headquarters Air Force HAFMD—Headquarters Air Force Mission Directive HERO—Hazards of Electromagnetic Radiation to Ordnance HPT—High Performance Team HQ—Headquarters HwA—Hardware Assurance IA—Information Assurance IAW—In Accordance With ICD—Initial Capabilities Document ICEP—Information Certification Evaluation Plan i.e—that is ILCM—Integrated Life Cycle Management IOC—Initial Operational Capability IOT&E—Initial Operational Test and Evaluation IPT—Integrated Product Team IRB—Investment Review Board ISP—Information Support Plan IT—Information Technology ITAB—Information Technology Acquisition Board  AFI99-103  6 APRIL 2017 91 ITC—Integrated Test Concept ITP—Integrated Test Plan ITT—Integrated Test Team JCIDS—Joint Capabilities Integration and Development System JCB—Joint Capabilities Board JCS—Joint Chiefs of Staff JCTD—Joint Capability Technology Demonstration JDRS—Joint Deficiency Reporting System JEON—Joint Emergent Operational Need JITC—Joint Interoperability Test Command JMETC—Joint Mission Environment Test Capability JP—Joint Publication JRMET—Joint Reliability and Maintainability Evaluation Team JROC—Joint Requirements Oversight Council JSIG—Joint Special Access Program (SAP) Implementation Guide JUON—Joint Urgent Operational Need JT&E—Joint Test and Evaluation KIP—Key Interface Profile KLP—Key Leadership Position KPP—Key Performance Parameter KSA—Key System Attribute LAT—Lot Acceptance Test LCSP—Life Cycle Sustainment Plan LD—Limited Deployment LDTO—Lead Developmental Test and Evaluation Organization LFT&E—Live Fire Test and Evaluation LRIP—Low-Rate Initial Production M&S—Modeling and Simulation MAIS—Major Automated Information System MAJCOM—Major Command MCF—Mission Critical Functions MCOTEA—Marine Corps Operational Test and Evaluation Agency  92 AFI99-103  6 APRIL 2017 MDA—Milestone Decision Authority MDAP—Major Defense Acquisition Program MDD—Materiel Development Decision MIL-HDBK—Military Handbook MIPRB—Materiel Improvement Program Review Board MOA—Memorandum of Agreement MOE—Measure of Effectiveness MOP—Measure of Performance MOS—Measure of Suitability MOT&E—Multi-Service Operational Test and Evaluation MRTFB—Major Range and Test Facility Base MS—Milestone MSA—Materiel Solution Analysis MUA—Military Utility Assessment NDAA—National Defense Authorization Act NDI—Non-Developmental Item NIST—National Institute of Standards and Technology NNMSB—Nonnuclear Munitions Safety Board NR-KPP—Net-Ready Key Performance Parameter NSS—National Security System O&M—Operations and Maintenance OA—Operational Assessment OARL—Operating at Risk List OCR—Office of Collateral Responsibility OFP—Operational Flight Program OPR—Office of Primary Responsibility OPTEVFOR—Operational Test and Evaluation Force OSD—Office of the Secretary of Defense OT&E—Operational Test and Evaluation OTA—Operational Test Agency OTO—Operational Test Organization OTRR—Operational Test Readiness Review  AFI99-103  6 APRIL 2017 93 OUE—Operational Utility Evaluation PAT&E—Production Acceptance Test and Evaluation PDR—Preliminary Design Review PEM—Program Element Monitor PEO—Program Executive Officer PIA—Privacy Impact Assessment PIR—Post-Implementation Review PIT—Platform Information Technology P.L—Public Law PM—Program Manager PMD—Program Management Directive (term deleted) POA&M—Plan of Actions and Milestones POC—Point of Contact POM—Program Objective Memorandum PPP—Program Protection Plan PPQT—Pre-Production Qualification Test PQT—Production Qualification Test PTO—Participating Test Organization QOT&E—Qualification Operational Test and Evaluation QRC—Quick Reaction Capability QRF—Quick Reaction Fund QT&E—Qualification Test and Evaluation R&D—Research and Development RALOT—Risk Assessment Level of Test RAM—Reliability, Availability, and Maintainability RDS—Records Disposition Schedule RDT&E—Research, Development, Test, and Evaluation RFP—Request for Proposal RGC—Reliability Growth Curve RMF—Risk Management Framework RRF—Rapid Reaction Fund RSR—Requirements Strategy Review  94 §—section S&T—Science and Technology SecDef—Secretary of Defense SAE—Service Acquisition Executive AFI99-103  6 APRIL 2017 SAF/AQ—Assistant Secretary of the Air Force (Acquisition) SAF/IAPT—Deputy  Undersecretary  of  the  Air  Force,  International  Affairs,  Weapons, Disclosure and Technology Transfer Division SAP—Special Access Program SEP—Systems Engineering Plan SF—Standard Form SIMCERT—Simulator Certification SIMVAL—Simulator Validation SIP—System Information Profile SMC—Space and Missile Systems Center SOO—Statement of Objectives SORN—System of Record Notice SOTR—Sufficiency of Operational Test Review SOW—Statement of Work SP—Special Publication SRD—System Requirements Document SSE—System Security Engineering STAT—Scientific Test and Analysis Techniques STP—Space Test Program SUT—System Under Test SwA—Software Assurance T&E—Test and Evaluation TD&E—Tactics Development and Evaluation TDS—Technology Development Strategy TDSB—Test Data Scoring Board TEMP—Test and Evaluation Master Plan TIPT—Test Integrated Product Team TM—Test Manager TO—Technical Order  AFI99-103  6 APRIL 2017 95 TRP—Test Resource Plan TSN—Trusted Systems & Networks TSP—Transfer Support Plan TTP—Tactics, Techniques, and Procedures UC—Unified Capabilities UID—Unique Identification Number UON—Urgent Operational Need USAF—United States Air Force USAFWC—United States Air Force Warfare Center U.S.C—United States Code USD(AT&L)—Under Secretary of Defense (Acquisition, Technology, and Logistics) VV&A—Verification, Validation, and Accreditation VMP—Vulnerability Management Process WIPT—Working-level Integrated Product Team WIT—Watch Item WSEP—Weapons System Evaluation Program Terms Note:—A common understanding of terms is essential to effectively implement this instruction.  In  some  cases,  definitions  from  multiple  sources  are  offered  where  they  may  be  of  value.  “Notes” and italicized words in brackets at the end of definitions are not an official part of that definition, and are added for clarity for information only. Note: For additional terms and definitions not listed below, see Joint Publication (JP) 1-02, Department of Defense Dictionary of Military and Associated Terms, and Air Force Doctrine Document (AFDD) 1-2, Air Force Glossary, which contain standardized terms and definitions for DoD and Air Force use.  Also see Test and Evaluation Management Guide, 5th edition, Defense Acquisition University (DAU) Press.   Note: See the AF/A5R Requirements Development Guidebook, Volume 1 and AFI 63-101/20-101 for definitions of terms relating to the requirements and acquisition processes.  Acquisition Category (ACAT)—Acquisition categories determine the level of review, decision authority,  and  applicable  T&E  policies  and  procedures.    They  facilitate  decentralized  decision making  and  execution,  and  compliance  with  statutorily  imposed  requirements.    (See  DoDI 5000.02, Enclosure 1 for details.)  Agile Release—(1) The act of issuing a software version for publication, use, or distribution as a result of agile software development.  (2)  A new release of a software program. Agile  Software  Development  (ASD)—(1)  A  group  of  software  development  methodologies based  on  iterative  and  incremental  development  where  requirements  and  solutions  evolve through  highly  collaborative,  self-organizing,  cross-functional  teams.   (2)  An  iterative  96 AFI99-103  6 APRIL 2017 development  approach  that  focuses  on  mature  technologies,  continuous  testing,  test-driven development, continuous user involvement and requirements definition, and rapid early fielding of working functionality. Availability (Ao)—A measure of the degree to which an item is in the operable and committable state  at  the  start  of  a  mission  when  the  mission  is  called  for  at  an  unknown  (random)  time.  (DAG)  BIG  SAFARI—The  645th  Aeronautical  Systems  Group  (also  known  as  the  BIG  SAFARI Program)  executes  sensitive  United  States  Government  and  foreign  military  sales  programs  in support  of  high  priority,  rapid-requirement,  and  urgent  operational  needs  by  direction  of  the Assistant Secretary of the Air Force for Acquisition (SAF/AQ).  BIG SAFARI is responsible for total life cycle ownership over those assigned programs and projects, and functions as PM with systems  engineering,  LDTO and OTO responsibilities to  assure Operational  Safety, Suitability, and Effectiveness of the systems(s) in coordination with the ultimate end user. Block—Major capability release.  USAF Weapon Systems Software Management Guidebook, 15 August 2008. Build—a  testable,  integrated  subset  of  the  overall  capability  –  which  together  with  clearly defined  decision  criteria,  ensure  adequate  progress  is  being  made  before  fully  committing  to subsequent  builds.    Several  software  builds  are  typically  necessary  to  achieve  a  deployable capability  such  as  a  release.    Each  build  has  allocated  requirements,  resources,  and  scheduled testing  to  align  dependencies  with  subsequent  builds  and  to  produce  testable  functionality  to ensure  that  progress  is  being  achieved.  (DoDI  5000.02)    A  build  is  a  developmental  increment (version) of the system or software.  (SEI Technical Memorandum) Capabilities and Limitations (C&L) Report—An optional, quick-look report of limited scope that operational  testers provide to  operational  commands and operational  units to  support rapid and/or early fielding of developing capabilities before dedicated operational testing is complete and  formal  production  begins.    It  provides  the  most  current  operational  test  perspectives  on system capabilities and limitations based  on testing done to date, and describes any untested or unknown areas. Capability-Based Testing—A mission-focused strategy for T&E for verifying that a capabilities solution  will enable operations at  an acceptable level  of risk.  Capabilities-oriented evaluations are  the  primary  T&E  methodology  throughout  system  testing,  but  traditional  evaluations  of system  performance  measured  against  specification-like requirements  are  also  used.  Capabilities-based  testing  requires  understanding  operational  concepts  and  involves  developing strategies for T&E and plans to determine whether a capability solution option merits fielding. Center  Test  Functional  Leader—The  senior  individual  responsible  for  overseeing/managing T&E  functional  processes  and  policy  across  the  Center.    Also  responsible  for  managing  the functional  workforce,  to  include  planning,  advocating  for  Center  resources,  identifying workforce  competencies/gaps  and  providing  highly  skilled  T&E  personnel  to  their  supported organizations. (AFMCI 36-2645) Chief Developmental Tester (CDT)—A designated government T&E professional in an MDAP or  MAIS  program  office  reporting  to  the  PM  to  coordinate,  plan,  and  manage  all  DT&E activities,  to  include  contractor  testing,  and  who  makes  technically  informed,  objective  AFI99-103  6 APRIL 2017 97 judgments about DT&E results.  For non-MDAP and non-MAIS programs, this person is known as the Test Manager (TM).  (10 U.S.C. § 139b) Combined  Test  Force  (CTF)—An  integrated  team  of  military,  civilian,  and  contractor  T&E professionals empowered to plan and execute tests and report results in a collaborative, effective, and efficient manner over the entire life cycle of a system. Combined Testing—Combined Testing – The combined planning and execution of test phases and  events  to  provide  independent  data  in  support  of  independent  analysis  and  reporting  on independent  test  objectives  between  contractor  and  government  or  developmental  and operational test communities for efficient use of resources. Common  T&E  Database—A  repository  of  all  available  T&E  data  for  a  single  acquisition program or system under test that is accessible to all program stakeholders with a need to know. Covered  System—DoD  term  that  is  intended  to  include  all  categories  of  systems  or  programs requiring  Live  Fire  Test  and  Evaluation.  A  covered  system  means  a  system  that  the  Director, Operational Test and Evaluation, acting for the Secretary of Defense, has designated for LFT&E oversight. These include, but are not limited to, the following categories: a. Any major system within the meaning of that term in Title 10 U.S.C. § 2302(5) that is user-occupied and designed to provide some degree of protection to its occupants in combat; or  b. A conventional munitions program or missile program; or a conventional munitions program for which more than one million rounds are planned to be acquired (regardless of whether or not it is a major system); or c. A modification to a covered system that is likely to affect significantly the survivability or lethality of such a system.    Covered Product Improvement Program—See Covered System. Critical  Operational  Issue  (COI)—1—Operational  effectiveness  and  operational  suitability issues  (not  parameters,  objectives,  or  thresholds)  that  must  be  examined  during  operational testing  to  determine  the  system’s  capability  to  perform  its  mission.    (paraphrased  from  DAU’s Test  and  Evaluation  Management  Guide)    2.    A  key  question  to  be  answered  by  operational testers when evaluating a system’s overall operational effectiveness, suitability, and operational capabilities. Critical  Technical  Parameter  (CTP)—Measurable  critical  system  characteristics  that,  when achieved,  allow  the  attainment  of  operational  performance  requirements.    They  are  technical measures  derived  from  user  requirements.    Failure  to  achieve  a  critical  technical  parameter should  be  considered  a  reliable  indicator  that  the  system  is  behind  in  the  planned  development schedule or will likely not achieve an operational requirement.  (paraphrased from DAG) Criticality  Analysis  (CA)—An  end-to-end  functional  decomposition  performed  by  systems engineers to identify mission critical functions and components. Includes identification of system missions,  decomposition  into  the  functions  to  perform  those  missions,  and  traceability  to  the hardware,  software,  and  firmware  components  that  implement  those  functions.  Criticality  is assessed in terms of the impact of function or component failure on the ability of the component to complete the system mission(s). (TSN)  98 AFI99-103  6 APRIL 2017 Critical  Component  (CC)—A  component  which  is  or  contains  ICT,  including  hardware, software,  and  firmware,  whether  custom,  commercial,  or  otherwise  developed,  and  which delivers or protects mission critical functionality of a system or which, because of the system’s design,  may  introduce  vulnerability  to  the  mission  critical  functions  of  an  applicable  system. (TSN) Critical  Program  Information  (CPI)—Refers  to  the  United  States  capability  elements  that contribute  to  the  warfighters’  technical  advantage,  which  if  compromised,  undermine  United States military preeminence. Cyber Testing—The testing of systems and sub-systems that operate in the cyberspace domain, and  the  access  pathways  to  such  systems  that  are  part  of  DoD  weapon  systems.    Cyber  testing includes  cybersecurity  testing  (with  associated  Risk  Management  Framework  processes)  and cyber resiliency testing. Cybersecurity—Prevention of damage to, protection of, and restoration of computers, electronic communications  systems,  electronic  communications  services,  wire  communication,  and electronic  communication,  including  information  contained  therein,  to  ensure  its  availability, integrity, authentication, confidentiality, and nonrepudiation (DoDI 8500.01). Cybersecurity  Testing—The  testing  of  the  systems’  and  sub-systems’  ability  to  protect  or defend  against  a  cyber  attack.  Cybersecurity  testing  focuses  on  identifying  and  eliminating  or mitigating  system  cyber  vulnerabilities.    It  is  scoped  through  assessing  a  system’s  cyber boundary and risk to mission assurance.   Risk analysis, at a minimum, should consider the threat and  threat  severity,  the  likelihood  of  attack,  and  system  vulnerabilities.    Cybersecurity  is evaluated based on the Security Assessment Plan, Program Protection Plan, Information Support Plan, and Risk Management Frame-work artifacts. Cyber  Resiliency  Testing—The  testing  of  the  systems’  and  sub-systems’  ability  to  detect  and respond  to  a  cyber  attack  if  cybersecurity  defensive  protections  are  defeated.  Cyber  resiliency testing  evaluates  a  system’s  ability  to  meet  operational  requirements  while  under  cyber  attack. Cyber  resiliency  testing  focuses  on  detection  and  reaction  to  a  successful  cyber  attack  and  the continuity,  recovery  and  restoration  of  data  and  system  functionality.  Cyber  resiliency  testing also evaluates the operators’ ability to continue mission execution if system restoration/recovery is impossible or impractical. Cyber  Attack—An  attack,  via  cyberspace,  designed  to  infiltrate,  disrupt,  disable,  deceive, destroy, or maliciously control a target within cyberspace or a physical system. Cyberspace—  The  interdependent  network  of  information  technology  infrastructures,  and includes  the  Internet,  telecommunications  networks,  computer  systems,  and  embedded processors and controllers in critical industries. (CNSSI 4009) Dedicated  Operational  Testing—Operational is  conducted independently  from  contractors,  developers,  and  operational  commands  and  used  to  support production or fielding decisions. test  and  evaluation that Deficiency  Report  (DR)—The  generic  term  used  within  the  USAF  to  record,  submit,  and transmit deficiency data which may include, but is not limited to, a Deficiency Report involving quality, materiel, software, warranty, or informational deficiency data submitted using Standard Form (SF) 368 or equivalent format.  (TO 00-35D-54)  AFI99-103  6 APRIL 2017 99 Category  I  Deficiency—Those  deficiencies  which  may  cause  death,  severe  injury,  or  severe occupational illness; may cause loss or major damage to a weapon system; critically restricts the combat  readiness  capabilities  of  the  using  organization;  or  which  would  result  in  a  production line stoppage, and for which there is no viable work-around. Category  II  Deficiency—Those  deficiencies  that  impede  or  constrain  successful  mission accomplishment (system does not meet minimum operational requirements but does not meet the safety  or  mission  impact  criteria  of  a  Category  I  deficiency).    It  may  also  be  a  condition  that complements,  but  is  not  absolutely  required  for,  successful  mission  accomplishment.    The recommended enhancement, if incorporated, will improve a system’s operational effectiveness or suitability. Deployment—(1) The movement of forces within operational areas.  (2) The relocation of forces and  materiel  (to  include  software  deployment)  to  desired  operational  areas.    Deployment encompasses all activities from origin or home station through destination.  (JP 1-02) Developmental  Test  and  Evaluation  (DT&E)—Test  and  evaluation  conducted  to  evaluate design  approaches,  validate  analytical  models,  quantify  contract  technical  performance  and manufacturing  quality,  measure  progress  in  system  engineering  design  and  development, minimize  design  risks,  predict  integrated  system  operational  performance  (effectiveness  and suitability) in the intended environment, and identify system problems (or deficiencies) to allow for early and timely resolution.  DT&E includes contractor testing and is conducted over the life of the system to support acquisition and sustainment efforts.  (DAG) Early Operational Assessment (EOA)—An operational assessment (OA) conducted before MS B.  An EOA assesses the design approach sufficiently early in the acquisition process to assure it has the potential to fulfill user requirements.  (See Operational Assessment.) Elevated  Level  of  Assurance  (ELA)—A  measure  of  confidence  that  the  security  features, practices,  procedures,  and  architecture  of  an  information  system  accurately  mediates  and enforces  the  security  policy.    On  the  Common  Criteria  predefined  assurance  scale,  higher (elevated)  levels  indicate  the  most  rigorous,  formal  criteria  for  security  evaluation.    (CNSS National IA Glossary) Enabling  Concept—Describes  how  a  particular  task  or  procedure  is  performed,  within  the context of a broader functional area, using a particular capability, such as a specific technology, training  or  education  program,  organization,  facility,  etc.    An  enabling  concept  describes  the accomplishment  of  a  particular  task  that  makes  possible  the  performance  of  a  broader  military function or sub-function.  (See AFI 10-2801 for further information on Air Force concepts.) Enhancement—A condition that improves or complements successful mission accomplishment but  is  not  absolutely  required.    The  recommendation,  if  incorporated,  will  enhance  a  system’s operational  safety,  suitability  and/or  effectiveness.    An  enhancement  report  should  not  be designated as such solely due to an “out-of-scope” effort of the contractual requirements. Evaluation  Criteria—Standards  by  which  the  accomplishment  of  required  technical  and operational  effectiveness  and/or  suitability  characteristics,  or  resolution  of  operational  issues, may be addressed.  (DAG) Executing  Test  Organization  (ETO)—Test  organization,  usually  at  the  squadron  level, accomplishing DT under supervision of the LDTO.  100 AFI99-103  6 APRIL 2017 Failure  Mode,  Effects,  and  Criticality  Analysis  (FMECA)—A procedure for analyzing each potential  failure  mode  in  a  product  to  determine  the  results  or  effects  thereof  on  the  product.  When  the  analysis  is  extended  to  classify  each  potential  failure  mode  according  to  its  severity and  probability  of  occurrence,  it  is  called  a  Failure  Mode,  Effects,  and  Criticality  Analysis (FMECA).    FMECA  analysis  are  typically  delivered  by  the  contractor  and  sustained  by  Air Force R&M systems engineering. Fielding Decision—The decision to acquire and/or release a system to users in the field. First Article Test (FAT)—Production testing that is planned, conducted, and monitored by the materiel  developer.    FAT  includes  pre-production  and  initial  production  testing  conducted  to ensure  that  the  contractor  can  furnish  a  product  that  meets  the  established  technical  criteria.  (DAU’s Test and Evaluation Management Guide)  Follow-on  Operational  Test  and  Evaluation  (FOT&E)—FOT&E  is  the  continuation  of operational  test and evaluation (OT&E) after  IOT&E, QOT&E, or OUE  and is  conducted only by AFOTEC.   It  answers specific questions about  unresolved COIs and test issues;  verifies the resolution  of  deficiencies  or  shortfalls  determined  to  have  substantial  or  severe  impact(s)  on mission  operations;  or  completes  T&E  of  those  areas  not  finished  during  IOT&E,  QOT&E,  or OUE. Force  Development  Evaluation  (FDE)—A  type  of  OT&E  performed  by  MAJCOM  OTOs  in support  of  MAJCOM-managed  system  acquisition-related  decisions  prior  to  initial  fielding,  or for MAJCOM sustainment or upgrade activities. Foreign  Comparative  Test  (FCT)—A  DoD  Test  and  Evaluation  (T&E)  program  that  is prescribed in Title 10 U.S.C. § 2350a(g), and is centrally managed by the Comparative Testing Office, Office of the Assistant Secretary of Defense (Research and Engineering) (ASD(R&E)). It provides  funding  for  U.S.  T&E  of  selected  equipment  items  and  technologies  developed  by allied  countries  when  such  items  and  technologies  are  identified  as  having  good  potential  to satisfy valid DoD requirements.  (DAG) Full-Up,  System-Level  Testing—Testing  that  fully  satisfies  the  statutory  requirement  for “realistic  survivability  testing”  or  “realistic  lethality  testing”  as  defined  in  10  U.S.C.  §  2366.  (DAG) Hardware  Assurance  (HwA)—The  level  of  confidence  that  hardware  functions  only  as intended  and  is  free  of  vulnerabilities,  defects,  and  weaknesses,  either  intentionally  or unintentionally designed or inserted as part of the hardware throughout the life cycle. Implementing Command—Air Force Materiel Command and Air Force Space Command.  The command  providing  the  majority  of  resources  in  direct  support  of  the  PM  responsible  for development,  production,  and  sustainment  activities.    Such  resources  include  technical assistance,  infrastructure,  test  capabilities,  laboratory  support,  professional  education,  training and  development,  management  tools,  and  all  other  aspects  of  support,  including  support  for product development and DT&E. (AFI 63-101/20-101) Increment—(1) A formal acquisition effort approved by the milestone decision authority. Each increment  may  have  one  or  more  releases  constituting  a  change  to  the  fielded  hardware  and software baseline.  (Draft Guidelines for OT&E of Software Intensive Systems—DOT&E)  (2) A militarily  useful  and  supportable  operational  capability  that  can  be  effectively  developed,  AFI99-103  6 APRIL 2017 101 produced or acquired, deployed, and sustained.  Each increment of capability will have its own set of threshold and objective values set by the user.  (CJCSI 3170.01)  Information Support Plan (ISP)—The identification and documentation of information needs, infrastructure  support,  IT  and  NSS  interface  requirements  and  dependencies  focusing  on  net-centric, interoperability, supportability and sufficiency concerns.  (DoDI 4630.8) Initial Operational Test and Evaluation (IOT&E)—See Operational Test and Evaluation. Integrated Product Support Elements—A composite of all support considerations necessary to ensure the effective and economical support of a system for its life cycle.  It is an integral part of all other aspects of system acquisition and operation.  Note: The twelve product support elements are: sustaining/systems engineering; design interface; supply support; maintenance planning and management; support equipment/automatic test systems; facilities; packaging, handling, storage, and  transportation;  technical  data  management/technical  orders;  manpower  and  personnel; training; computer resources; and protection of critical program information and AT provisions. (AFPAM 63-128) Integrated Testing—The collaborative planning and collaborative execution of test phases and events to provide shared data in support of independent analysis, evaluation and reporting by all stakeholders,  particularly  the  developmental  (both  contractor  and  government)  and  operational test and evaluation communities.  (DAG) Integrated  Test  Concept  (ITC)—An  outline  of  an  integrated  test  approach,  validated objectives, and known requirements for all testing on a program, to include initial descriptions of test scenarios, test locations, exercises, T&E methodologies, operational impacts and issues, and projections for  future  capabilities.  As part of the TEMP,  the  ITC  supports the development  of test plans that are integrated using a systems engineering approach. Integrated  Test  Team  (ITT)—A  cross-functional  team  of  empowered  representatives  from multiple  disciplines  and  organizations  and  co-chaired  by  operational  testers  and  the  PM.    The ITT  is  responsible  for  developing  the  strategy  for  T&E,  TEMP,  assisting  the  acquisition community  with  T&E  matters,  and  guiding  the  development  of  test  plans  that  are  integrated.  Note: The ITT is the Air Force equivalent to the T&E Working Integrated Product Team (T&E WIPT) described in the DAG. Joint  Capability  Technology  Demonstration  (JCTD)—JCTDs  fill  the  gap  between  science and  technology  and  acquisition  for  the  combatant  commands  (CCMDs).  JCTDs  focus  on resolving the joint, combined, coalition, and interagency warfighting and operational problems of the  CCMDs  within  a  1-  to  3-year  timeline.  JCTDs  resolve  problems  primarily  by  conducting technology  and  operational  demonstrations  and  operational  utility  assessments  of  mature technology/solutions (Technology Readiness Level 5-7) and transitioning them to the acquisition community  for  post-JCTD  development,  production,  fielding,  and  operation  and  maintenance. (JP 1-02) Joint Test and Evaluation (JT&E)—An OSD-sponsored T&E program conducted among more than one military Service to provide T&E information on combat operations issues and concepts.  JT&E does not support system acquisition.  (DoDI 5010.41& AFI99-106) Lead  Command—The  command  designated  to  advocate  for  a  weapon  system  and  respond  to issues  addressing includes  capabilities-based  planning, its  status  and  use.  Advocacy  102 AFI99-103  6 APRIL 2017 and for designated budgeting programming, equipment, upgrades/modifications,  initial  spares  and  other  weapon  system-unique  logistics  issues,  and follow-on test and evaluation.  Inherent in lead command responsibility is also the responsibility for support systems and equipment directly associated with a particular weapon system.  (AFPD 10-9) system-wide unique Lead  Developmental  Test  and  Evaluation  Organization  (LDTO)—The LDTO is nominated by  the  ITT  and  approved  by  the  PEO.    The  LDTO  functions  as  the  lead  integrator  for  a program’s DT&E activities.  It is separate from the program office, but supports the PM and ITT in  a  provider-customer  relationship  with  regard  to  scope,  type  and  conduct  of  required  DT&E.  The  LDTO  assists  the  CDT  with  oversight  of  contractor  DT&E  results  and  managing  studies, analyses  and  program  documentation  from  the  requirements,  acquisition  and  cyber  test communities.  The  LDTO is  selected from the list  of qualified candidates published by AFMC and AFSPC. Lethality—The  capability  of  a  munition  or  directed  energy  weapon  to  cause  damage  that  will cause  the  loss  or  a  degradation  in  the  ability  of  a  target  system  to  complete  its  designated mission(s).  (DAG) Live Fire Test and Evaluation (LFT&E)—The firing of actual weapons (or surrogates if actual weapons  are  not  available)  at  components,  subsystems,  sub-assemblies,  and/or  full-up,  system-level  targets  or  systems  to  examine  personnel  casualties,  system  vulnerabilities,  or  system lethality; and the evaluation of the results of such testing.  (DAG)   Logistics System Test and Evaluation—The test methodology, criteria, and tools for evaluating and  analyzing  the  logistics  support  elements  (DAG)  /  Integrated  Product  Support  Elements (AFPAM  63-128)  as  they  apply  to  a  system  under  test.    The  objective  is  to  influence  system design  as  early  as  possible  in  the  acquisition  cycle,  and  verify  that  the  logistics  support  being developed  is  capable  of  meeting  peacetime  and  wartime  employment  objectives.    (paraphrased from DAU’s Test and Evaluation Management Guide, 5th edition, January 05) Lot Acceptance Test (LAT)—A test based on a sampling procedure to ensure that the product retains its quality.  No acceptance or installation should be permitted until this test for the lot has been successfully completed.  (Glossary, Defense Acquisition Acronyms and Terms, and DAU’s Test and Evaluation Management Guide) Low-Rate  Initial  Production  (LRIP)—Production  of  the  system  in  the  minimum  quantity necessary  (1)  to  provide  production-configured  or  representative  articles  for  operational  tests pursuant to § 2399; (2) to establish an initial production base for the system; and (3) to permit an orderly  increase  in  the  production  rate  for  the  system  sufficient  to  lead  to  full-rate  production upon  the  successful  completion  of  operational  testing.    Note:  The  LRIP  quantity  should  not exceed  10  percent  of  the  total  number  of  articles  to  be  produced  as  determined  at  the  MS  B decision.  (10 U.S.C. § 2400)  Maintainability—The capability of an item to be retained in or restored to a specified condition when  maintenance  is  performed  by  personnel  having  specified  skill  levels,  using  prescribed procedures and routines, at each prescribed level of maintenance and repair.  (DAG) MAJCOM-Directed  Acquisition  Program—An  acquisition  program  originated  by  and directed at the MAJCOM level.  AFI99-103  6 APRIL 2017 103 Major Munitions Program—See Covered System. Measurable—Having  qualitative  or  quantitative  attributes  (e.g.,  dimensions,  velocity, capabilities) that can be ascertained and compared to known standards.  (See Testable.) Measure  of  Effectiveness  (MOE)—(1)  The  data  used  to  measure  the  military  effect  (mission accomplishment)  that  comes  from  the  use  of  the  system  in  its  expected  environment.  That environment  includes  the  system  under  test  and  all  interrelated  systems,  that  is,  the  planned  or expected  environment  in  terms  of  weapons,  sensors,  command  and  control,  and  platforms,  as appropriate, needed to  accomplish  an end-to-end mission in  combat.  (DAU Glossary)   (2)  A criterion used to assess changes in system behavior, capability, or operational environment that is tied to measuring the attainment of an end state, achievement of an objective, or creation of an effect.  (JP 1-02)   Measure  of  Performance  (MOP)—(1)  System-particular  performance  parameters  such  as speed,  payload,  range,  time-on-station,  frequency,  or  other  distinctly  quantifiable  performance features.  Several  MOPs  may  be  related  to  the  achievement  of  a  particular  measure  of effectiveness.    (DAU  Glossary)    (2)  A  criterion  used  to  assess  friendly  actions  that  is  tied  to measuring task accomplishment.  (JP 1-02)  Measure  of  Suitability  (MOS)—Measure  of  an  item’s  ability  to  be  supported  in  its  intended operational  environment.  MOS's  typically  relate  to  readiness  or  operational  availability  and, hence, reliability, maintainability, and the item’s support structure.  (DAU Glossary) Military  Utility—The  military  worth  of  a  system  performing  its  mission  in  a  competitive environment  including  versatility  (or  potential)  of  the  system.    It  is  measured  against  the operational  concept,  operational  effectiveness,  safety,  security,  and  cost/worth.    Military  utility estimates  form  a  rational  basis  for  making  management  decisions.    (Glossary,  Defense Acquisition Acronyms and Terms)  Military  Utility  Assessment  (MUA)—A  determination  of  how  well  a  capability  or  system  in question  responds  to  a  stated  military  need,  to  include  a  determination  of  its  potential effectiveness and suitability in performing the mission.  It is a "characterization" of the capability or  system  as  determined  by  measures  of  effectiveness,  measures  of  suitability,  measures  of performance, and other operational considerations as indicators of military utility, as appropriate, and  answers  the  questions,  "What  can  it  do?"  and  "Can  it  be  operated  and  maintained  by  the user?" Modification—For the purposes of this instruction, a modification is defined as an alteration to the form, fit, function, or interface (F3I) of an in-service AF hardware or software Configuration Item.  (AFI 63-101/20-101) Multi-Service—Involving two or more military Services or DoD components. Multi-Service  Operational  Test  and  Evaluation  (MOT&E)—OT&E  conducted  by  two  or more  Service  OTAs  for  systems  acquired  by  more  than  one  Service.    MOT&E  is  conducted according to the T&E directives of the lead OTO, or as agreed in a memorandum of agreement between  the  participants.    Note:  MAJCOM  OTOs  may  at  times  be  responsible  for  conducting MOT&E in lieu of AFOTEC.  104 AFI99-103  6 APRIL 2017 Objective—An operationally significant increment above the threshold.  An objective value may be the same as the threshold when an operationally significant increment above the threshold is not significant or useful.  (JCIDS Manual) Operating Concept—A description in broad terms of the application of military art and science within  a  defined  set  of  parameters.    In  simplest  terms,  operating  concepts  articulate  how  a commander  will  plan,  prepare,  deploy,  employ  or  sustain  a  joint  force  against  potential adversaries within a specified set of conditions.  Operating concepts encompass the full scope of military  actions  required  to  achieve  a  specific  set  of  objectives.    (See  AFI  10-2801  for  further information on Air Force concepts.) Operational  Assessment  (OA)—An analysis of  progress  toward operational  capabilities made by  an  OTO,  with  user  support  as  required,  on  other  than  production  systems.    The  focus  of  an operational  assessment  is  on  significant  trends  noted  in  development  efforts,  programmatic voids, areas of risk, adequacy of requirements, and the ability of the program to support adequate operational  testing.    Operational  assessments  may  be  made  at  any  time  using  technology demonstrators,  prototypes,  mockups,  engineering  development  models,  or  simulations,  but  will not substitute for the dedicated OT&E necessary to support full production decisions. Operational  Command—Air  Combat  Command,  Air  Mobility  Command,  AF  Special Operations  Command,  Air  Education  and  Training  Command,  Air  Force  Global  Strike Command, and Air Force Space Command.  Those commands that will ultimately operate, or are operating, a system, subsystem, or item of equipment.  (AFI 63-101/20-101) Operational Effectiveness—Measure of the overall ability of a system to accomplish a mission when  used  by  representative  personnel  in  the  environment  planned  or  expected  for  operational employment  of tactics,  supportability, survivability, vulnerability, and threat.  (DAG) the  system  considering  organization,  doctrine, Operational  Environment—A  composite  of  the  operational  conditions,  circumstances,  and influences  that  affect  the  employment  of  capabilities  and  bear  on  the  decisions  of  the commander.  (JP 1-02, JP 3-0) Operational  Suitability—The  degree  to  which  a  system  can  be  placed  and  sustained satisfactorily  in  field  use  with  consideration  being  given  to  availability,  compatibility, transportability, interoperability, reliability, wartime usage  rates, maintainability, safety, human factors,  habitability,  manpower,  logistics  supportability,  natural  environmental  effects  and impacts, documentation, and training requirements.  (DAG) Operational  Test  Agency  (OTA)—An  independent  agency  reporting  directly  to  the  Service Chief  that  plans  and  conducts  operational  tests,  reports  results,  and  provides  evaluations  of overall  operational  capability  of  systems  as  determined  by  effectiveness,  suitability,  and  other operational  considerations.    Note:  DoDD  5000.01  states,  “Each  Military  Department  shall establish an independent OTA . . .”  Therefore, each Service has one designated OTA which are as follows.  The Air Force has the Air Force Operational Test and Evaluation Center (AFOTEC).  The  Navy  has  the  Operational  Test  and  Evaluation  Force  (OPTEVFOR).    The  Army  has  the Army  Test  and  Evaluation  Command  (ATEC).    The  Marine  Corps  has  the  Marine  Corps Operational Test and Evaluation Agency (MCOTEA). Operational  Test  Organization  (OTO)—A  generic  term  for  any  organization  that  conducts operational testing as stated in its mission directive.  AFI99-103  6 APRIL 2017 105 Operational  Test  and  Evaluation  (OT&E—)—(1)  The  field  test,  under  realistic  combat conditions,  of  any  item  of  (or  key  component  of)  weapons,  equipment,  or  munitions  for  the purpose of determining the effectiveness and suitability of the weapons, equipment, or munitions for  use  in  combat  by  typical  military  users;  and  the  evaluation  of  the  results  of  such  test.    (10 U.S.C.  §  139(a)(2))      (2)  Testing  and  evaluation  conducted  in  as  realistic  an  operational environment  as  possible  to  estimate  the  prospective  system's  operational  effectiveness, suitability,  and  operational  capabilities.   In  addition,  OT&E  provides  information  on organization, personnel requirements, doctrine, and tactics.  It may also provide data to support or  verify  material  in  operating  instructions,  publications,  and  handbooks.    Note:  The  generic term OT&E is often substituted for IOT&E, QOT&E, FOT&E, OUE, FDE, WSEP, and TD&E and depending on the context, can have the same meaning as those terms. Operational  Testing—A  generic  term  encompassing  the  entire  spectrum  of  operationally oriented test activities, including assessments, tests, and evaluations.  Not a preferred term due to its lack of specificity. Operational  Utility  Evaluation  (OUE)—Evaluations  of  military  capabilities  conducted  to demonstrate or validate new operational concepts or capabilities, upgrade components, or expand the  mission  or  capabilities  of  existing  or  modified  systems.  AFOTEC  or  MAJCOMs  may conduct  OUEs  whenever  a  dedicated  operational  test  and  evaluation  event  is  required,  but  the full  scope  and  rigor  of  a  formal  IOT&E,  QOT&E,  FOT&E,  or  FDE  is  not  appropriate  or required.  OUEs may be used to support operational decisions (e.g., fielding a system with less than full capability) or acquisition-related decisions (e.g., low-rate production) when appropriate throughout  the  system.    OUEs  will  not  be  used  when  IOT&E,  QOT&E,  FOT&E  or  FDE  are more appropriate per existing guidance and definitions. Operator—See  “User.”    Refers  to  the  operating  command  which  is  the  primary  command operating  a  system,  subsystem,  or  item  of  equipment.    Generally  applies  to  those  operational commands or organizations designated by Headquarters, US Air Force to conduct or participate in  operations or operational  testing, interchangeable with  the term  "using command" or “user.”  In other forums the term “warfighter” or “customer” is often used.  “User” is the preferred term in this AFI. Oversight—Senior  executive-level  monitoring  and  review  of  programs  to  ensure  compliance with policy and attainment of broad program goals. Oversight  Program—A program on the OSD T&E Oversight  List for DT&E, LFT&E, and/or OT&E.  The list includes all major defense acquisition programs (MDAP) (e.g., ACAT I), Major Automated Information Systems (MAIS) (e.g., ACAT IA), and any other programs selected for OSD  T&E  Oversight  IAW  10  U.S.C.  §  2430(a)(1).    These  programs  require  additional documentation and have additional review, reporting, and approval requirements. Participating Test Organization (PTO)—Any test organization required to act in a supporting role  to  the  ETO  or  LDTO  by  providing  specific  T&E  data  or  resources  for  a  T&E  program  or activity. Penetration  Testing—(1)  A  live  test  of  the  effectiveness  of  security  defenses  through mimicking  the  actions  of  real-life  attackers.    (Information  Systems  Audit  and  Control Association (ISACA) dictionary)  (2) A method of evaluating the security of a computer system  106 AFI99-103  6 APRIL 2017 or network by simulating an attack from malicious outsiders (who have no access) and malicious insiders who have some level of authorized access. Platform  Information  Technology  (PIT)—A  special  purpose  IT  system  which  employs computing  resources  (i.e.,  hardware,  firmware,  and  optionally  software)  that  are  physically embedded in,  dedicated  to,  or essential in  real  time to  mission performance [of a host system].  PIT only performs (i.e., is dedicated to) the information processing assigned to it by its hosting special purpose system (this is not for core services).  (AFI 17-101) Pre-Production  Qualification  Test  (PPQT)—The  formal  contractual  tests  that  ensure  design integrity  over  the  specified  operational  and  environmental  range.    These  tests  usually  use prototype or pre-production hardware fabricated to the proposed production design specifications and drawings.  Such tests include contractual reliability and maintainability demonstration tests required prior to production release.   (Glossary, Defense Acquisition Acronyms and Terms, and DAU’s Test and Evaluation Management Guide)  Product  Support—A  continuous  and  collaborative  set  of  activities  that  establishes  and maintains  readiness  and  the  operational  capability  of  a  system,  subsystem,  or  end-item throughout  its  life  cycle  to  meet  its  availability  and  wartime  usage  requirements.    Planned product support includes the following:  test, measurement, and diagnostic equipment; spare and repair  parts;  technical  data;  support  facilities;  transportation  requirements;  training;  manpower; and software. (AFI63-101/20-101) Production  Acceptance  Test  and  Evaluation  (PAT&E)—Test  and  evaluation  of  production items to demonstrate that items procured fulfill requirements and specifications of the procuring contract or agreements.  (DAU’s Test and Evaluation Management Guide) Production  Qualification  Test  (PQT)—A  technical  test  conducted  prior  to  the  full  rate production decision to ensure the effectiveness of the manufacturing processes, equipment, and procedures.    These  tests  are  conducted  on  a  number  of  samples  taken  at  random  from  the  first production lot, and are repeated if the manufacturing process or design is changed significantly, or  when  a  second  source  is  brought  on  line.    (Glossary,  Defense  Acquisition  Acronyms  and Terms, and DAU’s Test and Evaluation Management Guide) Program Manager (PM)—(1) The designated individual with responsibility for and authority to accomplish program objectives for development, production, and sustainment to meet the user’s operational  needs.    The  PM  shall  be  accountable  for  credible  cost,  schedule,  and  performance reporting to the MDA.  (DoDD 5000.01)  (2) Applies collectively to system program directors, product  group  managers,  single  managers,  acquisition  PMs,  and  weapon  system  managers.  Operating  as  the  single  manager,  the  PM  has  total  life  cycle  system  management  authority.  Note: This AFI uses the term “PM” for any designated person in charge of acquisition activities, to  include  those  prior  to  MS  A  (i.e.,  before  a  technology  project  is  officially  designated  an acquisition program). Prototype—A  model  suitable  for  evaluation  of  design,  performance,  and  production  potential.  (JP  1-02)Note:  The  Air  Force  uses  prototypes  during  development  of  a  technology  project  or acquisition program for verification or demonstration of technical feasibility.  Prototypes are not usually representative of the final production item. Qualification  Operational  Test  and  Evaluation  (QOT&E)—A  tailored  type  of  IOT&E performed  on  systems  for  which  there  is  little  to  no  RDT&E-funded  development  effort.   AFI99-103  6 APRIL 2017 107 Commercial-off-the-shelf  (COTS),  non-developmental  items  (NDI),  and  government  furnished equipment (GFE) are tested in this manner. Qualification Test and Evaluation (QT&E)—A tailored type of DT&E for which there is little to  no  RDT&E-funded  development  effort. (COTS),  non-developmental  items  (NDI),  and  government  furnished  equipment  (GFE)  are  tested  in  this manner.   Commercial-off-the-shelf Recoverability—Following combat damage, the ability to take emergency action to prevent loss of  the  system,  to  reduce  personnel  casualties,  or  to  regain  weapon  system  combat  mission capabilities.  (DAG) Release—a  distinct,  tested,  deployable  software  element  of  a  militarily  useful  capability  to  the government.  A release is an increment (version) of the system/software that is transferred from one organization to another. (SEI Technical Memorandum 9 June 2015) Relevant Environment—The specific subset of the operational environment that is required to demonstrate  critical  "at  risk"  aspects  of  the  final  product  performance  in  an  operational environment.    It  is  an  environment  that  focuses  specifically  on  stressing  the  technology  in question.  Not all systems, subsystems, and/or components need to be operated in the operational environment  in  order  to  satisfactorily  address  performance  margin  requirements.    Note:  A relevant environment is required for Technology Readiness Levels 5 and 6. Reliability—The  capability  of  a  system  and  its  parts  to  perform  its  mission  without  failure, degradation, or demand on the support system.  (DAG) Research, Development, Test, and Evaluation (RDT&E)—The type of funding appropriation (3600)  intended  for  research,  development,  test,  and  evaluation  efforts.    (DoD  7000.14-R,  Vol 2A, and AFI 65-601, Vol I)  Note: The term “research and development” (R&D) broadly covers the work performed by a government agency or the private sector.  “Research” is the systematic study  directed  toward  gaining  scientific  knowledge  or  understanding  of  a  subject  area.  “Development” is the systematic use of the knowledge and understanding gained from research for  the  production  of  useful  materials,  devices,  systems,  or  methods.    RDT&E  includes  all supporting test and evaluation activities. Risk—(1)    A  measure  of  the  inability  to  achieve  program  objectives  within  defined  cost  and schedule constraints. Risk is associated with all aspects of the program, e.g., threat, technology, design  processes,  or  Work  Breakdown  Structure  elements.    It  has  two  components:  the probability of failing to achieve a particular outcome, and the consequences of failing to achieve that outcome.  (Glossary, Defense Acquisition Acronyms and Terms)  (2) Probability and severity of loss linked to hazards.  (JP 1-02) Severity  Category—The  category  a  certifying  authority  assigns  to  an  IT  system  security weakness  or  shortcoming  as  part  of  a  certification  analysis  to  indicate  the  risk  level  associated with the security weakness and the urgency with which the corrective action must be completed. Severity  categories  are  expressed  as  “Category  (CAT)  I,  CAT  II,  or  CAT  III,”  with  CAT  I indicating  the  greatest  risk  and  urgency.  Severity  categories  are  assigned  after  consideration  of all  possible  mitigation  measures  that  have  been  taken  within  system  design/architecture limitations for the DoD information system in question.  (DoDI 8510.01)  108 AFI99-103  6 APRIL 2017 CAT  I  Severity  Category—Assigned to findings that allow primary security protections to be bypassed, allowing immediate access by unauthorized personnel or unauthorized assumption of super-user  privileges.  An  Authorization  to  Operate  (ATO)  will  not  be  granted  while  CAT  I weaknesses are present. CAT II  Severity Category—Assigned to findings that have a potential to lead to unauthorized system access or activity. CAT II findings that have been satisfactorily mitigated will not prevent an ATO from being granted. CAT  III  Severity  Category—Assigned  findings  that  may  impact  IA  posture  but  are  not required to be mitigated or corrected in order for an ATO to be granted. Simulator Certification (SIMCERT)—The process of ensuring through validation of hardware and software baselines that a training system and its components provide accurate and credible training.    The  process  also  makes  sure  the  device  continues  to  perform  to  the  delivered specifications,  performance  criteria,  and  configuration  levels.    It  will  also  set  up  an  audit  trail regarding specification and baseline data for compliance and subsequent contract solicitation or device modification.  (AFI 36-2251) Simulator Validation (SIMVAL)—The process for (1) comparing a training device’s operating parameters  and  performance  to  the  current  intelligence  assessment  of  a  weapon  system,  threat, and interaction between the weapon system and threat, and (2) documenting the differences and impacts. This process includes generation and deployment of an intelligence data baseline of the system,  comparison  of  simulator  characteristics  and  performance,  support  for  the  modification and  upgrade  of  the  simulator,  a  comparison  of  simulator  and  threat  operating  procedures,  and correction of any significant deficiencies.  Uncorrected deficiencies are identified and published in validation reports.  The process continues throughout the life cycle of the simulator.  (AFI 36-2251) Software Assurance (SwA)—The level of confidence that software functions as intended and is free of vulnerabilities, either intentionally or unintentionally designed or  inserted as part of the software throughout the lifecycle. Specification—A  document  intended  primarily  for  use  in  procurement  which  clearly  and accurately  describes  the  essential  technical  requirements  for  items,  materials,  or  services, including  the  procedures  by  which  it  will  be  determined  that  the  requirements  have  been  met.  Specifications may be prepared to cover a group of products, services, or materials, or a single product,  service,  or  material,  and  are  general  or  detail  specifications.    (Glossary,  Defense Acquisition Acronyms and Terms) Spiral—One subset or iteration of a development program within an increment.  Multiple spirals may overlap or occur sequentially within an increment.  Note: An obsolete term, but may be in older  documents.    Generally,  spirals  are  not  fielded  IAW  DoDI  5000.02,  CJCSI  3170.01,  and AFI 63-101/20-101. Strategy  for  T&E—A  high-level  conceptual  outline  of  all  T&E  required  to  support development and sustainment of an acquisition program. Sufficiency  of Operational  Test  Review  (SOTR)—An examination by MAJCOM operational testers  of  all  available  test  data  to:  1)  determine  if  adequate  testing  has  been  accomplished  for  AFI99-103  6 APRIL 2017 109 programs  of  limited  scope  and  complexity;  and  2)  to  assess  the  risk  of  fielding  or  production without a dedicated OT&E.  An examination of existing test data, not an operational test per se. Survivability—The capability of  a system and crew to  avoid  or withstand a man-made hostile environment without suffering an abortive impairment of its ability to accomplish its designated mission.  Survivability consists of susceptibility, vulnerability, and recoverability.  (DAG) Susceptibility—The degree to which a weapon system is open to effective attack due to one or more inherent weaknesses.  (Susceptibility is a function of operational tactics, countermeasures, probability of enemy fielding a threat, etc.)  Susceptibility is considered a subset of survivability.  (DAG) Sustainment—(1)  The  provision  of  personnel,  logistic,  and  other  support  required  to  maintain and prolong operations or combat until successful accomplishment or revision of the mission or of the national objective.  (JP 1-02)  (2) The Service's ability to maintain operations once forces are engaged.  (AFDD 1-2)  (3) Activities that sustain systems during the operations and support phases of the system life cycle.  Such activities include any investigative test and evaluation that extends  the  useful  military  life  of  systems,  expands  the  current  performance  envelope  or capabilities  of  fielded  systems  or  modifies/acquires  support  equipment  for  the  system.  Sustainment  activities  also  include  T&E  for  modifications  and  upgrade  programs,  and  may disclose  system  or  product  deficiencies  and  enhancements  that  make  further  acquisitions necessary. System Security Engineer—Responsible for implementing SSE processes and best practices to ensure cybersecurity is addressed throughout the acquisition life cycle. Tactics  Development and Evaluation (TD&E)—TD&E is a tailored type of FDE specifically designed  to  further  exploit  doctrine,  system  capabilities,  tactics,  techniques,  and  procedures during the sustainment portion of the system life cycle.  TD&Es normally identify non-materiel solutions to tactical problems or evaluate better ways to use new or existing systems. Testable—The attribute of being measurable and repeatable with available test instrumentation and  resources.    Note:  Testability  is  a  broader  concept  indicating  whether  T&E  infrastructure capabilities  are  available  and  capable  of  measuring  the  parameter.    The  difference  between testable and measurable may indicate a test limitation.  Some requirements may be  measurable but not testable due to T&E infrastructure shortfalls, insufficient funding, safety, or statutory or regulatory prohibitions. Test  and  Evaluation  (T&E)—The  act  of  generating  empirical  data  during  the  research, development or sustainment of systems, and the creation of information through analysis that is useful to technical personnel and decision makers for reducing design and acquisition risks.  The process by which systems are measured against requirements and specifications, and the results analyzed so as to gauge progress and provide feedback. Test and Evaluation Master Plan (TEMP)—Documents the overall structure and objectives of the T&E program.  It provides a framework within which to generate detailed T&E plans and it documents  schedule  and  resource  implications  associated  with  the  T&E  program.    The  TEMP identifies  the  necessary  developmental,  operational,  and  live-fire  test  activities.    It  relates program  schedule,  test  management  strategy  and  structure,  and  required  resources  to:  COIs; CTPs;  objectives  and  thresholds  documented  in  the  requirements  document;  and  milestone decision points.  (DAU’s Test and Evaluation Management Guide)  110 AFI99-103  6 APRIL 2017 Test and Evaluation Organization—Any organization whose designated mission includes test and evaluation. Test  Deferral—The  movement  or  delay  of  testing  and/or  evaluation  of  a  specific  critical technical  parameter,  operational  requirement,  or  critical  operational  issue  to  a  follow-on increment or later test period.  A test deferral does not change the requirement to test a system capability or function. Test  Director––A  person  responsible  for  coordinating,  leading,  and  executing  a  test  and reporting  the  results  according  to  a  specific  test  plan.—Test  Integrated  Product  Team (TIPT)—Any  temporary  group  consisting  of  testers  and  other  experts  who  are  focused  on  a specific  test  issue  or  problem.   There  may  be  multiple  TIPTs  for  each  acquisition program/project. Test  Limitation—Any  condition  that  hampers  but  does  not  preclude  adequate  test  and/or evaluation of a critical technical parameter, operational requirement, or critical operational issue during a T&E program. Test Manager (TM)—–A designated government T&E professional in a non-MDAP/non-MAIS program  office  selected  to  coordinate,  plan,  and  manage  all  DT&E  activities,  to  include contractor  testing,  and  who  makes  technically  informed,  objective  judgments  about  DT&E results.    For  MDAP  or  MAIS  programs,  this  responsibility  is  fulfilled  by  the  Chief Developmental Tester (CDT). Test  Resources—A  collective  term  that  encompasses  all  elements  necessary  to  plan,  conduct, and collect/analyze data from a test event or program.  Elements include test funding and support manpower  (including  temporary  duty  costs),  test  assets  (or  units  under  test,  test  asset  support equipment,  technical  data,  simulation  models,  test  data  analysis  software,  test  beds,  threat simulators, surrogates and replicas, special  instrumentation  peculiar to  a  given test asset or test event,  targets,  tracking  and  data  acquisition,  instrumentation,  equipment  for  data  reduction, communications,  meteorology,  utilities,  photography,  calibration, recovery, maintenance  and  repair,  frequency  management  and  control,  and  base/facility  support  services.  (DAU’s T&E Management Guide)  security, Test Resource Plan (TRP)—The single program document AFOTEC uses to request personnel and  other  resource  support  for  operational  test  and  evaluation  from  MAJCOMs  and  other agencies. Test Team—A group of testers and other experts who carry out integrated testing according to a specific  test  plan.    Note:  A  combined  test  force  (CTF)  is  one  way  to  organize  a  test  team  for integrated testing. Threshold—A  minimum  acceptable  operational  value  below  which  the  utility  of  the  system becomes questionable. Trusted  Systems  &  Networks  (TSN)—A  comprehensive  systematic  approach  that  analyzes threats, vulnerabilities, and mitigation strategies to preserve mission assurance. User—Refers  to  the  operating  command  which  is  the  primary  command  operating  a  system, subsystem,  or  item  of  equipment.    Generally  applies  to  those  operational  commands  or organizations designated by Headquarters, US Air Force to conduct or participate in operations or operational  testing, interchangeable with  the term  "using command" or “operator.”    In other  AFI99-103  6 APRIL 2017 111 forums the term “warfighter” or “customer” is often used.  Also refers to maintainers.  “User” is the preferred term in this AFI. Validation (Testing)—The process of evaluating a system or software component during, or at the  end  of,  the  development  process  to  determine  whether  it  satisfies  specified  requirements. (DAG) Verification  (Testing)—Confirms that  a  system  element  meets  design-to  or  build-to specifications. Throughout  the system’s life  cycle, design solutions  at  all levels  of the physical architecture  are  verified  through  a  cost-effective  combination  of  analysis,  examination, demonstration, and testing, all of which can be aided by modeling and simulation. (DAG) Verification,  Validation  and  Accreditation  (VV&A)—VV&A  is  a  continuous  process  in  the life cycle of a model or simulation as it gets upgraded or is used for different applications.  (AFI 16-1001) Verification—Process  of  determining  that  M&S  accurately  represents  the  developer’s conceptual description and specifications. Validation—Rigorous  and  structured  process  of  determining  the  extent  to  which  M&S accurately represents the intended “real world” phenomena from the perspective of the intended M&S use. Accreditation—The official determination that a model or simulation is acceptable for use for a specific purpose. Vulnerability—The characteristic of a system that causes it to suffer a definite degradation (loss or reduction of capability to perform its designated mission) as a result of having been subjected to  a  certain  (defined)  level  of  effects  in  an  unnatural  (man-made)  hostile  environment.  Vulnerability is considered a subset of survivability.  (DAG) Waiver—A decision not to conduct OT&E required by statute or policy. Weapon  System  Evaluation  Program  (WSEP)—A test program conducted by MAJCOMs to provide an end-to-end tailored evaluation of fielded weapon systems  and  their support systems using  realistic  combat  scenarios.    WSEP  also  conducts  investigative  firings  to  revalidate capabilities or better understand munitions malfunctions.   112 AFI99-103  6 APRIL 2017 Attachment 2 INFORMATION REQUIREMENTS FOR OSD T&E OVERSIGHT PROGRAMS information A2.1.  Information  Requirements.  Table  A2.1  provides  details  about exchanges and interfaces between the Air Force and OSD.  The requirements in this table may be modified  by  direction  of,  or  by  specific  agreement  with,  the  program  action  officer(s)  in AF/TEP, DASD(DT&E), and DOT&E. the Table A2.1.  Information Requirements for OSD T&E Oversight Programs. Item of Information  HAF OPRs Due to OSD2 Comments TEMPs1 a.  Draft TEMP3 b.  Service-approved TEMP   c.  Newly-designated TEMP  OPR:  PEM6 OCR:  AF/TEP a.  90 days prior to milestone b.  45 days prior to milestone, and again at 10 days prior if OSD sends back for changes c.  120 days after program designation for OSD T&E Oversight  OSD (i.e., DASD(DT&E)) and DOT&E) approval required prior to milestones and major decision reviews.  “Updates” required for significant changes.  “Administrative changes” required for minor updates.   OPR:  PEM OCR:  AF/TEP Prior to MS B DOT&E sends notification to Congress prior to MS B.  LFT&E Waivers and Alternate LFT&E Strategies and Plans (if required)  Test Concept Briefings for IOT&E, QOT&E, OUE, FOT&E or FDE, to include all types of OAs.  See Note 7 for FDEs.  AF/TEP Test Plans for IOT&E, QOT&E, OUE, or FOT&E, to include all types of OAs (Service-approved)  AF/TEP FDE Plans7  AF/TEP IOT&E, QOT&E, OUE, or FOT&E Test Concept Briefings (to include all OAs) 180 days prior to test start unless waived by DOT&E.8    Required 60 days prior to start of IOT&E, QOT&E, OUE, FOT&E, to include all OAs.  See DoDI 5000.02, ¶6.d.(14) and Enclosure 6, ¶5.a.(2) Note: DOT&E may request an additional briefing on test plans prior to starting these tests.  60 days prior to start of designated FDEs.4, 7 Note: DOT&E may request an additional briefing on test plans prior to starting these tests.  Requirement stated in DoDI 5000.02, Enclosure 2, ¶6.d.(14), and Enclosure 6, ¶5.a.(2). DOT&E written approval required before IOT&E, QOT&E, OUE, FOT&E, or OA may start.  Report major revisions to DOT&E.   Note: A briefing may be required on these plans at DOT&E’s discretion. DOT&E will direct which subparts of OT&E Oversight programs require approval.  Significant Test Event Reports a. PEM for DT&E   b. AF/TEP for OT&E OPR:  OTO 24 hours after event Events and addressees as listed in TEMP and test plans.  AFI99-103  6 APRIL 2017 113 Item of Information  HAF OPRs Due to OSD2 Comments Final Reports and Briefings: a.  For OA, IOT&E, QOT&E, OUE, and FOT&E  b.  For FDE7 AF/TEP OPR:  ITT a. and b.  Reports due not later than 30 days prior to the decision review according to paragraph 7.4.2.  For multi-Service tests, reports are due 45 days prior to the decision review.     A single report is required for multi-Service programs.   Final results briefings will be provided to DOT&E as requested. LFT&E Reports OPR:  PEM OCR:  AF/TEP 45 days prior to the FRP/FD decision review. Due to DOT&E.   Synopsis Reports of EW Programs     AF/TEP Due annually by 15 Nov to DASD(DT&E) Congressionally required.5 Notes: 1. All references to TEMP in this table are meant to include the tailored implementing documentation described in paragraph 5.17, whichever is applicable.  Only the T&E portions of tailored implementing documents require AFOTEC/CC, LDTO, and AF/TE coordination, and DASD(DT&E) and DOT&E approval.  2. All days are “calendar” days.  Time periods and dates are “Not Later Than” due dates to OSD.   3. “Draft TEMP” means that all signatures below HQ USAF level or below the final signature for non-OSD oversight programs are complete according to paragraphs 4.11.4 through 4.11.8.  4. Only for programs on OSD OT&E Oversight.  5. Required by P.L. 103-160 § 220(a).  6. The PEM is the person from the Secretariat or Air Staff who has overall responsibility for a program element and who harmonizes program documentation.  7. Selected FDEs require DOT&E Oversight (see paragraph 4.7) and will follow the same planning, briefing, and reporting guidance in paragraph 6.6.   8. DOT&E memo, Timelines for Operational Test and Evaluation (OT&E) Plans, 24 June 2011 (or latest update), to be inserted into next revision to DoDI 5000.02.   