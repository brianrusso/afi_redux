BY ORDER OF THE  SECRETARY OF THE AIR FORCE   AIR FORCE HANDBOOK 36-2235 VOLUME 8 1 NOVEMBER 2002   Personnel  INFORMATION FOR DESIGNERS OF INSTRUCTIONAL SYSTEMS  APPLICATION TO AIRCREW TRAINING  Certified by: HQ USAF/DPDT (Col Patricia L. C. Priest) Pages: 253/Distribution: F  RECORDS  DISPOSITION:    Ensure  that  all  records  created  by  this  handbook  are maintained and disposed of IAW AFMAN 37-139, “Records Disposition Schedule”  NOTICE:    This  publication  is  available  digitally  on  the  AFDPO  www  site  at: http://afpubs.hq.af.mil.   OPR: HQ AETC/DOZ (Gary J. Twogood)  Supersedes: AFH 36-2235, Volume 8, 1 November 1993    This  volume  provides  information  and  guidance  for  applying  the  Instructional  System Development process described in AFMAN 36-2234.  This handbook is a guide for Air Force  personnel  who  plan,  design,  develop,  approve,  administer,  or  manage  aircrew training in the Air Force.  It is designed to be used as you need it in aircrew training.  Although  several  references  are  listed,  this  handbook  is  intended  to  be  used  alone without any requirement to read other ISD handbooks.  Each handbook is developed for a particular community and has the appropriate language and applications to support it.  You are not required to read this handbook from cover to cover but may enter it at any desired  phase  of  instruction  where  information  is  available  to  resolve  your  problem.  You may use this handbook to design or revise a curriculum, a course, or any isolated aspect of it. Chapter 1 GENERAL INFORMATION........................................................................... 4 Chapter 2 INSTRUCTIONAL SYSTEMS DEVELOPMENT MODEL ............................. 9 Figure 1 System Functions............................................................................................ 10 Figure 2 Functions with Phases .................................................................................... 11 Figure 3 Updated AF ISD Model ................................................................................... 14 Section A Analysis .................................................................................................. 16 Section B Design..................................................................................................... 18 Section C Development .......................................................................................... 20 Section D Implementation....................................................................................... 22 Section E Evaluation............................................................................................... 23 AFH 36-2235 Volume 8 1 November 2002 2 Section F Total Training System Functions............................................................. 25 Section G Quality Improvement.............................................................................. 26 Figure 4 Chart it, Check it, Change it ............................................................................ 32 Figure 5 Shewhart Cycle ............................................................................................... 33 Figure 6 Standard Flowchart Symbols .......................................................................... 35 Figure 7 (Part 1) Training Development Flowchart - Planning ...................................... 37 Figure 7 (Part 2) Training Development Flowchart - Analysis ....................................... 38 Figure 7 (Part 3) Training Development Flowchart - Design ......................................... 39 Figure 7 (Part 4) Training Development Flowchart - Development ............................... 40 Figure 7 (Part 5) Training Development Flowchart - Implementation and Evaluation ................................................................................................................. 41 Figure 8 Training System Metric Process...................................................................... 42 Chapter 3 PLANNING.................................................................................................. 43 Section A Analyze Training Needs.......................................................................... 45 Section B Determine Personnel Requirements and Responsibilities ...................... 48 Section C Determine Training Needed to Meet Requirements ............................... 51 Section D Determine Training System Requirements............................................. 53 Section E Develop ISD Evaluation Plan.................................................................. 55 Section F Write Training Development Plan ........................................................... 59 Section G Design Training Information Management System................................. 62 Chapter 4 ANALYSIS................................................................................................... 64 Figure 9 Analysis Phase................................................................................................ 64 Section A Conduct Task Analysis ........................................................................... 66 Section B Conduct Target Population Analysis....................................................... 76 Section C Analyze Resource Requirements / Constraints ...................................... 78 Section D Update ISD Evaluation Plan ................................................................... 83 Section E Update Training Development Plan ........................................................ 85 Chapter 5 DESIGN....................................................................................................... 86 Figure 10 Design Phase................................................................................................ 86 Section A Review Types of Learning ...................................................................... 88 Section B Select Training Methods ......................................................................... 94 Section C Develop Objectives................................................................................. 98 Figure 11 Task Hierarchy ............................................................................................ 108 Section D Develop Tests....................................................................................... 113 Section E Review Existing Materials..................................................................... 128 Section F Design Training..................................................................................... 131 Section G Update Training Development Plan...................................................... 144 Chapter 6 DEVELOPMENT ....................................................................................... 145 Figure 12 Development Phase.................................................................................... 145 Section A Prepare Course Documents ................................................................. 147 Section B Develop Training Materials ................................................................... 157 Section C Develop Training Alternatives............................................................... 161 Section D Install Training Information Management System................................. 162 Section E Update ISD Evaluation Plan ................................................................. 163 Section F Update Training Development Plan ...................................................... 165 AFH 36-2235 Volume 8 1 November 2002 3 Section G Finalize Training Materials.................................................................... 166 Chapter 7 IMPLEMENTATION .................................................................................. 168 Figure 13 Implementation Phase ................................................................................ 168 Section A Implement Training System Functions.................................................. 170 Section B Conduct Training .................................................................................. 177 Section C Conduct Operational Evaluation ........................................................... 183 Section D Revise Courseware .............................................................................. 207 Chapter 8 EVALUATION ........................................................................................... 215 Figure 14 Evaluation ................................................................................................... 215 Section A  Validate Training.................................................................................. 218 Section B Formative Evaluation ............................................................................ 230 Section C Summative Evaluation.......................................................................... 233 Section D Operational Evaluation ......................................................................... 234 Attachment  1  –  GLOSSARY  OF  REFERENCES  AND  SUPPORTING INFORMATION..................................................................................................... 237  AFH 36-2235 Volume 8 1 November 2002 4 Introduction Background Chapter 1 GENERAL INFORMATION Overview  Training from the back seat of an aircraft with a new student is often a harrowing experience.  Questions such as "When do I take over control of the aircraft?  Does the student know what he or she is doing?  What am I doing here?" are not new.  All instructor pilots have these feelings at one time or another.  To ensure that the ground training and flying training support each other requires a systematic approach that standardizes each training program.  No one in the training business wants to lose an aircraft or crewmember, or be unable to defeat the enemy.  Therefore, to assist you with this very important mission, this handbook serves as a guide for applying Instructional System Development (ISD) to the design, implementation, administration, and management of aircrew training in the Air Force.  It adheres to the policies of AFPD 36-22 and follows the principles and procedures of AFMAN 36-2234.  The guide is intended to be easy reading for novice and experienced managers, curriculum developers, and instructors.  While it is designed as a "stand-alone" document, reading and being familiar with AFPD 36-22 and AFMAN 36-2234 will make developing training easier.  As the need arises for more information on specific areas of training development, it is recommended that you read the other sources of information referenced throughout this handbook.   In the past, the Air Force ISD manual and handbook were focused on applying ISD to technical training.  There was little, if any, guidance on how to apply ISD in the other training communities such as aircrew and acquisition.  The revised AFMAN 36-2234 provides the necessary principles, procedures, and guidance for applying ISD throughout the Air Force, with the various volumes of AFH 36-2235 providing more detailed and specific information for applying ISD in the various communities.  Also, other volumes of AFH 36-2235 have been developed to cover such areas as selecting the appropriate computer-based training, applying ISD to unit-level training, and a look at the latest training technologies available to the aircrew trainer.  AFH 36-2235 Volume 8 1 November 2002 5 Purpose Is this handbook for you?         What is ISD? Yes No   This handbook provides specific information and guidance for using the ISD process to develop aircrew training.  It provides information on the ISD model, planning ISD projects, phases of development, system functions, and quality improvements.   This handbook addresses the question: "How do you apply ISD to aircrew training?"  It is applicable to both Air Force and contractor-developed training.  But is it for you?  Are You Responsible For . . . Developing training for any task associated with flying aircraft? Managing contractors during training development? Are You . . . An aircrew member responsible for developing and delivering a training course?A training specialist using ISD to develop aircrew training? A "novice" curriculum developer?  If you checked YES to any of these questions, this handbook will help you do your job.   ISD is a systematic, but flexible, process used to plan, design, develop, and deliver an effective and efficient training system. It is a total quality process, which continuously strives to improve the system.  ISD ensures that:  No  Yes         There is a training need. There is a solution to the need. The solution can be implemented. The solution can be assessed to determine whether it meets the need. Quality improvements are made throughout the ISD process.  AFH 36-2235 Volume 8 1 November 2002 6 Basis for ISD Why use ISD? Goals of ISD How to use ISD  ISD is based on:  Basic research as to how people learn Basic research as to how people communicate The systems engineering process The concepts of instructional technology   Using ISD helps ensure that training is both effective and efficient.  ISD requires that you:  Design training to meet specific job requirements that have been identified through a training needs assessment. Design training for all job elements that are critical to successful performance. Design training to meet specific training objectives. Choose methods and media to optimize cost-effectiveness and efficiency of training. Evaluate training to make sure the objectives are being met. Collect and use student data to improve training quality.   The goals of ISD are to:    ISD is: Produce graduates who meet job performance requirements. Produce a cost-effective, efficient training system. Continually improve training quality. Flexible and systematic A tool to get the right training to solve the problem Not a lock-step, linear process  ISD is a cyclic process that begins with the analysis phase, during development of the instructional system, and continues through the implementation phase.  Evaluation is an active, integral part of each phase of the process and the source of   AFH 36-2235 Volume 8 1 November 2002 7 How to use ISD (Continued) How to use this handbook                   Yes No  Page48   continuous feedback and improvements for the life cycle of the system.  During initial system development, instructional system developers may reenter phases of the process, as necessary, to ensure effectiveness and cost-efficiency of the system.  During updates or revisions to a mature system, the developer enters the phase of the process as determined by the nature and scope of the update and reenters phases of the process if necessary.   This handbook will help you develop aircrew training courses.  Think about your specific assignment and use the information in this handbook to design, develop, and implement an effective and efficient training system.  Start by using the following questionnaire to read about sections that affect you.  Do You Have To . . . Determine personnel needs of the system? Determine Training Needed to Meet Requirements? Determine training system requirements? Develop an ISD evaluation plan? Write a training development plan? Design Training Information Management System? Analysis? Conduct a task analysis? Conduct a target population analysis? Analyze Resource Requirements / Constraints? Develop objectives? Develop tests? Review existing materials? Design training?  98 113 128 131 53 55 59 62 64 66 76 78               51               AFH 36-2235 Volume 8 1 November 2002 8 How to use this  handbook (Continued)       Do You Have To . . . Select Media? Conduct training? Conduct operational evaluations? Validate training? Conduct formative evaluations? Conduct summative evaluations?  Yes       No  Page137   178 184   219 230   233 AFH 36-2235 Volume 8 1 November 2002 9 INSTRUCTIONAL SYSTEMS DEVELOPMENT MODEL Chapter 2 Overview Introduction Objectives Updated Air Force ISD model System functions  Designing, developing, implementing, and supporting a training system requires considerable time and effort on the part of managers and instructional system developers.  Developers must design, develop, and implement effective and efficient training, while managers must control, coordinate, and integrate the training into a total training system using the principles of Quality Improvement (QI).  The total instructional system model includes the ISD phases, the system functions, and the QI process.   The objectives of this chapter are to:  Describe the phases of ISD. Discuss the system functions. Explain the QI process.   The updated ISD model has been designed to represent simplicity and flexibility so that instructional designers with varying levels of expertise can understand the model and use it to develop effective, efficient instructional systems.  This model depicts the flexibility that instructional designers have to enter or reenter the various stages of the process as necessary.  The entry or reentry into a particular stage of the process is determined by the nature and scope of the development, update or revision activity.   An extension of the systems approach places the ISD process within the functional design of a total instructional system.  Figure 1 shows the basic top-level system functions of the instructional system: management, support, administration, delivery, and evaluation.  AFH 36-2235 Volume 8 1 November 2002 10 System functions (Continued)  Figure 1 System Functions  Functions   Relation to ISD   The system functions of the ISD model are:  Management: The function of directing or controlling instructional system development and operations. Support: The function of maintaining all parts of the system. Administration: The function of day-to-day processing and record keeping. Delivery: The function of bringing instruction to students. Evaluation: The function of gathering feedback data through formative, summative, and operational evaluations to assess system and student performance.   Using these essential functions to design the overall instructional system architecture and then allocating them to the respective instructional system components, or people responsible, ensures that these functions are operational when the total training system is fielded.  ISD products are integrated into the total instructional system, and aspects of the instructional system functions are active throughout all phases of the ISD process.  AFH 36-2235 Volume 8 1 November 2002 11 Relation to ISD (Continued)  Figure 2 shows the phases most often used in the systems approach, which are analysis, design, development, and implementation, with the evaluation activities integrated into each phase of the process.  The phases are embedded within the system functions.  Evaluation is shown as the central feedback "network" for the total system.  Figure 2 Functions with Phases   The instructional development process, which the model summarizes, calls for instructional designers to:  Analyze and determine what instruction is needed. Design instruction to meet the need. Develop instructional materials to support system requirements. Implement the instructional system.  Evaluation is a central function that takes place at every phase.  Symbolically, Figure 2 shows that all phases of the model depend on each of the other phases.  The ISD process allows the instructional designer or design team to enter or reenter the various phases of the process as determined by the nature and scope of the development or revision activity.  The phases of the updated model are described below.  AFH 36-2235 Volume 8 1 November 2002 12 Analysis phase Design phase Development phase  In courses that tie the content directly to preparing a student to do a job, the instructional designer analyzes the job performance requirements and develops a task list.  The designer then analyzes the job tasks and compares them with the skills, knowledge, and abilities of the incoming students.  The difference between what they already know and can do and what the job requires them to know and be able to do determines what instruction is necessary.  The activities of formative evaluation begin.   In the design phase, the instructional designer develops a detailed plan of instruction, which includes selecting the instructional methods and media, and determining the instructional strategies.  Existing instructional materials are reviewed during this phase to determine their applicability to the specific instruction under development.  In this phase, the instructional designers also develop the instructional objectives and test and design the instruction.  The implementation plan for the instructional system is developed in this phase and a training information management system is designed, if required.  Formative evaluation activities continue in this phase.   In the development phase, both the student and instructor lesson materials are developed.  If the media selected in the design phase included items such as videotapes, sound/slides, interactive courseware (ICW), and training devices, these are developed.  If a training information management system was developed for the instructional system, it is installed in this phase.  As a final step in this phase, the implementation plan is updated.  During this phase, instructional designers also validate each unit/module of instruction and its associated instructional materials as they are developed.  They correct any deficiencies that may be identified.  Validation includes:  Internal review of the instruction and materials for accuracy Individual and small-group tryouts Operational tryouts of the "whole" system   Revision of units/modules occurs as they are validated, based on feedback from formative and summative evaluation activities.  The final step in this phase is to finalize all training materials.  AFH 36-2235 Volume 8 1 November 2002 13 Implementation phase Evaluation   The instructional system has been designed and developed, and it is now time for the actual system to become operational.  In this phase, the instructional system is fielded under operational conditions and the activities of operational evaluation provide feedback from the field on the graduate's performance.   Evaluation is a continuous process beginning during the analysis phase and continuing throughout the life cycle of the instructional system.  Evaluation consists of:  Formative Evaluation, consisting of process and product evaluations conducted during the analysis and design phases, and validation, which is conducted during the development phase.  Included are individual and small group tryouts.  Summative Evaluation, consisting of operational tryouts and conducted as the last step of validation in the development phase.  Operational Evaluation, consisting of periodic internal and external evaluation of the operational system during the implementation phase.  Each form of evaluation should be used during development, update, and revision of instruction, if possible, and if the form of evaluation is application.  AFH 36-2235 Volume 8 1 November 2002 14 Quality improvement  Figure 3 depicts the completed ISD model.  This completed figure shows the system functions and ISD phases embedded within the quality improvement (QI) process.  Figure 3 Updated AF ISD Model   The QI process, which is further discussed in this chapter, is briefly described below.  Quality improvement is the continuous, organized creation of beneficial change to the system.  The objective of quality improvement is to foster continuous improvement in the process and products of ISD.  It is an independent evaluation to determine whether the products are meeting the users' needs.  The objective of quality improvement is to ensure on-time development of high-quality courseware that enables students to reach the desired performance levels in an effective and cost-efficient manner.  The updated model graphically illustrates that:  Evaluation is the "centerpiece" to the ISD process. ISD is a continuous process with the flexibility to enter and reenter the various phases, as necessary, to develop, update, or revise instruction.  AFH 36-2235 Volume 8 1 November 2002 15  Quality improvement (Continued) All ISD activities take place within and are dependent on the system functions. Teamwork is required between personnel performing system functions and those designing, developing, and implementing instructional systems. All ISD activities and system functions focus on continuous quality improvements in the system. Where to read about it            This chapter contains seven sections.  Section A B C D E F G   Title Analysis Design Development Implementation Evaluation Total Training System Functions Quality Improvement  Page 16 18 20 22 23 25 26 AFH 36-2235 Volume 8 1 November 2002 16 Section A Analysis What is it? Why do it? Where to read about it          When you do it What you get  During analysis, you:  Collect information on missions/jobs/duties/tasks. Analyze the information. Determine job performance requirements.   You conduct analysis to identify the training needs so training can be developed to meet those needs.   Details on the analysis phase of ISD are available in Chapters 3 and 4.  Specific topics are listed below. Title Page 45 66 76 78 83 85  Analyze Training Needs Conduct Task Analysis Conduct Target Population Analysis Analyze Resource Requirements/Constraints Update ISD Evaluation Plan Update Training Development Plan   Conduct an analysis before you begin to design new or revise existing training.   If you have conducted the analysis correctly, you will get valid training requirements and accurate predictions of the resource requirements.  AFH 36-2235 Volume 8 1 November 2002 17  To conduct analysis, you need to assess:  Equipment Subject matter experts (SMEs) Weapon system data Existing course materials Technical data Occupational Survey Report (OSR) data Engineering data Similar system or programs Training standards What you need   AFH 36-2235 Volume 8 1 November 2002 18 Section B Design What is it?  Why do it?   Where to read about it          When you do it   Training design is like architectural design.  You figure out what you want the training to look like and how you want it to work.  The analysis that you previously conducted will help determine the design of the training in this phase.   The purpose of design is to be sure that the product teaches.  If a product does not teach it has no value.  Supposedly a designer knows what is effective in promoting learning for different kinds of outcomes.  The designer then builds into the instruction those conditions that are necessary for effective and efficient learning to occur.  If these conditions are not built into the materials then there is strong likelihood that the materials will not teach.  The additional benefits of design are to save money, increase product quality, and get the training done on time.  You don't just go out and start developing instruction, just as you don't start building a training facility without planning and designing it first.   Details on the design phase of ISD are available in Chapter 5.  Specific topics are listed below.   Review Types of Learning Select Training Methods Develop Objectives Develop Tests Review Existing Materials  Design Training  Update Training Development Plan   Design the training system before beginning to develop training.  Page 88 94 98 113 128 131 144 Topic AFH 36-2235 Volume 8 1 November 2002 19 What you get  What you need     Proper design will result in:  Training objectives  Tests that measure the objectives Training methods, media, and strategies to deliver the training Training information management systems   For ISD design, you need all the products developed during initial project planning and the analysis phase.  AFH 36-2235 Volume 8 1 November 2002 20 What is it? Why do it? Where to read about it          When you do it Section C Development  During this phase, training is developed including all training materials.  After the training has been developed, follow-on evaluation (validation) phase ensures that each component is contributing to the overall effectiveness and efficiency of the training system.   Development is the production of the materials.  Without development there is no instruction except perhaps in the case of extemporaneous, live instructions.  Technology production (development) has become a highly specialized area with different people needed for different aspects of the production (artists, technical writers, computer programmers, web site designers, etc.)  The purpose of development is to have a valid product ready for implementation.   Details on the development phase of ISD are available in Chapter 6.  Specific topics are listed below.   Prepare Course Documents Develop Training Materials Develop Training Alternatives Install Training Information Management System Update ISD Evaluation Plan Update Training Development Plan Finalize Training Materials   Develop training after it has been designed and before it is implemented.  Page 147 157 161 162 163 165 166 Topic AFH 36-2235 Volume 8 1 November 2002 21 What you get What you need  Properly developed training results in training that meets the design specifications.   For development you need:  Analysis and design documents and products Students and equipment for validation   AFH 36-2235 Volume 8 1 November 2002 22 Section D Implementation What is it? Why do it? Where to read about it       When you do it What you get What you need   Topic  In this phase, the training system that has been designed and developed is implemented and the system produces graduates.   Implementation produces graduates who have been trained to meet a specific need.   Details on the implementation phase of ISD are available in Chapter 7.  Specific topics are listed below.   Implement Training System Functions Conduct Training Conduct Operational Evaluation Revise Courseware   Training is implemented after it has been validated and revised.   Successfully implemented training results in graduates who meet the job performance requirements.   For implementation you need:  Page 170 177 183 207 Finished training product All training system functions in place  AFH 36-2235 Volume 8 1 November 2002 23 Section E Evaluation What is it? Why do it? Where to read about it        When you do it What you get  Evaluation measures the quality, effectiveness, and efficiency of the training system.  Evaluation answers the questions:  Is the process effective and cost-efficient? Are quality products being developed? How well are the course graduates performing on the job? How can the system be improved?   Evaluation improves the quality of the ISD process and products while producing graduates who can meet job performance requirements.   Details on evaluation of ISD are available in Chapter 8.  Specific topics are listed below. Topic Page 218 230 234 235  Validate Training Formative Evaluation  Summative Evaluation Operational Evaluation   Evaluation begins in the initial planning stages of the ISD project and continues throughout the life cycle of the system.   Evaluation provides data on the quality of the ISD process and products and determines whether graduates are meeting job performance requirements.  AFH 36-2235 Volume 8 1 November 2002 24 What you need  To properly perform evaluation you need:  Evaluation plan Completed ISD activities ISD products Operational system Graduates Evaluator   AFH 36-2235 Volume 8 1 November 2002 25 Section F Total Training System Functions  The system functions must be in place before a training system can be properly operated and maintained.  The basic training system functions are shown below.  Function Management Support Administration Delivery Description The practice of directing and controlling all aspects of the training system. Provides for and maintains the training system on a day-to-day and long-term basis.  Examples are resources you need to keep equipment functioning. The part of management that performs day-to-day tasks such as documentation, student assignments, and student records. The means by which training is provided to students.  Instructors, computers, printed materials, audio, visual programs are all examples of training delivery methods.   System functions must be in place and working before the training system is implemented.  These functions will support the training system throughout its life cycle.  What are system functions?       When you implement  AFH 36-2235 Volume 8 1 November 2002 26 Section G Quality Improvement Introduction What is it?  Objectives of QI  Results  ISD and quality relationship  ISD is a cyclic, systematic process that continually evaluates the instructional system throughout its life cycle.  The Air Force uses the ISD process as a tool to ensure that instructional systems deliver quality instruction to Air Force personnel in the most effective and efficient manner possible.  The ISD process implements the principles of the Quality Air Force (QAF) program.  Quality improvement (QI) is the continuous, organized creation of beneficial change to the system.  It is an independent evaluation to determine whether the instructional products are meeting the students' needs.   The objective of quality improvement is to foster continuous improvement in the products and processes.   Quality instructional product development results in:  Increased student satisfaction Products that are easy to use and maintain An increase in the students' ability to perform on the job  Quality instructional design results in:  Fewer errors Less rework (and waste) Higher success training Less time spent in development of new training products Lower life cycle costs   The principles of quality are implemented in the ISD process, resulting in total quality in the education and training environment by continuously evaluating the processes and products.  The relationship between the key concepts of QI can be easily seen in the ISD process.   AFH 36-2235 Volume 8 1 November 2002 27 ISD examples  ISD examples are:  Customers:    Team Players: Know your customers.  The information gained in the mission/job analysis process gives the instructional design team information that defines the customer's expectations. Focus on customers.  As mentioned earlier, the needs of the work center drives the instructional needs.  By continuing to trace the relationship between the job requirements and all aspects of the instructional program, you maintain a continual focus on the actual field requirements.  In addition, ISD also requires that the capabilities, aptitudes and attitudes of the student target population be considered during the design process. Foster teamwork.  A training program cannot be designed and developed in a vacuum.  In order to develop effective training, the design team must include representatives from the work center and evaluation offices.  This helps ensure that the training matches the performance requirements of the job. Empower your people.  ISD is a problem solving, decision- making model.  The flexibility of the process, combined with the fact that there are any number of ways to solve a given training problem, requires that design teams be allowed freedom and authority to design, develop, and implement training that meets job performance requirements.  Final Product: Know your mission.  ISD depends on mission and job analysis for basic data.  All instruction must be based directly on mission or job requirements.  The checks in the process help eliminate instruction not related to the job. Job analysis uses data from many sources, including mission statements found in regulations or locally developed statements.  Analysts also make use of management engineering reports, occupational survey data, and direct observation to determine the actual job requirements.  As part of the job analysis process, a training needs assessment is conducted to arrive at the actual performance   ISD examples (Continued) AFH 36-2235 Volume 8 1 November 2002 28 problem.  In some cases, a problem is not related to lack of training, but to a problem with the job structure or environment.  The ISD process helps ensure that you don't build a training program for a non-training problem.  Set goals and standards.  Goals and standards for an instructional development effort come in many variations.  First, the job requirements and the impact of the performance deficiency determine the timing required for the development process and the conduct of the instructional program.  Second, the content of the training is determined by the job performance requirements.  The design team should directly translate the cues, conditions, and performance standards of the job directly into the instructional program. Manage by fact.  Each phase of the ISD process requires constant evaluation against the job requirements identified earlier in the process.  In addition, a variety of tools have been developed to help ensure that design and development decisions are made with supporting data.  For example, a number of media selection tools are being used that provide managers with information that matches training media with the training requirements.  These matches are based on learning theories and development cost factors (money and time).  ISD is designed to guide managers and developers to awareness of factors affecting their decisions. Integrate quality in all phases.  Evaluation is continuous quality checking.  This is true during each phase of the ISD process, from analysis to implementation.  Built-in checks in each phase ensure the quality of the ISD process and products.  The emphasis is on satisfying the job performance requirements and producing graduates who can do their jobs. Evaluate quality constantly.  The ISD process is a cyclical, ongoing process of continuous improvements.  As curriculum developers progress through the different phases of ISD, the process and products of each phase are constantly evaluated against the job requirements and principles of learning.  The       AFH 36-2235 Volume 8 1 November 2002 29 ISD examples (Continued) Other ISD examples   results of the evaluations determine which phase of ISD to enter next.  Constant evaluation identifies changes in job requirements due to updates in equipment and personnel, which results in new ISD efforts to provide the best possible training to Air Force personnel.   Other ISD examples are:  Set goals and standards.  The goals and standards for an instructional development effort come in many variations.  First, the job requirements and the impact of the performance deficiency determine the timing required for the development process and the conduct of the training program.  Second, the content of the instruction is determined by the person's need to do the job.  The design team must directly translate the cues, conditions, and performance standards of the job directly into the instructional program.     Manage by fact.  Each phase of the ISD process requires constant evaluation against the job requirements identified earlier in the process.  In addition, a variety of tools have been developed to ensure that design and development decisions are made with supporting data.  For example, a number of media selection tools are being used which provide managers information that matches training media with the instructional requirements.  These matches are based on learning theories and development cost factors (money and time).  ISD is designed to guide managers and curriculum developers to awareness of factors affecting their decisions. Integrate quality in all phases.  Evaluation is continuous.  This is true during each phase of the ISD process, from analysis to implementation.  Built-in checks in each phase ensure quality of the ISD process and instructional products, with emphasis on the graduate's performance.   Evaluate quality constantly.  The ISD process is a cyclic, ongoing process.  As curriculum developers progress through the different phases of ISD, the process and products of each phase are constantly evaluated against the instructional requirements and principles of learning.  Results of the  AFH 36-2235 Volume 8 1 November 2002 30 Other ISD examples (Continued) Basis of process improvement   evaluations determine which phase of ISD to enter next.  Constant evaluation identifies changes in instructional requirements due to updates in equipment and personnel, which results in new ISD efforts to provide the best possible instruction to Air Force personnel.   The basis of process improvement is Quality Air Force (QAF).  QAF is a management philosophy and a methodology that work together to produce continuous process improvements.  It is based on ten principles:  All work is a process.  Processes receive work from suppliers, add value and deliver output to customers.   Anyone from whom a process receives work is a supplier.  Anyone to whom a process delivers output is a customer.  Customers have needs and expectations.  Customers will define and measure quality in terms of those needs and expectations.  Quality is meeting customer needs and expectations.  Improving process quality increases productivity.  Processes can be identified, understood, measured, and improved.  The people who operate the processes know best how to improve them. AFH 36-2235 Volume 8 1 November 2002 31 Procedure for process improvement          Ways to implement the procedure   In order to ensure process improvements, you will need to use a systematic method to identify and correct the causes of the problems.  The six steps of process improvement are outlined in the next table.  Step 1 2 3 4 5 6   Activity Define the process and determine the main problem areas. Analyze the problems and identify the causes of each. Identify and evaluate possible changes to the process. Implement the changes and monitor the process. Institutionalize the changes. Repeat for continuous improvements.   There are many different ways to implement the basic procedure mentioned above.  Two of the ways are:  "Chart It, Check It, Change It" Shewhart Cycle (plan-do-check-act)  Each of these techniques uses the six basic steps mentioned above.  AFH 36-2235 Volume 8 1 November 2002 32 What it is      How to use it  Step 1 2 3   Chart It  Check It "Chart It, Check It, Change It"  This technique is a systematic approach to continuous improvement.  This approach has three principal steps, as shown below and in Figure 4.  Name  What You Do Chart It Check It Change It Describe the process. Gather data. Analyze the data. Evaluate the process. Identify opportunities. Improve the process. Institutionalize the change. Using a process flowchart, describe the process to be improved. Gather data on the process and its products. Analyze the data to isolate the problems and opportunities. Evaluate the process to identify alternative approaches. Identify opportunities (i.e., useful changes) from the alternatives.  Change It Improve the process by implementing changes identified as opportunities.  Figure 4 Chart it, Check it, Change it CHART  ITCHECK  ITCHANGE  IT   Institutionalize the changes through training, standardization, etc.  Then, use another process (or use this same one again) to make further improvements. AFH 36-2235 Volume 8 1 November 2002 33 "Shewhart Cycle"  The Shewhart Cycle is a systematic approach to achieving a continuous improvement in quality.  The cycle includes planning, doing, checking, and acting.  Because the approach involves repetition, it is represented graphically as a circle in Figure 5.   Figure 5 Shewhart Cycle ACTON THERESULTSPLANANAPPROACHCHECKTHERESULTSDOTHEACTIVITY  To use the Shewhart Cycle, follow the steps listed below.  Plan an approach for quality improvement.  Study the process flow and any existing data.  Formulate possible improvements, experiments to be run, or additional data to be gathered.     Do the activity planned.  Implement the improvement effort that you planned.  Train the people who are responsible for implementation. Check the results.  Measure the results of the improvement effort you implemented.  Analyze the data you collected. Act on the results.  If the effort was truly an improvement, standardize and document it.  If it wasn't successful, determine what could be done to improve it. Repeat.  Continue around the cycle again by planning and carrying out further activity.   Many tools are used to make process improvements.  One tool that is often used to analyze process problems is the flowchart.  What it is  How to use it  Flowchart: A process improvement tool  AFH 36-2235 Volume 8 1 November 2002 34  A flowchart is a graphical representation of all the major steps of a process.  It can help you:  Understand the complete process. Identify the critical stages of the process. Locate problem areas. Show relationships between different steps in a process.   Points on how to use it:  What it is  How to use it  Standard flowchart symbols            Identify the process.  Define the start point and finish point for the process to be examined.  Chart the ideal process.  Try to identify the easiest and most efficient way to go from the start block to the finish block.  While this step isn't absolutely necessary, it does make it easier to find improvements.  Describe the current process.  Chart the whole process (i.e., lay out all the steps) from beginning to end.  You can use standard symbols to improve the clarity of your flowchart.  Search for improvement opportunities.  Identify all areas that hinder your process or add little or no value.  Examine all areas that differ from your ideal process and question why they exist.  Update your chart.  Build a new flowchart that corrects the problems you identified in the previous step.   Figure 6 depicts the standard flowchart symbols.  AFH 36-2235 Volume 8 1 November 2002 35 Standard flowchart symbols (Continued)                         Using process improvement methods  Figure 6 Standard Flowchart Symbols This symbol…  Represents…     Start/Stop Decision Point Activity Document Some examples are… Receive tasking to develop training. Complete training development process. Approve/Disapprove. Yes/No. Develop training/non-training solution. Develop objective. Develop test. Product training materials. Fill out task analysis worksheet. Update training development plan. Document evaluation results.   Connector  (to another page or part of the diagram.) DecisionActivityAB  There are numerous process improvement tools that can be used to document and improve the aircrew training development process.  As managers and curriculum developers, you are encouraged to use a process improvement tool such as flowcharting any time you are involved in developing a new process or revising an existing process.  Also, if you haven't documented your current training development process, it is recommended that you do so in order to improve the process.  AFH 36-2235 Volume 8 1 November 2002 36 Example of the training development process    Figure 7 depicts the overall training development process used to develop aircrew training.  You may want to adapt this flowchart to the specific or unique needs of your training organization.  AFH 36-2235 Volume 8 1 November 2002 37 Figure 7 (Part 1) Training Development Flowchart – Planning  PlanningDevelopNon-trainingSolutionRepeat Cycle123578Receive TrainingTaskingConduct TrainingNeeds AssessmentNoIs Trainingthe Solution?YesEstablish TrainingSystem ConceptDevelop TrainingStrategies46Identify InitialResourceRequirementsA9AFH 36-2235 Volume 8 1 November 2002 38 Figure 7 (Part 2) Training Development Flowchart – Analysis AnalysisA10NoIsOccupationalJob AnalysisRequired?YesOccupational / JobConductAnalysis1112Analyze ResourceRequirements /Constraints17Document ResourcesYou Need18Document OccupationSurvey Report &Training Extract13DocumentTraining PlanConductTask Analysis14ConductEvaluationDocument TrainingStandards(STS, JGS)15Revise ISDEvaluation Plan192021ConductLearning Analysis16Update TrainingStrategies22 B23  AFH 36-2235 Volume 8 1 November 2002 39 Figure 7 (Part 3) Training Development Flowchart – Design DesignB24Develop Objectives25DocumentTraining PlanDevelop Test Items(written / practical)26Develop Test27Design TrainingInformationManagement SystemConduct Evaluationof Plan, Tests,ObjectivesReview ExistingMaterials28ReviseEvaluation Planas Necessary31323334Design Training29Update TrainingStrategies35 FinalizeTraining Plan30 C36AFH 36-2235 Volume 8 1 November 2002 40 Figure 7 (Part 4) Training Development Flowchart – Development DevelopmentC37Prepare Instruction(Task Breakdown)38Revise ISDEvaluation Plan44DocumentInstruction39Validate & ReviseTraining45Develop TrainingMaterials40DocumentValidation46Develop Print-based,ICW, CBT,Audiovisual MaterialsInstall TrainingInformationManagement System(i,e., TrackingDocumentationConduct Installation  414243Finalize TrainingMaterials47Document RevisedTraining Materials48D49AFH 36-2235 Volume 8 1 November 2002 41 Figure 7 (Part 5) Training Development Flowchart – Implementation and Evaluation ImplementationEvaluationD50E54Implement TrainingSystem Functions51ConductEvaulation55Conduct Training52Self-Inspection Reportand Evaulation56E53Conduct ExternalEvaluation57Training EvaluationReport (TER)58Repeat Cycle59   AFH 36-2235 Volume 8 1 November 2002 42 Metrics  Training system metric process   Metrics are standards of measurement or quality indicators that are critical to maintaining quality in the training development process.  The purpose of metrics is to provide qualitative and quantitative evaluations of the process and products within each phase of ISD.  Metrics are based on key factors such as performance, cost, and schedule.  Types of metrics include, but are not limited to, items such as:  Qualitative: Directives Evaluation criteria SME review Format guide  Quantitative:   Personnel/skill allocation Schedule Test and evaluation data   Figure 8 depicts a typical metric process that can be used to develop and measure aircrew training.  It can be modified to meet specific needs.  Figure 8 Training System Metric Process  SelectPhaseTo MeasureIdentifyQualitive andQuantitiveMeasuresAnalyzeResults andRevise asApplicableFEEDBACKEstablishComparativeStandardDevelopandImplementChanges Measure andDescribeDesiredPerformanceMeasure andDescribeComparativeStandardPerformanceRecommendChangesCompare Actualvs. DesiredPerformance withComparativeStandardPerformance toIdentify GapsAFH 36-2235 Volume 8 1 November 2002 43 Introduction  Objectives  Where to read about it          Chapter 3 PLANNING Overview  Planning is a key element in the overall management of a training system as well as the ISD process itself. Proper planning has the same basic system functions of management, support, administration, and delivery.  As a curriculum developer, you may not be involved in the overall planning for the training system; however, you still play a vital role in analyzing the training needs and providing information to the planners as well as identifying situations where inadequate planning has occurred.  You also play a more active role in planning for the design and development processes.    The objectives of this chapter are to:  Conduct training needs analysis. Determine personnel requirements and responsibilities. Discover training needed to meet requirements. Determine training system requirements. Plan and execute ISD management and evaluation plans. Design and update the Training Information Management System The lack of planning is costly.   This chapter contains seven sections. Section A B C D E F G  Title  Analyze Training Needs Determine Personnel Requirements and Responsibilities Determine Training Needed to Meet Requirements Determine Training System Requirements Develop ISD Evaluation Plan Write Training Development Plan Design Training Information Management System Page 45 48 51 53 55 60 63 AFH 36-2235 Volume 8 1 November 2002 44  For additional information on planning, see:  AFMAN 36-2234, Instructional System Development. Knirk, F. G. and Gustafson, K. L. (1986).  Instructional Technology: A Systematic Approach To Education. New York: Holt, Rinehart, and Winston. Rossett, A. (1987).  Training Needs Assessment.  Englewood Cliffs, New Jersey: Educational Technology Publications. Additional information    AFH 36-2235 Volume 8 1 November 2002 45 Section A Analyze Training Needs  Introduction  Purpose   Who is responsible?  When do you conduct TNA?   The planning phase of training development normally begins with a needs analysis to make sure that the stated problem or identified deficiency can be solved with training.  Training will solve only certain types of performance deficiencies.  They are the performer's lack of ability to recall information and lack of knowledge, skill and ability to perform a task and the attitude or willingness to perform.  If the identified performance deficiency does not fall into one of these areas, training cannot solve it, and thus there is no need to proceed further with the training development effort.   Although often confused, there is a distinct difference been a "needs assessment" and a "training needs assessment."  For this reason, the purpose of each is given.  Needs Assessment – The process of analyzing a problem or deficiency in order to determine if the solution is training or non-training related.  Training Needs Assessment (TNA) – The process of identifying the deficiency or "gap" between current performance and desired performance as part of the analysis phase.  This gap equates to or identifies the training need.   Curriculum developers are responsible for conducting the needs assessment, as well as the training needs assessment, if necessary.   Needs or training needs analysis should be one of the first tasks accomplished in the analysis phase.  It is important to do the assessment first, since the problem or deficiency may not have a training solution, which means that proceeding with training development would be inappropriate.  AFH 36-2235 Volume 8 1 November 2002 46  The request to do a needs assessment usually comes from a higher headquarters in the form of a tasking letter, message, or memo.  Also, a needs assessment can be initiated by the curriculum developer as a result of analyzing critiques of an existing course.   Several possible causes of performance problems are:  Personnel were never trained. Personnel did not receive follow-up training after the initial training. Correct training was not provided during the initial training. Personnel are not allowed to perform their jobs correctly. Personnel disregard the rules. Receiving the tasking  Possible causes of  problems  Identifying a need  assessment How to do a needs     The requirement to do a needs or training needs analysis can come from many different sources.  Most needs are identified because individuals are not meeting job performance requirements.  This situation is often caused when job requirements change as a result of new equipment or job tasks.  Several sources for identifying training needs are:   Job evaluations Course critiques Field inputs Accident board findings   The needs assessment process is accomplished in several stages.  The number of stages will vary, depending on the assessment being done.  However, for explanation purposes, six stages will be used to describe the process.  Stage 1 – Define the purpose of the assessment.  Identify information such as who is involved, what the problem is, or who wants the problem solved.  Stage 2 – Identify the data sources to be used in the assessment, such as evaluation reports, critiques, and subject matter experts (SMEs).  AFH 36-2235 Volume 8 1 November 2002 47 How to do a needs  assessment  Non-training solution    Stage 3 – Select data gathering instruments such as questionnaires, interviews, or direct observations.  Stage 4 – Collect data by sending out questionnaires, or conducting interviews.  Stage 5 – Analyze the data by comparing the present performance against the desired performance.  Also, confirm the accuracy and quality of the data gathered.  Stage 6 – Document findings and recommend solutions.  Document the problems, identify the causes, explain the gap or deficiency between the present and desired performance, and recommend a solution.   If the curriculum developer determines that the deficiency in job performance is not training-related, such as equipment obsolescence, a non-training solution should be pursued.  Once a solution has been found, it should be sent back to the organization requesting the needs analysis.  AFH 36-2235 Volume 8 1 November 2002 48 Determine Personnel Requirements and Responsibilities Section B Introduction  Purpose  Who is responsible?  When do you determine requirements and responsibilities?  What is required?   Once it has been determined that training is needed, determining personnel requirements and responsibilities becomes a key factor in getting ready to apply the ISD process to aircrew training development.  In this process you will identify information such as the types of people you need to develop the training, the skills they should have, the number of people you need, when you need them, and their responsibilities.  As you can see, there are several factors to be considered when determining personnel requirements and responsibilities.   The purpose of determining personnel requirements and responsibilities is to ensure that the personnel required to design, develop, implement, and maintain the training system are identified and their responsibilities are defined.   Curriculum developers and their supervisors are normally responsible for identifying personnel requirements for the training system.  During this process, the responsibilities of the individuals are also identified.   Determining training system personnel requirements and their responsibilities is one of the first stages, if not the first, in initial ISD project planning.  This task should be done as early as possible to ensure that the right people, with the right skills, in the right numbers, are available at the right time to develop the training.   In order to determine personnel requirements and responsibilities, several tasks must be performed:   Determine the scope and level of the project. Identify the need for personnel such as the SME. Define the role of the personnel required.  Each of these tasks will be discussed further.  AFH 36-2235 Volume 8 1 November 2002 49  The scope and level of the ISD project will determine the personnel requirements.  For example:  Developing initial and continuation training courses for a new defense system will normally require more personnel than revising training in an existing course. Scope and level of the project  Identifying need for SMEs      Developing a course using interactive courseware (ICW) will require more personnel, using different skills, than are required for print-based materials. Levels of complexity within ICW will impact personnel requirements since the more complex the material, the more time it takes to design and develop. Using a multimedia approach in a course(s) will impact the type, number, and skills of personnel needed to develop the training.  In one case you may need computer programmers while in another case you may need graphic artists, sound engineers, and photographers. Define the scope and level of the ISD project early in the initial planning stages.  Early in the planning stage, you may not be able to define the exact scope and level of the ISD project.  However, as you enter the actual analysis and design phases of training development, the scope and level of training will be clearly defined, allowing personnel requirements to be determined.        Once the scope and level of the ISD project have been adequately defined, you may need content specialists to provide the technical expertise (pilots, weapon system officers).  Sometimes these SMEs are not available and someone from the weapon system or similar system will require training to perform as the SME.  When identifying personnel requirements, you should:  Identify the specialties that are needed for the project. Indicate the number of each specialty required. AFH 36-2235 Volume 8 1 November 2002 50  Identifying need for SMEs (Continued)  Define roles  List any special skills the specialist may need. Estimate when specific specialists will be needed and for how long. Identify how they will retain currency, if necessary.  Identification of specialists should be completed as soon as possible to ensure that the specialists needed on the project have sufficient time to schedule their work on the project.  In cases where a specialist may need to be permanently assigned to the project, early identification of personnel is essential.  This allows sufficient time for the assignment process to work, which can take several months in some cases.   Once you have established the need for an SME, you should clearly define that person's role in the project.  Clearly defined roles will:  Enable the elements of the project to be more accurately defined, such as project definition, tasks, milestones. Allow specialists to know their role as members of the project development team. Allow work to be scheduled with minimum conflict. Allow specialists to be selected who have the needed skills.  You may not be able to totally define the role of each specialist in the project until the training system design has been established.  AFH 36-2235 Volume 8 1 November 2002 51 Determine Training Needed to Meet Requirements Section C Introduction  Purpose  Who is responsible?  When should it be done?  How should it be done?   At this point in the process of getting ready to apply ISD to develop aircrew training, the training needed to meet the requirements should be determined.  It is important to identify the necessary training as early as possible in the planning stages so that adequate preparations can be made to design and develop training that will meet system requirements.   The purpose for determining the training needed to meet system requirements is to identify types of training, such as performance or complex performance.  Identification of needed training will enable the instructional developer to more accurately design an instructional system based on the effect the type of training will have on training system components such as equipment, facilities, time, media, methods, aircraft, ranges and ordnance.   Curriculum developers are responsible for determining the training that is needed to meet requirements of the training system.   Determining the needed training to meet requirements should be done during the initial planning stages of the ISD project.  The training needed should be determined prior to making preliminary decisions such as methods, media, and training alternatives.  Refer to AFH36-2235, Volume 3, for the schedule of planning activities.   During initial ISD project planning, you will not be able to determine all of the training needed to meet the system requirements.  As the training system design begins to take shape you will be better able to identify the types of training required.  However, you may not have the luxury of time due to weapon system bed-downs.  When this occurs, there are several ways to initially identify the training that may be needed.  AFH 36-2235 Volume 8 1 November 2002 52 How should it be done? (Continued)     Compare the training system you intend to design with a similar system that already exists.  This will help identify the types and levels of training that may be required.  Be sure to include lessons learned by the similar training system. Review existing information and technical data to determine what types of training may be required for the system. Check Air Force directives to determine if they require specific training to specified levels to meet established requirements. Use your experience and the experience of SMEs to identify possible training performance requirements. AFH 36-2235 Volume 8 1 November 2002 53 Determine Training System Requirements  Section D Introduction  Purpose  Who is responsible? When should it be done?  What should be determined?   Prior to applying the ISD process to develop aircrew training, some initial planning will need to be done to set the stage for the development project.  Part of the initial planning is to determine the requirements of the training system.  Since the requirements identified at this point are preliminary, they may need to be revised during the actual training development process.  Additional information on determining training system requirements, system training plans (STP) and training planning teams (TPT) is found in AFH 36-2235, Volume 3.   The purpose of identifying the training system is to establish the "framework" for building the training system.   Determining the training system requirements is the responsibility of the training planning team (TPT).   Determining the training system requirements should be done during the initial planning and preparation of the ISD project. You should have an idea of the system requirements before starting the analysis of ISD.   Determining training system requirements includes:  Assessing technology and methodology.  This includes looking at the technology that is available and the new methodology or procedures that have been developed in order to determine how they might be used to meet training system requirements. Making preliminary methods and media selection.  The preliminary selection of methods and media is a "best guess" which allows further planning and design activities to proceed, such as determining training alternatives. Examining training alternatives.  This includes looking at the anticipated training system and selecting alternative ways to provide training in the event the primary training delivery system is rendered ineffective.  Determining training alternatives is further explored in the next topic.  AFH 36-2235 Volume 8 1 November 2002 54 Select Training Alternatives Introduction  Purpose Who is responsible?  When should alternatives be selected?  What should be done?   During planning for the ISD project, curriculum developers initially examined possible training alternatives and in the analysis phase the need for alternative training strategies was determined.  At this point, the training alternatives are selected, if applicable.   The purpose of selecting a training alternative is to have an alternative method of providing training in case the primary delivery system is not available for training.  For example, if the primary delivery for a particular lesson is the flight simulator, and it is not available, an alternative way of providing the training may be the actual aircraft or a part-task trainer.   Curriculum developers and SMEs are responsible for selecting alternative methods and media for providing training in their courses.  Management within the training organization has the responsibility for ensuring that the training alternative selected is acceptable and meets the training goals of the organization.   Training alternatives should be selected after the objectives and tests have been developed and you know what it is you are going to teach.  Ideally, this activity is accomplished at the same time the methods and media are selected, since they may help you select training alternatives.   When selecting training alternatives, consider:  Objective(s) to be trained using the alternative Effectiveness of using the alternative to deliver training Resource requirements of the alternative Training environment in which the alternative will be used Cost-efficiency of the alternative   AFH 36-2235 Volume 8 1 November 2002 55 Introduction  Purpose of the plan  Who is responsible? When is a plan needed?  Section E Develop ISD Evaluation Plan  ISD is a total quality process, which ensures that the training development process is both effective and efficient and that quality products are produced in each phase of ISD, with special emphasis on the final training system product–the graduate.  The system must produce quality graduates who have the necessary skills, knowledge, and attitudes to perform in an operational environment.  To ensure quality, evaluation must begin in the planning stages and continue throughout the life cycle of the training system.  Updating the plan ensures that the most current and accurate information is used when evaluating the process and products of the training.   The purpose of the ISD evaluation plan is to provide an organized, structured plan for evaluating the ISD process and products to ensure that a quality training system is designed, developed, and implemented.   Curriculum developers or their supervisors are responsible for developing the ISD evaluation plan for the training system.  However, everyone in the training organization is responsible for quality.   An ISD evaluation plan should be developed for each new training system developed and for any existing system that is revised significantly enough to warrant a new evaluation plan.  This does not mean you cannot use an existing plan if it fits.  AFH 36-2235 Volume 8 1 November 2002 56 Format of the plan Evaluation plan job aid  The format of evaluation plans will be determined by the training organization developing the training.  The content of the plan is flexible and will depend to a great degree on the complexity of the training project being developed and the strategy used to evaluate the system.  In all cases, the plan should contain no more or less information than necessary to document the plan for evaluation.  The plan should cover:  Formative evaluation Summative evaluation Operational evaluation  The training organization can develop a separate evaluation plan for each phase of the evaluation process.  The evaluation plan(s) can be included as part of the training development plan.   A job aid for developing an evaluation plan is provided.  The example job aid includes some of the information that should be described in an evaluation plan.      Example of Evaluation Plan Job Aid Purpose of the evaluation. Type of evaluation (e.g., summative, formative, training effectiveness, training capabilities, cost-effectiveness, test items, course or materials review). Method of evaluation (e.g., empirical, analytic, internal, external). Types of information to be collected (e.g., opinion, observation, performance). Procedures for collecting information as follows:  Criteria to select size and composition of target population sample. Criteria for site selection. Methods for collecting information about student/target population sample participants. Criteria for selection of instructors. Methods for collecting information about instructor participants. Methods for preparing facilities and equipment prior to conducting evaluation. Methods for preparing students and instructors to participate in the evaluation. Methods for test administration. Methods for collecting student reactions to the training during the presentation. Methods for observing presentation of training. AFH 36-2235 Volume 8 1 November 2002  Evaluation plan job aid (Continued) Example of Evaluation Plan Job Aid (Continued) 57        Methods for collecting student and instructor comments at the conclusion of training. Methods for recording data. Methods for conducting interviews. Methods for participants to provide additional data for an extended period following completion of the actual evaluation. Methods for determining test validity and reliability. Procedures for data analysis as follows: Criteria for assessing performance. Criteria and procedures for validating the evaluation. Analytical treatment of data (e.g., statistical treatment). Criteria and procedures for estimating criticality of deficiencies. Procedures for reporting findings. Procedures for reporting conclusions. Procedures for reporting recommendations. Data collection instruments (e.g., tests, checklists, structured interviews, questionnaires, and job performance indicators). Schedule for data collection and performing the evaluation. Resource requirements (e.g., personnel, materials, special equipment, travel funds, facilities). Responsibility for testing and responsibility for conducting the evaluation. Roles and responsibilities of all personnel involved (command, students, evaluators, graduates, and supervisors of graduates). Identification of agencies and decision authorities who will receive the report. Listing of proposed evaluation sites. Scope of the evaluation (e.g., training objectives and critical standards).  AFH 36-2235 Volume 8 1 November 2002 58 Updating the evaluation plan  Tracing the quality process    To be an effective tool for managing the QI process, the evaluation plan must be continually updated with current and accurate information.  The plan should be updated at the end of each phase of the ISD process or when significant changes warrant a revision to the plan.  Information that may require updating in the evaluation plan includes items such as:  Procedures to be used in evaluating the design process and products Design products to be evaluated, such as objectives and test items Standards to be used to evaluate the design process and products New or revised milestones Changes in the validation procedures Changes in the evaluation procedures Changes in the data to be collected during internal or external evaluations Changes in the procedures for evaluating the development process Changes in the methods of collecting external evaluation data Revisions to the evaluation schedule, such as: When the design process and products will be evaluated Quality of products to be evaluated  Results of the design phase Rationale for changes made to the ISD evaluation plan Lessons learned during evaluation of the design process and products   Quality of the design process and the resulting products is essential to the training development process.  Therefore, you should document evaluation of the design phase to the point that the quality process can be traced throughout the entire ISD process, including the design phase.  However, document only what is needed.  Never document more than you need.  AFH 36-2235 Volume 8 1 November 2002 59 Section F Write Training Development Plan  One of the keys to developing an effective and efficient training system is to have a good training development plan.  In some training communities, the training development plan is also called an ISD management plan.  The plan should be broad in scope and should focus on the various development activities of the organizations involved in the development effort.    The purpose of the training development plan is to provide planning information and define the responsibilities for developing the training system.   Writing the training development plan is the responsibility of the curriculum developer.  However, the need to develop a plan is determined by management.   A training development plan is not required for each training development project.  The scope and complexity of the project will determine the need for a development plan.  Management should consider these two factors before making the decision to write a plan.  For example, an item that would influence the need for a plan would be more than one site conducting the same training.   The format of the plan is determined by the organization responsible for writing the plan.  Its content and detail are determined by the scope of the ISD project.  However, the format should be flexible to ensure that it meets the needs of each training development program and that it contains no more or less information than necessary to develop an effective and efficient training system.   A job aid for writing a development plan is provided. Introduction  Purpose  Who is responsible?  When is a plan needed?  Format of the plan  Training development plan job aid   AFH 36-2235 Volume 8 1 November 2002 60  Training development plan job aid (Continued)  Example of Training Development Plan Outline Job Aid A.  Description – Scope of the project B.  Operational Concept – Operating location, environment C.  Maintenance Concept – Levels and locations of maintenance D.  Personnel Concept – Types, numbers, utilization E.  Training Support Issues – Significant and unique issues     1.  Schedules 2.  Costs 3.  Supportability 4.  Issues needing resolutions/decisions for implementation 1.  Summary of training system 2.  Training strategy 3.  Constraints/alternatives/tradeoffs  I.  Approval/Authority Statement  II.  Program Summary           III.                           Instructional System Development Summary A.  Training Concept    B.  Target Population Description    C.  Curriculum Development     D.  Training Program Implementation            1.  Entry tasks, skills, knowledge 2.  Selection procedures/constraints 3.  Target population issues to be resolved 1.  Description of steps and courseware formats 2.  Resource requirements–personnel, equipment 3.  Development milestones 4.  Curriculum issues needing resolution 1.  Implementation milestones 2.  Location/time 3.  Resource requirements–personnel, funds, equipment, facilities 4.  Faculty composition and training requirement 5.  Curriculum administration and management a.  Types of courses, content, and duration  b.  Distribution procedures  c.  Media/training equipment required   d.  Types of courseware e.  Testing procedures  AFH 36-2235 Volume 8 1 November 2002 61  Training development plan job aid (Continued)  Example of Training Development Plan Outline Job Aid (Continued) f.  Student management procedures–records, scheduling  6.  Evaluation Plan            a.  ISD process and product evaluation milestones b.  Process and product evaluation procedures c.  Revision responsibilities and procedure e.  Validation milestones f.  Resource requirements–personnel, time g.  Validation procedures h.  Revision responsibilities and procedures i.   Summative evaluation milestones j.  Summative evaluation procedures k.  Resource requirements–personnel, funds l.  Revision responsibilities and procedures              E.  Curriculum Development and Implementation Issues to Resolve                             IV.  Training Planning Team A.  Team Composition  B.  Responsibilities  C.  Approval Authority   D.  Coordination and Distribution requirements   1.  Airspace 2.  Fuel 3.  Flying hours 4.  Personnel  5.  Facilities 6.  Equipment  Updating the development plan    Include, as a part of the training development plan, the scheduled reviews and revisions to the plan.  For the plan to be effective, it must be periodically updated to include information such as:  Changes to the milestones Addition or deletion of information New or revised resource requirements Changes in the constraints   AFH 36-2235 Volume 8 1 November 2002 62 Design Training Information Management System Section G Introduction  Purpose  Who is responsible?   Management of training information is essential for effective and cost-efficient training systems. It includes updating records, scheduling students, tracking equipment and meeting budgets.  Automated training information management systems enhance the training organization's ability to perform this essential task.   The purpose of automated training information management systems is to increase the effectiveness and cost-efficiency of the training system while managing the system in real time.  It can also provide instructors with the latest training/data materials available.  For example, automated systems can be used by:  Instructors to update student status Registrar to track student status Schedulers to schedule students and equipment Curriculum developers to update training materials Managers to manage resources Students to tell where they are in the program   Training information management system design normally falls on a project manager assigned the responsibility to oversee the procurement of the system.  However, training organization management and curriculum developers carry the responsibility for ensuring that the training information is managed effectively and cost-efficiently for their training.  In most cases, managers will not have an option as to whether an automated information management system is designed for use or not.  This decision is normally based on availability of a management system.  If a system does exist, it will likely be used by all involved in the training organization.  AFH 36-2235 Volume 8 1 November 2002 63 Design considerations  Off-the-shelf systems   As a curriculum developer, you may have the opportunity to help design or redesign a training information management system.  When you provide your input to project management, some of the issues to be considered are:  What is the cost? What are the hardware capabilities? Are there any special environmental requirements for the hardware? What are the software capabilities? Is the system documentation adequate? Who are the system users? Are the hardware and software user-friendly? Does this system have proven reliability? Is the system maintainable? Will the system interconnect/interface to existing systems? Is the system software compatible with other software? Does the hardware have expansion capability? Can the software be upgraded or modified easily? What training information will the system be able to manage?  What data is required for individuals to do their jobs? What are the sources of data and are they current? What data do all multiple users require? What data is unique to specific users? What are the required data formats? What system security is required?   There are commercial "off-the-shelf" software (cots) programs that can be purchased that will accomplish many, if not all, of your training information management requirements.  Purchase of these programs eliminates the need to design and develop an entire, single database system to manage the training information, thus saving considerable time and effort.  AFH 36-2235 Volume 8 1 November 2002 64 Chapter 4 ANALYSIS Overview Introduction Where are you in the process?  Once initial ISD project planning has been done and adequate preparations have been made, it is time to enter the analysis phase of ISD.  The nature and scope of each ISD project will be determined by the assessed training deficiency or training requirement.  The assessed training need or requirement will also determine the level of analysis that will be required for the project.  In order to visualize where you are in the training development process, an ISD model is depicted in Figure 9 with the analysis phase highlighted.  Figure 9 Analysis Phase  Objectives    The objectives of this chapter are to:  Examine the various types of analyses that are done in the analysis phase. Determine the requirement to update the ISD evaluation plan. Execute the requirement to update the ISD management plan.  AFH 36-2235 Volume 8 1 November 2002 65 Where to read about it  This chapter contains five sections.         Additional information  Section A B C D E Title  Conduct Task Analysis Conduct Target Population Analysis Analyze Resource Requirements / Constraints Update ISD Evaluation Plan Update Training Development Plan Page 66 76 78 83 86   For additional information on the various analyses, see:  AFMAN 36-2234, Instructional System Development, Analysis chapter. Carlisle, K. E. (1986). Analyzing Jobs and Tasks. Englewood Cliffs, New Jersey: Educational Technology Publications. Wolfe, P., Wetzel, M., Harris, G., Mazour, T. and Riplinger, J. (1991). Job Task Analysis: Guide to Good Practice. Englewood Cliffs, New Jersey: Educational Technology Publications. Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992). Instructional Design Strategies and Tactics. Englewood Cliffs, New Jersey: Educational Technology Publications.   AFH 36-2235 Volume 8 1 November 2002 66 Section A Conduct Task Analysis Introduction  What is task analysis?  Purpose   The first stage of the analysis process is conducting task analysis.  Since every job in the Air Force is made up of duties and each duty is made up of tasks, curriculum developers need to be experts in task analysis.  Failure to do a good task analysis could result in ineffective training.  A thorough task analysis will identify the tasks that need to be trained, under what conditions they should be taught, and the standard of performance that must be achieved.   In order to understand task analysis, a definition of a task is given first.  A task is an observable and measurable unit of work activity or operation, which forms a significant part of a job.  It constitutes a logical and necessary step in performance, and has a logical beginning and end.  It may also be defined as a series of actions leading to a terminal outcome, such as "starting engines on an F-15" or "performing an aborted take-off."  Now that a task has been defined, what is task analysis?  Task analysis is the process of breaking a task down to identify the:  Component steps of a task Sequence of those steps Conditions under which the task will be performed such as using the simulator, part-task trainer, or materials required to perform the task  Standard of performance that must be achieved   During task analysis, each task is examined to determine performance requirements, such as which tasks must be performed, the conditions under which they are performed, and the performance standard that must be achieved.  From this information evolves the task statement/list which is used to develop the course objectives.  AFH 36-2235 Volume 8 1 November 2002 67 Data collection    Data collection techniques     Ask the right question         The first stage of task analysis is collecting the data for analysis.  Several sources of data can be used in the task analysis process.  Some of the sources are shown following.    Data Sources Checklist AF Directives, Manuals, Handbooks Standard Operating Procedures MAJCOMs (user) System Program Office (SPO) Manufacturer's Manuals Contractor's Data SME   Numerous techniques can be used to collect data for task analysis.  Examples of some of the most common techniques are shown below.     Data Collection Techniques Review of job-related documents such as flight manuals, checklists Observation of actual job performance Interviews with SMEs Written questionnaire/survey Simulations of actual performance Analysis of similar systems   When collecting data, focus on getting the necessary information for task analysis.  This can be done by asking the right questions as you collect the data.  Examples are given below.  To Identify. . . Procedures, activities, steps Tools, materials Ask the Question. . . What does the person do first? Next? What is used to do the task?     AFH 36-2235 Volume 8 1 November 2002 68 Ask the right question (Continued) Weapon system, computer Cues    Identifying tasks  Task criteria    What is the task performed on? Work environment or conditions Performance standards  What is the standard of How do they know when to do what? Under what condition is the task performed? acceptable performance?   Identifying the tasks that need to be trained for is a critical part of task analysis.  When identifying tasks, you should:  Use all of the available data sources. Assume that not all tasks may have been identified in the initial task analysis.   Understanding task criteria will help identify tasks during task analysis.   Task Criteria A task is a group of related manual activities directed toward a goal. A task has a definite beginning and end. A task involves people interacting with equipment and media. A task is observable. A task is measurable. A task is an independent part of a duty. A task results in a meaningful product. A task includes a mixture of decisions, perceptions, and/or physical activities required of an individual. A task may be any size or degree of complexity.  AFH 36-2235 Volume 8 1 November 2002 69 Guidelines for task statements  Identifying tasks is a repeating process based on human activities that meet the above criteria.  Each time it is repeated, the results of the process are made more complete and accurate.  Continually repeating the process refines the task, which is continued until every task that makes up the duty is covered.  As the job tasks are identified, they are included in the listing in the form of a task statement.  To ensure that quality task statements are developed, several guidelines and standards are provided.       Standards for task statements Step 1 2 3   Description Arrange major duty responsibilities in order of typical performance.   Indicate which tasks are performed in a fixed order and which ones have no particular order.  Translate duty responsibilities (or human functions) into a list of task statements, composed of an action verb and an object.  Edit the list according to the standards provided below.    Use the following standards when developing task statements.  Clarity Use wording that is easily understood. Be precise so it means the same thing to each individual. Write separate, specific statements for each task.  Avoid com-bining vague items of skill, knowledge, or responsibility.  Completeness  Use abbreviations only after spelling out the term. To complete a form, include both form and title number, unless all that is needed is the general type form. AFH 36-2235 Volume 8 1 November 2002 70 Standards for task statements (Continued) statements Sample task      Identifying subtasks   Conciseness Be brief. Begin with a present-tense action word (subject "I" or "you" is understood). Indicate an object of the action to be performed. When there are several different techniques or methods of doing a task, adding condition phrases to the task statement may be necessary. Use terminology that is currently used on the job, such as "Perform an ASLAR departure."  Relevance Do not state a person's qualifications. Do not include items on receiving instruction, unless actual work is performed.  The exception is commands from lead A/C, radio calls, or others.   The following is a sample of satisfactory task statements.  Function or Duty Sample Task Statements Perform Air Refueling Activities  Given an F15, a tanker and the Perform Aircraft Recovery Activities Prepare for Mission  checklist, perform radio-silent air-to-air refueling. Given an F16C and the Dash One checklist, perform single ship recovery. Given all mission relevant information, perform basic flight premission planning IAW the Dash One   After the tasks have been identified, it is time to identify the subtasks.  Subtasks are work activities that, when combined, make up a task.  It should be noted that not all tasks are made up of subtasks.  There are several reasons for identifying subtasks:   AFH 36-2235 Volume 8 1 November 2002 71  Identifying subtasks (Continued)  For some task statements to be useful in training development, the curriculum developers must identify subtasks so the steps involved in performing tasks are known. Complex tasks can be analyzed more easily if they are broken into subtasks. Breaking a task into subtasks often helps sequence the training. Guidelines for identifying subtasks  Example of subtask      There are two basic guidelines for identifying subtasks:   Examine each task statement to see if it contains more than one group of activities that must be performed sequentially or independently. Review the complete list of subtasks for each task, to ensure that no subtasks overlap and that together they account for all performance required.   An example of subtasks that make up a task is given below. –  Fly the F16 Block 42 aircraft.   Example of Subtasks  Job Duty  –  Perform a LANTIRN air-to-surface mission. Task  –  Perform premission planning.    Subtasks       Determine the mission data.   Determine pretakeoff data.  Load the DTC. AFH 36-2235 Volume 8 1 November 2002 72 Sequentially and independently performed subtasks    Validate task information       When identifying subtasks, be aware that some subtasks must be performed sequentially while others can be performed independently.  Examples are provided below.  Sequentially Performed Subtasks Independently Performed Subtasks Task – Detect and react to aircraft malfunctions Subtasks  Abort takeoff. Perform emergency engine shutdown. Perform emergency egress.  Task – Takeoff and climb Subtasks  Accomplish single ship takeoff. Execute a climb and VFR departure. Execute a climb and instrument departure if in IFR conditions. Level aircraft at assigned altitude and accomplish cruise check.   Once the lists of tasks and subtasks have been analyzed, the results should be verified.  Several techniques can be used for verifying the lists, as shown in the examples below.   Verifying assembled task lists for an existing job Function  Technique Questionnaires SME interviews Task observations of perfor-mance expert Interviews Questionnaires Simulations SME assumptions Deriving a new task list for a new job  AFH 36-2235 Volume 8 1 November 2002 73 Selecting tasks to be trained  Documenting the tasks      After the task and subtask lists are verified, the tasks to be trained are selected.  When selecting the tasks to be trained, several things should be considered.  For example, when reviewing the list, consider:  Can most job incumbents perform the task without training? How often is the task trained on the job? Will job degradation occur if the task is not trained? Is the task critical to job or mission performance? Is it economical to train the task? Is there enough time to train the task adequately?  If people are trained to do another task, will they be able to transfer what they have learned to perform this task without additional training?  Will geographical, procedural, and environmental conditions make it unreasonable to train everyone to perform the task? Is the number of persons required to perform the task sufficient?  What is the percentage of those performing the task? Is the task difficult to learn?  After some experience? If individuals are trained to perform the task, will they remember how to perform the task when they get to the job?   A variety of formats can be used to document the task analysis.  One simple way to document task analysis is to use a Task Analysis Worksheet similar to the one provided next.  Regardless of the format that you use, the key is to document only the identity, training, and support information.  The goal is to ensure the quality of the analysis process and have adequate information to develop objectives in the next phase of ISD.    Check the data collection techniques used:      review of technical documents (list sources)      direct observation of job performance (list sources)      interview with subject matter experts (list sources)      group meetings (list sources )      survey questionnaire (list sources)   Task AFH 36-2235 Volume 8 1 November 2002 74  Documenting the tasks (Continued)   Example task analysis job aid                    Check the data collected: (see attachment #____)     procedures, activities, steps     tools, materials (see attachment #____) (see attachment #____)     equipment, weapon systems     cues (see attachment #____)     work environment/time constraints  (see attachment #____)     performance standards (see attachment #____)  If an item is not checked, explain why the data is not necessary.    A task analysis job aid can be in any form that meets your need. The main consideration is that you collect sufficient data in order to do a task analysis.  Following is an example of a task analysis worksheet that can be modified to meet your specific need.  Job Aid – Task Analysis Worksheet Job Duty Task Task Attribute Output (desired behavior) Standard of Performance (time, rate, percent)  Equipment (equipment required to perform task) Tools (tools required to perform task) Safety (safety considerations when performing task) Cue (what prompts performance) Conditions (weather, 2v Many, etc.) Location (where the task is performed–aircraft, OFT, etc?) References (documentation used in task performance such as checklist, Dash One) Human Interface (will others be involved in performing the task?)  Description           AFH 36-2235 Volume 8 1 November 2002 75 Example task analysis job aid (Continued)       Description   Also, the task analysis worksheet should identify each subtask and its attributes that make up a task.  An example is shown below.  Subtask Attribute Skill (skill that is necessary to perform the task) Knowledge (facts, concepts, principles that are required to perform the task) Attitude (interest, motivation  necessary to perform the task)    AFH 36-2235 Volume 8 1 November 2002 76 Section B Conduct Target Population Analysis  In order to design effective and efficient training, the curriculum developer should know for whom the training is being designed.  It is essential that an accurate description of the target population be defined so the proper decisions can be made about course design and the resources required to support the design.  This analysis further defines the resource decisions made during the initial planning stages for the ISD project.   A target population analysis is the analysis of the persons for whom the training is designed in order to determine the range of their aptitudes, experiences, and skills and knowledge about the subject matter.   Curriculum developers are responsible for conducting the target population analysis, when necessary.   The target population analysis should be conducted prior to starting to design the training system.   Target population analysis should be conducted to gather information so proper decisions can be made about:  Training requirements Course content Course length Method and type of media to be used Equipment and facilities Training strategies to be employed   The target population analysis should identify the probable range of aptitudes, experience, skills and knowledge about the subject matter that individuals in the group possess.  Introduction  Definition  Who is responsible? When should it be done? Why should it be done?  What should be the results?  AFH 36-2235 Volume 8 1 November 2002 77  One of the simplest ways to conduct the analysis is to conduct interviews and research all available information about the target population.  The goal of the interviews and research is to find answers to questions that address:  Job/position level of the students Previous background of the students Courses they may have completed relating to the job Job experience relating to the training Skills and knowledge already possessed by the students Any other related qualifications How to conduct an analysis    AFH 36-2235 Volume 8 1 November 2002 78 Analyze Resource Requirements / Constraints Section C Introduction  What are resources?  Why analyze resources?  Who is responsible?   Resources are the critical factor in every training system.  During the initial planning stages for the ISD project, one of the first concerns is identifying resources required for the training system.  This same concern for resources continues into the analysis phase of training development as well.  It is unlikely in this phase that all of the needed resources can be identified.  However, identification of long-lead-time resources, such as training equipment and facilities, should be identified in order to allow sufficient time to secure the resource.  If you are facing a resource constraint, you will need time to work out alternatives.   Training system resources can be categorized into five major areas:  Equipment, such as aircraft, simulators, part-task trainers Facilities, such as classrooms, ranges, air space Funds for equipment, supplies Personnel, such as instructors, curriculum developers, SMEs (loadmasters, pilots, flight engineers, etc.) Time, such as flying hours, simulator time, training development time   Resources are analyzed in order to identify:  Type of resources required, such as personnel and equipment Quantity of resources required, such as number of instructors, classrooms, trainers When the resources are needed in order to meet the training delivery date Total cost of resources   Conducting the resource analysis is the responsibility of the curriculum developer.  However, securing the needed resources is the responsibility of organizational management, with the assistance of the organization that provides the resources.  AFH 36-2235 Volume 8 1 November 2002 79 Before you analyze the resource requirements   Before beginning to analyze the resources required for the training system, consider several things that may help to conduct a better analysis.  Equipment Most major system-specific or unique equipment items must be identified as early as possible in the ISD process, since they must normally undergo a cycle of design, development, production and testing, prior to delivery. Failure to identify training and support equipment requirements early may result in delay of training implementation. Equipment should not be placed on order or procured until the objectives have been tentatively set for the course, ensuring that there is a valid requirement for the equipment. When selecting equipment for the training system, consider these factors: Suitability or appropriateness Usability Reliability Maintainability Cost  Facilities  Funding Identify facility requirements as early as possible since the time required to get funds is normally long; also, the time to get approval and build new facilities or modify existing facilities can be considerable. Budgets must often be prepared or revised prior to getting the necessary funds appropriated to: Procure equipment. Buy fuel. Construct or modify facilities. Pay personnel cost. Budgets are normally submitted and approved long before money is actually spent; thus, organizational managers and curriculum developers must determine, as precisely as possible, what resources will be required for the training system.  AFH 36-2235 Volume 8 1 November 2002 80 Before you analyze the resource requirements (Continued)  Analyzing resource requirements         Personnel  Time Lead- time for additional personnel such as curriculum developers and instructors can be lengthy, since it involves the budget and personnel authorizations. When requesting personnel, such as curriculum developers and instructors, allow sufficient time to properly train them for their assigned duties. Identify additional support personnel, such as typists or hardware fabricators, if applicable. If possible, allow sufficient lead time to: Obtain the necessary equipment. Build new or modify existing facilities. Get personnel authorizations approved and personnel properly trained. Secure the required funding. Design, develop, and provide an effective and efficient training system. Determine the flying hours that will be needed in the aircraft and simulator.   In the analysis phase, all of the resources will need to be analyzed in order to identify and estimate the resource requirements for the training system.  To help analyze resources, a simple job aid can be developed using questions.  An example of a job aid is provided below.  Sample Questions What types of equipment will be required (training, support)? What equipment will be required (simulators, part-task trainers, computers)? Where and how will the equipment be obtained? How will the equipment be used in the course? What quantities will be required? What is the life cycle of the equipment? If faced with an equipment constraint, can alternative equipment be used?  If so, what equipment? Resource Equipment     AFH 36-2235 Volume 8 1 November 2002 81 Resource  Facilities  Funds  Personnel  Time Analyzing resource requirements (Continued)       What about resource constraints?  Sample Questions What types of facilities will be required (classrooms, ramp areas)? How much space will be required? Are facilities available on the base? If facilities exist on the base, will they require modifications? Does the facility meet environmental re-quirements, if applicable? What are the initial costs of equipment, facilities and personnel? Will the training development be contracted for? What are the recurring costs associated with operating the training system? Are there TDY costs associated with training? How many curriculum developers will be required to meet the training delivery date? Will instructors be needed?  If so, how many? Will the training system require additional overhead personnel? How many flying hours are needed? How much simulator time is needed? What is the training delivery date? How much time will be required to develop the training? Is there any equipment lead-time requirements?  If so, how much? If new or modified facilities are required, how long will it take? What is the estimated course length?  Finding answers to questions such as these will help analyze resource requirements for the training system.   Everyone in the training organization should conserve resources.  Since resources are usually in short supply, you may not be able to obtain resources that are required for the training system.  Regardless of the situation, the constraint needs to be resolved or an alternative solution selected.  The following table provides several examples of actions/alternatives that might be considered when faced with a resource constraint.  AFH 36-2235 Volume 8 1 November 2002 82  What about resource constraints? (Continued)    Constraint Equipment Facilities Funding Personnel Time      Updating resource requirements  Action/Alternative Borrow equipment belonging to other organizations or MAJCOMs. Share equipment with other organizations or MAJCOMs. Use prototype equipment. Use a trainer or simulator rather than the actual equipment. Increase the number of students using a single piece of equipment. Operate on more than one shift. Increase class intervals. Use temporary facilities. Use other organization or MAJCOM facilities. Operate on more than one shift. Decrease group size. Increase class intervals. Reduce resource requirements. Seek alternative funding sources. Contract out if funding available. Reduce the number of graduates produced. Borrow curriculum developers or instructors from other organization or MAJCOMs. Reduce the multiple instructor requirements. Borrow additional personnel, such as instructors, from other organizations or MAJCOMs. Reduce course length. Select alternative methods or media.   In the initial planning stages, some of the required resources were identified.  However, during the initial planning it is impossible to know all the resources that will be needed.  This is why resources should be analyzed during the analysis phase.  If the analysis is done properly in this phase, a good estimate of the types and quantities of resources can be identified.  But this is not the end of resource analysis.  As training development progresses through the design and development phases of ISD, resource requirements will continually need to be redefined and fine-tuned.  Updating the resource requirements will help to ensure that adequate resources are available to support the training system.  AFH 36-2235 Volume 8 1 November 2002 83 Section D Update ISD Evaluation Plan Introduction  Why update the evaluation plan?  Who is responsible?  What should be updated?  Caution – Document only as needed. Do not document just to fill a case file.  During the initial planning for the ISD project, an ISD evaluation plan was developed.  The purpose of the plan was to ensure the quality of the process and products of each phase of ISD.  Thus, the evaluation plan becomes the "metric" or standard for continuous evaluation of the training system.  To ensure that the plan remains an effective evaluation tool throughout the "life cycle" of the project, it will probably need to be periodically updated.   The nature and scope of every ISD project is different; thus, every ISD evaluation plan will be different.  These differences will make it difficult, if not impossible, to develop an ISD evaluation plan that will be "on target" and effective throughout the life cycle of the training system.  Therefore, as the analysis phase is completed, it is likely the evaluation plan will need to be updated to reflect the results of the evaluation efforts during the analysis phase.  Updating the plan will ensure that it is current and accurate and ready to be used to evaluate the process and the products of the design phase.   Curriculum developers are responsible for ensuring that the evaluation plan is updated at the end of the analysis phase, as applicable.  However, everyone involved in training development is responsible for the quality of the ISD process and products.   It is essential that the ISD evaluation plan be periodically updated if it is to be an effective tool in evaluating the ISD process and products.  Updating the ISD evaluation plan may include, but not be limited to: Revisions to the evaluation plan for the analysis phase, including: Analysis products to be evaluated, task list, and objective hierarchy  AFH 36-2235 Volume 8 1 November 2002 84 What should be updated? (Continued)     Tracing the quality process  Quality standards to be used to evaluate analysis phase products Procedures to be used in evaluating the analysis process and products Revisions to the evaluation schedule, such as: When the process and products will be evaluated Quantity of products to be evaluated Documenting the analysis phase evaluation results Rationale for changes made to the ISD evaluation plan Lessons learned in evaluating the analysis process and products   Quality of the analysis process and the resulting products is very important to the training development process.  Therefore, the curriculum developer should document the evaluation that was conducted during the analysis phase to ensure that the quality process can be traced throughout the entire ISD process, including the analysis phase.   AFH 36-2235 Volume 8 1 November 2002 85 Section E Update Training Development Plan Introduction  Why update development plans?  Who is responsible?  What should you update?   Even before the training development process entered the analysis phase, the curriculum developer wrote a training development plan to serve as a "roadmap" for managing the training development process.  It is essential that a plan be developed so that you will have a good idea of where you want to go and how you intend to get there.  Because it is essential to write the training development plan in the initial stages of project planning, it will probably need to be periodically updated to ensure that it remains an effective management and development tool.   The training development plan should be updated at the end of the analysis phase, as applicable, to reflect any changes in the training development process resulting from the analysis phase.  If the training development plan is not updated, it will become ineffective as a management and training development tool.   Curriculum developers are responsible for updating the training development plan.  The developer should keep the plan updated with the most current and accurate information resulting from the analysis phase.   After the analysis phase has been completed, the training development plan should be updated with new or revised information resulting from the analysis phase, such as:  Changes to the overall training development strategy New or revised milestones Refinements to project definition Changes in resource constraints Revisions to resource requirements Addition or deletion of tasking  Items or areas that will consistently change, e.g., milestones, can be placed in an appendix.  Changes would then be limited to the back part of your plan.  AFH 36-2235 Volume 8 1 November 2002 86 Chapter 5 DESIGN Overview Introduction  Where are you in the process?   At this point, the analysis phase has been completed and you are ready to enter the design phase of ISD.  During this phase, types of learning and training methods will be reviewed before the framework for the training will be built by developing objectives and designing the training necessary to achieve the objectives.  The design selected will play a key role in determining the effectiveness and efficiency of the training that will be developed.  Also in this phase, as well as throughout the entire ISD process, quality improvements in the process and products will be a major concern.  Much work has been contributed toward the cognitive approaches to instruction.  Curriculum designers are encouraged to review the research in this area (see AFHR PR 87-34).   An ISD model, with the design phase highlighted, is depicted in Figure 10 to help visualize the ISD process.  Figure 10 Design Phase   AFH 36-2235 Objectives  Where to read about it          Volume 8 1 November 2002 87  The objectives of this chapter are to:  Review types of learning and training methods Examine the elements of training design. Determine the process of designing training. Create the design phase planning activities. Prepare the quality improvement functions in the design phase.   This chapter contains seven sections. Section A B C D E F G  Title  Review Types of Learning Select Training Methods Develop Objectives Develop Tests Review Existing Materials Design Training Update Training Development Plan Page 88 94 98 113 128 131 144  AFH 36-2235 Volume 8 1 November 2002 88 Section A Review Types of Learning Introduction  Types of learning        Intellectual skills Title  In this section, learning is addressed in terms of the individual types of learning and the integration of human activities.  Categories of learning types establish a framework for how learning takes place.  In real life, these types of learning are integrated.  This integration is discussed in terms of schemas, enterprise theory and metaskills.   Learning theorists have categorized human activity into types of learned behavior.  Gagné's (1985) categories of learning types are the most inclusive. They include intellectual skills, verbal information, cognitive strategies, motor skills, and attitudes. Gagné suggests that each type of learning requires different internal conditions for processing to occur.  Internal conditions may be cued or prompted by external conditions present in the learning environment.  The following five areas will be covered:   Intellectual skills Verbal information Cognitive strategies Motor skills Attitudes and motivation   Intellectual skills are the foundation for all higher learning.  They consist of discrimination, concepts and rule using.  Cognitive strategies are often called a higher-order type of intellectual skill.  For the purpose of instructional design intellectual skills needs to be subdivided.  Concepts (kinds-of) are different from principles (what-happens) which are different from procedures (how-to).   Intellectual skills are hierarchical in nature.  In order to learn a higher-order skill, the learner should possess the prerequisites.  To learn a rule or principle, the learner should understand the component concepts and the relationships among the concepts.   Page 88 89 89 90 90 AFH 36-2235 Volume 8 1 November 2002 89 Intellectual skills (Continued)    Discriminations.  Discriminations are skills related to seeing differences between stimuli.  Most adult problems in discrimination come from physical disabilities like color blindness, hearing loss, or some injury that affects sensory perception.  Concrete Concepts.  Concrete concepts are skills related to categorizing physical objects into one or more classes based on their physical attributes.  Identifying resistors from among other electrical components is an example of concrete concept learning. Defined Concepts.  Defined concepts are skills related to classifying symbolic objects into one or more classes based on a definition.  The definition is actually a rule for classification.  For example, classifying a verbal statement from an officer as a command is an example of a learned defined concept. Verbal information Cognitive strategies Rule Learning.  Rule learning skills relate to applying principles or procedures to solve problems. Problem solving is the ability to recall relevant rules and use them to solve a novel problem.  The product of problem solving is not only a solution to the problem, but also learning a new rule or procedure to be used if a similar situation should arise in the future.   Verbal information is the learning of names and labels that can be verbalized.  It is also called declarative knowledge.  Verbal information learning requires some basic language skills.  In addition, verbal information is more readily retained when it is learned within a larger context of meaningful information.   The basic premise of an information-processing model is that individuals mentally process their environment.  This process consists of a number of stages in which the stimuli become information, which is given meaning by previous knowledge and current expectations.  Cognitive strategies are employed to maintain the knowledge in short-term memory and translate it to a structure that enters long-term memory as a type of knowledge in the form of propositions, productions or schemas.  AFH 36-2235 Volume 8 1 November 2002 90 Cognitive strategies (Continued) Motor skills Attitudes and motivation  Cognitive strategies are thought of as executive control mechanisms for learning.  Monitoring the use of strategies is "metacognition."  Cognitive strategies used in metacognition are called metacognitive strategies.  There are different types of cognitive strategies such as clustering items into similar groups to reduce memory load, reading strategies to increase comprehension, and others.  Good learners have a variety of strategies they can use to process new information.   Motor skills are learned behaviors that involve the smooth coordinated use of muscles.  Motor skills most often involve a sequence of activities that may be described verbally as an "executive subroutine."  This verbal information is learned to provide guidance for learning the execution of the motor skill.  When the learner has acquired the motor skill, the verbal routine is no longer needed and the skill is performed in a smooth and continuous manner.  Motor skills may be learned by modeling, as when a coach shows a student how to swing a golf club.  Motor skills require practice and kinesthetic (natural) feedback.  Verbal feedback from an observer also helps the learner make corrections in performance.  Much of the instruction is aimed at getting the student to recognize the feel of the motor performance when it is executed correctly.   The acquiring of particular attitudes may require the prior learning of intellectual skills or particular sets of information.  For example, if a positive attitude toward safety is to be acquired, the learner should have (1) intellectual skills (concepts and procedures) associated with safety, and (2) a variety of verbal information about the advantages of following safety procedures or the consequences of not following them.  Attitudes have mutually supportive relationships. An individual generally tries to maintain consistency with regard to choice    behaviors. However, attitudes are based on perceptions of reality. These perceptions are colored by misinformation or critical experiences.  Attitudes are learned by observing others and viewing the consequences of their behavior.  This type of learning (vicarious) is a distinct principle of social learning.  External conditions for learning attitudes include a human model.  Experiences play a major role in the formulation of attitudes.  Motivation plays a significant role in learning.  Keller (1987) has developed a general model integrating the various sources of motivation for learning.  He calls it the ARCS model, an acronym for the four sets of conditions that should be met to have a motivated learner:  A for attention.   Attention involves grabbing the learner's interest at the beginning of instruction and maintaining that interest throughout the lesson and course. Relevance is the personal significance and value to the learner of mastering the learning objectives. Confidence relates to the learner's expectancy of success. Satisfaction comes from achieving performance goals.  R for relevance.   C for confidence.    S for satisfaction.  AFH 36-2235 Volume 8 1 November 2002 91 Attitudes and motivation (Continued)   human activities   Integration of In real life, the types of learning are integrated.  This integration is discussed in terms of schemas, enterprise theory and metaskills  AFH 36-2235 Schemas Enterprise theory   Metaskills Volume 8 1 November 2002 92  Intellectual skills should be integrated into existing knowledge to be remembered and recalled.  They are thought to be stored as schemas and as part of pre-positional networks.  A schema is an organization of memory elements (propositions, images, and attitudes) representing a large set of meaningful information pertaining to a general concept.  The concept may be of an object, such as a jet fighter, weapon, or officer.  Or it may be an event, such as a preflight check or preventive maintenance procedure.  Regardless of type, schemas contain information on certain well-understood features of the object or event.  The learner fills in these features, called slots, when encountering new information that relates to the schema.  Schemas are acquired through experience and may be the greatest benefit of apprenticeships.  Recent theory proposes that intellectual skills are "situated."  That means their utility is in a large part a function of how they are learned.  In order that they do not become "inert knowledge," they should be learned and practiced within a broader context.  Gagné and Merrill (1990) proposed a method to identify learning goals that requires an integration of multiple objectives.  They proposed that such an integration of multiple objectives be conceived in terms of the pursuit of a comprehensive purpose in which the learner is engaged, called enterprise.  An enterprise is a purposeful, planned activity that may depend for its execution on some combination of verbal information, intellectual skills, and cognitive strategies, all related by their involvement in the common goal.  A task for the instructional developer is to identify the goal of a targeted enterprise along with its component skills, knowledge, and attitudes, and then to design instruction that enables the student to acquire the capability of achieving this integrated outcome.   The metaskill concept (Spears, 1983) refers to the complex skills of adapting, monitoring, and correcting the use of individual skills in complex performances that integrate cognitive, perceptual, and motor processes.  Proficiency in metaskills depends on the number of individual skills practiced.  Plateaus in performance are related to the intervals required for students to put together new sets of metaskills.  AFH 36-2235 Volume 8 1 November 2002 93  Processes involved include:  Learning Process Learning Phase Gaining organic knowledge of the effects of actions on overall goals. Organizing knowledge hierarchically to include cause-effect rules. Developing monitoring procedures that incorporate outcome expectancies.   Events of instruction are a set of communications embedded in instructional activities.  They serve the function of activating internal events of information processing.  The learning process is shown in the following table.   Expectancy Perception Working Storage Encoding Storage Retrieval Validation of Understanding Transfer Valuing   Motivation  Apprehending  Acquisition Processing  Retention  Recall  Feedback  Generalization  Personalizing   Metaskills (Continued) Internal events of information processing              AFH 36-2235 Volume 8 1 November 2002 94 Section B Select Training Methods Introduction  Definition  Training methods  Selection considerations  Constraints  Selecting the training method is an important activity in the training design process.  The method selected will impact effectiveness and efficiency of the training system.   A training method is the process used to deliver the instructional content and to provide guidance for students to retain the skills and knowledge imparted.  Examples include lecture, demonstration, and self-study.   There are many training methods that can be selected, as described in the following material.   There are several factors to be considered when selecting training methods for the training system.  These factors can be categorized into three major areas:  constraints, cost-efficiency, and training considerations.   Constraints include:  Geographical spread of target population–If the target population is widely spread it may not be feasible to bring students to a central location for training.  If this is the case, classroom training may not be the appropriate training method.  Consider other training methods such as OJT, self-study. Student availability–If there is an insufficient flow of students due to lack of resource constraints, it is unlikely that class-room training will be appropriate.  OJT or self-study may be a better method.  Also, consider using CBT if there are large numbers of students to be trained over a long period of time. Instructors/trainers availability–If instructors/trainers are not available, consider using other training methods such as self-study. Facilities and equipment availability–If there is a lack of adequate facilities and equipment to handle student flow, consider OJT or self-study.  AFH 36-2235 Volume 8 1 November 2002 95 Constraints (Continued) Presentation methods     Development time–Training methods such as CBT require considerable development time.  If there is limited time or only a few students to be trained, consider other training methods such as self-study or OJT. Safety–When performance of the objective could cause loss of life, bodily harm, or loss of equipment, consider other methods such as CBT. Non-availability of ICW development specialist–If neither the personnel nor the funds to acquire them are available, you may have to consider other training methods, such as lecture combined with video. The following form includes points for different training methods.  TRAINING METHODS  Presentation methods include: Lecture – A formal or semiformal oral presentation of information by a single individual; facts, concepts, problems, relationships, rules or principles presented orally either directly (as by classroom instructor) or indirectly (as by video).Demonstration – Presentation or portrayal of a sequence of events to show a procedure, technique, or operation; frequently combines an oral explanation with the operation or handling of systems equipment or material.  May be presented directly (as by a classroom instructor) or indirectly (as by video). Exhibit – A visual or print display used to present information; for example, actual equipment, models, mockups, graphic materials, displays, chalkboard, or projected images. Indirect Discourse – Verbal interaction among two or more individuals which is heard by the student; may be a dramatization, such as role playing, or a dialogue between panel members, or a teaching interview (a question and answer session between instructor and visiting "expert"). Assigned Reading – Printed verbal materials such as books, periodicals, manuals, or handouts.  Reading may be course-assigned or self-assigned. Teaching Interview – Question and answer session between the instructor and visiting "expert" following a highly structured plan.   AFH 36-2235 Volume 8 1 November 2002 96   TRAINING METHODS (Continued)  Student verbal instruction methods include: Questioning – An instructor and/or courseware controlled interactive process used to emphasize a point, stimulate thinking, keep students alert, check understanding, or review material.  Questioning may be direct, as by a classroom instructor, or may be designed into a film or television presentation. Programmed Questioning – An instructor and/or courseware controlled interactive process used to systematically demand a sequence of appropriate student responses; may be used directly (as by an instructor in a classroom) or indirectly (as by programmed booklets or computers). Student Query – The provision by which students are given the opportunity to search for information, as by questioning a classroom instructor, tutor, coach, or an appropriately programmed computer. Seminar – A peer-controlled group interactive process in which task- or objective-related information and experience are evoked from the students.  Questions may be used to evoke student contributions, but the seminar is distinguished from questioning. Discussion – An instructor-controlled interactive process of sharing information and experiences related to achieving a training objective.   Knowledge application methods include: Performance – Student interactions with things, data, or persons, as is necessary to attain training objectives; includes all forms of simulation (for example, games and interaction with hardware simulators) and interaction with actual equipment or job materials (for example, forms).  Performance may be supervised by classroom instructor, tutor, coach, or peer to provide needed feedback.  Case Study – A carefully designed description of a problem situation, written specifically to provoke systematic analysis and discussion. Student verbal instruction methods Knowledge application methods    AFH 36-2235 Volume 8 1 November 2002 97 Cost-efficiency Training considerations  Cost-efficiency points include:  Trained Personnel Requirements (TPR) – Expensive delivery systems such as CBT may be justified if the TPR is large and the training is required over a long period of time.  A training method you would not likely use to train a large TPR is OJT. Content stability – If training content requires frequent updates, methods such as CBT are less suitable than classroom, OJT, or self-study. Amount of practice required – If there is a lot of practice required you may want to consider ICW as a training method, since practice time is only limited by the availability of the student and the equipment.  In the classroom or OJT, however, an instructor or trainer is required, which makes practice time costly.   Training considerations include:  Task criticality – If task performance is critical, consider formal classroom training or OJT.  Self-study would be a questionable training method for training critical tasks. Learning difficulty – A task that is difficult to learn should be taught using the classroom or OJT method, or a part-task trainer may be appropriate. Training fidelity – If the training fidelity requirement is high, consider selecting a method that uses the actual equipment to teach the process or procedures. Interaction level.  If the learning process requires a great deal of interaction, OJT is probably the best, since it is highly interactive.  If the group size is small, classroom training can provide moderate interaction.  You may not want to use self-study if the learning process requires high interactivity.   AFH 36-2235 Volume 8 1 November 2002 98 Section C Develop Objectives Introduction  Definition  Purpose   Other terms for objectives   One of the products of the analysis phase is a listing of tasks requiring training. In the design phase of ISD, this task list will be used to develop the objectives for the course.  Objectives serve as the "foundation" of the course on which all training is built.  When developing the objectives, remember that objectives should be stated in terms of what the students must be able to do at the end of training.   An objective is a precise statement of the learned capability–skills, knowledge or attitudes (SKA)–a student is expected to be able to demonstrate, the condition under which the SKA is to be exhibited, and the minimum standard of acceptable performance.   Objectives serve several purposes.  Examples are given below.   For Curriculum Developers   For Students  Serve as the foundation for building training. Provide a basis for test development. Allow for selection of the most appropriate training strategies. Direct attention to the important content. Communicate standard of performance expected following the training. Serve as a self-check for progress.    Objectives have been given many names, such as:   Performance objectives Behavioral objectives Instructional objectives Training objectives Criterion-referenced objectives (CROs)  AFH 36-2235 Volume 8 1 November 2002 99 Parts of an objective  Levels of objectives  Examples of objectives   An objective has three parts:  Condition Learned Capability Standard  Each part of the objective is discussed later in this section under "Characteristics of Objectives."   Many terms have been used to describe the levels of objectives.  The following terms are often used to distinguish the levels.  Terminal Primary Enabling Secondary Supporting Subordinate  This handbook will only refer to the different levels in order to show the hierarchical relationship of objectives.   The following are some examples of objectives.  Given a diagram of the Primary Alerting System (PAS) panel, and a list of functions and incomplete statements relating to the PAS panel operation, the trainee will perform the following to 85% accuracy, correctable to 100%. Recall the function of each control and indication by matching the proper function to the control or indicator in the diagram. Recall each PAS operation by completing each statement.   Given a diagram of the Flight Director system display (ADI and HSI), identify the parts as they are labeled in Section I of the Dash One with 85% accuracy correctable to 100%. AFH 36-2235 Volume 8 1 November 2002 100 Where to read about it       Additional information    This section covers four topics. Topic  Characteristics of Objectives Guidelines for Developing Objectives Hierarchy of Objectives Prioritizing, Clustering, and Sequencing Objectives   For additional information on objectives, see:  Page 101 106 107 110 AFMAN 36-2234, Instructional System Development, Design chapter. Davies, I. K. (1976). Objectives in Curriculum Design. London: McGraw Hill Co. Kibler, R. J. (1981). Objectives for Instruction. Boston: Allyn and Bacon. Mager, R. F. (1962). Preparing Instructional Objectives  (2nd Ed.). Belmont, California: Fearon Publishers.  AFH 36-2235 Volume 8 1 November 2002 101 Introduction   What is contained in this topic      Characteristics of Objectives  Before starting to develop objectives, curriculum developers should be able to recognize each part of an objective, given examples.  Familiarity with the parts of an objective will ensure that better objectives are developed, which produces better training.   This topic is divided into three parts.    Parts of an Objective Condition  Learned Capability Standard   Page 102 103 104 AFH 36-2235 Volume 8 1 November 2002 102 Introduction Definition   Examples    Condition  Curriculum developers should thoroughly understand the condition part of an objective in order to develop more effective objectives.  When developing objectives, use the actual condi-tions under which the job will be performed, if possible.    A condition identifies the situation under which a student is expected to demonstrate a behavior as a result of the training.  A properly prepared objective clearly states the limits and/or conditions of student performance.  A condition may describe:  If the student can use the checklist If the student is allowed to use the flight manual What equipment students are allowed to use If students can use notes they have taken during the training   The following are some examples of variable conditions, which can very from situation to situation and not affect the type of learning involved.  "Given an AN/TPQ-43 radar set, ..." "Using a diagram of the Flight Director system, ..." "Given incomplete statements on Offensive Radar System operation, ..." "Without reference, ..." "Under conditions of total darkness, ..." "Given a checklist, and the Dash One, ..."  AFH 36-2235 Volume 8 1 November 2002 103 Introduction  Description  Action verbs     Examples  Learned Capability  The capability part of the objective states what the students will do to demonstrate that they have learned a specific skill, knowledge or attitude.  The capability must be written in measurable, observable terms so that student performance can be objectively evaluated.   A capability is defined as an observable, measurable performance that the student should demonstrate.   When writing the capability part of an objective, use action verbs to reduce ambiguity. Action verbs are observable and measurable, while ambiguous ones are not. The following table provides some examples.    Action Verbs   Ambiguous Verbs Perform Fly List Understand Know Learn   Several examples of learned capability statements are:  "… locate Electrical Multiplex controls …" "… recall the function of each control …" "… perform a built-in-test on HUD …" "… fly a low-approach …"  AFH 36-2235 Volume 8 1 November 2002 104 Introduction  Definition  Types of standards          Standard  The final part of the objective to be discussed is the standard.  The student's performance will result in an output; the quantity or quality of which is the standard of performance.   A standard defines the criteria for acceptable performance by the student. It is stated in terms of completeness, accuracy requirements, time constraints, performance rates, or qualitative requirements.  It identifies the proficiency the students must achieve when they perform the behavior under the specified conditions.  Without a standard, it is impossible to determine when the students have achieved the objective.   Standards can be classified in one of the following types.  Types of Standards Standard operating procedure No error Minimum acceptable level of performance  Time requirements   Rate of production Qualitative requirements Amount of supervision  Example "… will comply with AF and local regulations." "… egress from the aircraft." "… compute the distance to the nearest five-mile increment." "… take-off within five minutes.""… run built-in test on the INS three times." "… complete an S maneuver smoothly." "… identifies and corrects errors without assistance (ACC flying standards)." AFH 36-2235 Volume 8 1 November 2002 105 Example   Some examples of standards are:  "… list the procedures for aborting a take-off" (without error implied). "… within three miles." "… with no more than 5% error." "… IAW the Dash One." "… in no more than two minutes."   AFH 36-2235 Volume 8 1 November 2002 106 Guidelines for Developing Objectives Introduction  Guidelines   To this point, background information and the three parts of the objective have been discussed.  The next stage is to actually develop objectives.  The task list, which was developed in the previous phase, will be required in this effort.  Using the information on the task list, what has been learned about objectives to this point, and the following guidelines, effective objectives can be developed.   Use the guidelines below when developing objectives.  Use the task list developed during the analysis phase. Analyze each task or knowledge item on the task list to determine the number of objectives for each item. Document each objective on a worksheet.  Analyze each objective to determine the skills, knowledge, and attitudes necessary to support the objective. Use the supporting skills, knowledge, and attitudes to develop sub-objective(s). Document any sub-objective(s) on the worksheet of the objec-tive they support.  AFH 36-2235 Volume 8 1 November 2002 107 Hierarchy of Objectives  Each task is made up of steps or procedures.  Students must learn each of these steps or procedures before they can perform the particular task.  Thus, objectives need to be specified for each step or procedure, in addition to the task itself.  These different levels of objectives can be structured into an objective map that depicts the relationship of the various objectives and their sequence in the course.  This map is called objective or learning hierarchy.   The purpose of an objective hierarchy is to depict the relationship of objectives so that the most effective and efficient training sequence can be developed.   Objectives can be classified into two categories.  These categories are shown below.  Category Terminal Enabling   Description An objective the learners will be expected to accomplish when they have completed the in-struction.  It may contain several subordinate objectives. An objective that the student must attain in order to accom-plish a terminal objective. Other Names Primary Main End  Secondary Subordinate Supporting    Using Figure 11 as an example, the Terminal Objective is to "perform permission planning."  All other objectives in the figure, such as "determine takeoff data" and "perform DTC loading," are Enabling Objectives.  Introduction  Purpose  Classification of objectives     Examples of terminal and enabling objectives     AFH 36-2235 Volume 8 1 November 2002 108 Examples of task/objective hierarchy   Figure 11 and the following table shows examples of a task hierarchy and the task/objective hierarchy statements, which are derived from the task hierarchy itself.  Figure 11 Task Hierarchy 1.1Performpremissionplanning1.1.1Determinemissiondata1.1.1.2Determinetakeoff data1.1.1.1Determinepretakeoffdata1.1.1.1.2Calculate dragand weight ofF-16C or D1.1.1.1.3Deter mach as,true airspeed& CRS  RAD1.1.1.1.1Perform DTCloading1.1.2Collect missiondata fromagencies1.1.3Performmissionbriefing1.1.1.3Determinedeparturedata1.1.1.4Determinereroute data1.1.1.1.4Det fuel time & dist for climband opt alt Starting with the terminal objectives at the top, the question is asked: "To be able to perform as specified in the objective, what essential skills, knowledge, or attitudes must the individual possess?"  This will yield a number of major components.  For each component, the question is asked again.  The output of this procedure is a multilevel hierarchy in which the objectives at any one level of a hierarchy leg are prerequisite and essential to performing the objectives at the next higher level.  A completed objective hierarchy reveals all of the objectives that are to be learned.  It also specifies the coordinate and subordinate relationships, suggests the order in which they should be learned, and cross-references enabling objectives across legs of the hierarchy.     The hierarchy can be depicted graphically or as statements numbered to reflect the hierarchy.    AFH 36-2235 Volume 8 1 November 2002 109 Examples of task/objective hierarchy (Continued)  TASK/OBJECTIVE HIERARCHY STATEMENTS Behavior StatementCondition Standard Objective Number 1.1 1.1.1 Perform pre-mission planning Determine the mission data Must be done IAW T.O. 1F-16C-1CL-1 Must correctly complete 85% of the quiz/test questions to pass Must correctly complete 85% of the quiz/test questions to pass Must correctly complete 85% of the quiz/test questions to pass Unaided and in writing Describe the information that can be transferred by the DTE  Note the numbering used in the table.  The numbering is used to identify the terminal and enabling objectives.  These numbers are useful later when the objectives are sequenced for training, lesson materials are designed, and tests are developed for the objectives.     Given a mission, all relevant mission information, and a copy of T.O. 1F-16C-1-1 Given a mission, all relevant mission information, and a copy of T.O. 1F-16C-1-1 Given a mission, all relevant mission information, and a copy of T.O. 1F-16C-1-1 Given a suitable DTC loader/reader Given a graphic representation of data transfer equipment Unaided and in writing 1.1.1.1 Determine pre-takeoff data 1.1.1.1.1 Load the DTC Label data transfer equipment (DTE) main components Describe data transfer equipment (DTE) components 1.1.1.1.1.1 1.1.1.1.1.2 1.1.1.1.1.3  AFH 36-2235 Volume 8 1 November 2002 110 Prioritizing, Clustering, and Sequencing Objectives Introduction  Who is responsible?  Prioritize objectives   Once the objectives and sub-objectives have been identified, the next step is to prioritize, cluster, and sequence them into units of training.  A unit of training is defined as any unit of training such as a course, module, block, or lesson.  Guidelines for prioritizing, clustering, and sequencing objectives are provided in this portion of the section.  Additional information on clustering and sequencing objectives can be found later in this chapter under the section “Designing Training Activities.”   The curriculum developer and the SMEs for the flying organizations are responsible for prioritizing objectives, as necessary.  Also, the developer or development team is responsible for clustering and sequencing objectives.   Prioritizing objectives may not be required in all training development projects.  However, as the Air Force budget continues to shrink, you may be faced with the need to prioritize the objectives in order to provide the necessary training. For example:  “You need four days to train students on the Flight Director system.  However, due to scheduling constraints you can only have three days.”  Guideline There is a simple guideline to follow if faced with this situation.  That guideline is to have SMEs in the flying organizations prioritize the objectives that are to be taught. Once the objectives have been prioritized, the curriculum developer designs the training to cover as many of the objectives as possible in three days.  When Should You Prioritize? Objectives should be prioritized if there is a possibility that a resource constraint will not allow all objectives to be taught.  Decisions on prioritizing objectives can often be made during the Training Planning Team (TPT) meeting.  AFH 36-2235 Volume 8 1 November 2002 111  Clustering or grouping objectives allows logical and meaningful units of training to be designed.  Without clustering, it would be impossible to design effective and efficient training.  Four basic guidelines for clustering objectives are provided.  Techniques for Clustering Objectives into Individual Lessons Cluster objectives that are prerequisites to other objectives. For example – Basic flying skills and knowledge may be required for many tasks in a course; therefore, they may be clustered in the same unit and taught in the core training. Cluster objectives that are related to the same system or engage the same type of action. For example – All tasks related to the operating of a particular piece of navigation equipment on an aircraft may be clustered into a unit.  Cluster objectives with common skills and knowledge. For example – Several flying maneuvers (tasks) may require identical skills and knowledge to perform. Cluster objectives with potential for the same training method or delivery system. For example – Knowledge objectives may utilize a training method such as a lecture while skill objectives may be taught by demonstration/performance and clustered together.  Also, objectives with the same type of learning may be grouped together to facilitate learning such as classroom, equipment.  Clustering objectives        objectives Sequencing            Properly sequencing objectives plays a key role in the effectiveness and efficiency of training design.  Improperly sequenced objectives can result in training being provided at the wrong time for effective learning, which results in a waste of resources.  Properly sequencing objectives involves sequencing objectives within units or modules, as well as the course itself.  Two basic guidelines for sequencing objectives are provided below.  AFH 36-2235 Volume 8 1 November 2002 112 Sequencing objectives (Continued)     Guidelines for Sequencing Objectives Teach prerequisite skills and knowledge first. For example – A student should be taught aircraft engine operation prior to being taught how to start the engine. Follow the fixed sequence or logical order in performing a task. For example – The student should be taught to perform cockpit checks prior to being taught to start the engine. Place the more easily learned and familiar objectives early in the sequence with the more complex and unfamiliar objectives. For example – A student should be taught what ailerons are and how they work prior to being taught how to do an aileron roll.  AFH 36-2235 Volume 8 1 November 2002 113 Section D Develop Tests Introduction  Purpose  Testing concepts  Develop a test plan   At this point, the objectives have been developed, clustered, and sequenced for the course.  The next task in the training design process is to develop tests to assess the student's attainment of the learned capability specified in the objectives.  To ensure that tests adequately measure the objectives they support, the performance required in the test must match the performance required in the objective.  This section will address the various aspects of tests and test development.   The primary purpose of testing is to assess the student's attainment of the behavior specified in the objective.  Other purposes of tests are to:  Identify and correct problems or weaknesses in the training. Indicate instructor proficiency.   The Air Force measurement program is based on the concept of criterion testing, which is basically a go/no-go type of testing.  This form of testing rates the student with respect to the standards of performance that are specified in the objectives.  To attain an objective, the student must meet or exceed the standard specified in the objective.  All terminal objectives selected for training must be tested.  Enabling objectives can be randomly selected as long as they assess the student's attainment of the specified behavior.   The test plan may be developed as a separate document or as part of the training development plan or evaluation plan.  The primary areas of concern in the plan are:  How to handle student failure How to remediate a student(s) How tests are administered, scored, and trended When student should face flying evaluation board (FEB)  If a computer-managed testing program is used, the test plan should identify how it will be carried out and who is responsible.  AFH 36-2235 Volume 8 1 November 2002 114 Types of test       Characteristics of tests      Diagnostic Survey Reliability   Usability   As a curriculum developer, you will likely be required to develop one or more different types of tests.  The basic types of tests used in the Air Force are described below.  Type of Test Criterion   Purpose of Test Used to measure the student's attainment of the objective. Used to measure the effectiveness of the instruction. Used to determine attainment of supporting skills and knowledge necessary to perform the terminal objective. Used during validation to predict success, to identify and correct weaknesses in the instruction. Used to determine what prospective students already know and can do before receiving instruction. Used during development of instruction to gather data for design of instruction.   There are several characteristics to be considered when developing tests.  These characteristics ensure the tests measure what is intended each time they are administered.  The characteristics are shown in the following table.  Characteristic Validity   Definition Content:  Degree to which the test measures what it is intended to measure Predictive:  Degree to which the test predicts performance Degree to which the test yields the same results consistently Test-Retest:   Consistency across two administrations to the same students Split-halves:  Consistency across two forms of the same test Tests that are easy to administer, score, and interpret AFH 36-2235 Volume 8 1 November 2002 115 Comparison of performance and predictive tests      In the past, most predictive tests were written paper-and-pencil type tests, because they are easy to produce, administer and score.  The most common types of written test questions are essay, short answer, fill-in-the-blank, labeling, multiple-choice, matching, and true-false.  Today's media provide other testing options.  Computers provide different types of input systems that can have a high degree of fidelity with real-world tasks.  Even a simple input device such as a joystick or a mouse allows for identification by pointing with a cursor.  More elaborate devices such as magnetic field detectors, infrared detectors, etc., allow the computer to detect even more complex behavior.  How does an instructional designer decide what type of test to construct or what type of item to use?  The best type of test is one that gives the decision-maker the best information regarding the student's mastery of the objective.   Different types of test items have advantages and disadvantages with regard to good test construction.  These advantages and disadvantages have to be considered in terms of validity and reliability of the test.  Predictive Test Item Requires students to demonstrate knowledge by responding to various types of written questions. Emphasizes verbal or symbolic aspects. May require students to find, read, and use technical materials. Items are knowledge the student should learn to perform or make decisions on the job. Items are independent questions, not dependent on sequence. Errors on one item should not affect performance on another item. Performance Test Item Requires students to accomplish a job-like task under controlled conditions. Emphasizes nonverbal aspects. May require students to find, read, and use certain technical material (job aids, for example). Items are skills that students should perform, or the decisions they may make on the job. Items are dependent on sequence in which they are presented. Errors early in the sequence may affect final outcome of the task.  AFH 36-2235 Volume 8 1 November 2002 116 factors Test construction    Several key factors should be considered when constructing tests.  What to measure.  Analysis of objectives should identify what should be measured.  To determine what to measure, list the tasks to be performed or objective statement to be covered by the test.  One or more test items may be needed to adequately measure each objective.  Tests should measure application of principles, knowledge of factual information, ability to perform task, and transfer of knowledge and skills to solve similar problems.    This is the most important aspect of test construction.  If the conditions of testing are not consistent with the objectives then the test will not measure the desired behavior. Testing level.  The level of testing (know, comprehend, etc.) should correlate with the stated level of learning for that portion of the instruction being tested–no higher and no lower.  Test length.  Adequate coverage of the objective is the major factor in determining the length of test that is required.  Longer tests are normally more reliable since they usually cover the material better.  Selection and arrangement of test items.  Select test items that cover the most essential and significant portions of the material.  Test items selected should be clear, concise and well written to minimize misunderstandings.  Items of the same type should be grouped together in a test, if possible.  Individual test items should also be arranged in approximate order of difficulty, which allows the students to progress as far as they can without spending excessive time on difficult items at the first part of the test.  Testing level.  The level of testing (know, comprehend, etc.) should correlate with the stated level of learning for that portion of the instruction being tested – no higher and no lower. They are the fixed conditions that determine this level. AFH 36-2235 Volume 8 1 November 2002 117 factors (Continued)  Test construction   Test development guidelines            Test length.  Adequate coverage of the objective is the major factor in determining the length of test that is required.  Longer tests are normally more reliable since they usually cover the material better.  However, a very long test covering many different skills may be much more unreliable than a very short test covering only one skill.  Selection and arrangement of test items.  Select test items that cover the most essential and significant portions of the material.  Test items selected should be clear, concise and well written to minimize misunderstandings.  Items of the same type should be grouped together in a test, if possible.  Individual test items should also be arranged in approximate order of difficulty, which allows the students to progress as far as they can without spending excessive time on difficult items early in the testing.   There are many good tools / models used by curriculum developers to determine which test type would best measure attainment of the desired behavior.  AFH 36-2235, Volume 2, lists several.  Specific guidelines for constructing different test formats are provided.  Test Type  Multiple choice True / false Matching Completion Labeling Essay Performance test using a process checklist  Performance test using a product checklist  Page 119 120 121 122 123 124 126 127 AFH 36-2235 Volume 8 1 November 2002 118 Procedures to develop written tests        written tests Constructing   When developing written test items, follow these steps.  Step 1 2 3 4 5 6   Description Determine the test format and the number of items per objective. Generate required test items. Arrange test items (logical, simple-to-complex, and procedural).  Write directions for test administration.  Review test for accuracy and completeness.  Publish test in adequate copies.   As a curriculum developer, you will need to be able to write effective tests in order to measure what the students have learned or can do.  The following information is provided on preparing the most common types of written tests:  Multiple Choice True/False Matching Completion Labeling or Identification Essay  AFH 36-2235 Volume 8 1 November 2002 119  Multiple choice      Multiple-choice tests are probably the most used type of written tests.  They test recall, problem-solving skills, application of facts and principles, and understanding.  A multiple-choice item consists of a stem (a question or uncompleted statement), a correct response, and distracters (incorrect responses).  Example   Directions––Carefully read the questions or statements below and circle the correct response.  There is only one correct response.  1.  What is the minimum continuous level flight airspeed (lock point)?  a.  142 KTAS b.  151 KTAS c.  181 KTAS d.  184 KTAS   Construction Guidelines  Do not use the articles "a" and "an" at the end of the stem; this tends to indicate the correct answer. All responses should follow grammatically from the stem. All responses should be of approximately the same length. All responses should have a similar grammatical structure. All responses should use similar terminology. Provide as many responses as necessary, but normally no less than three. Position the correct response randomly throughout the test. Limit the use of responses such as "none of the above" or "all of the above." Distracters should be plausible, but incorrect. Numerical responses should be arranged in ascending or descending order.  AFH 36-2235 Volume 8 1 November 2002 120 True / false        A true/false test should be used sparingly since the chance of random guessing is high.  True/false tests may be used when you want a student to identify a completely true or completely false statement.  Example   Directions––Carefully read the question below and circle true or false to indicate the correct response.  True   False 1.  The minimum continuous level flight airspeed (lock point) is 151 KTAS. 2.  The L/D maximum airspeed is 85 KTAS electrical.    True   False   Construction Guidelines  Include only one idea in each statement. Place the crucial element at or near the end of the statement. Avoid using negatives, as they tend to confuse students. Do not use absolutes such as "all," "every," "none," and "never."  Do not use vague terms such as "some," "any," and "generally."  AFH 36-2235 Volume 8 1 November 2002 121  Matching    A matching test is used to measure a student's ability to identify and discriminate among related or similar items.  Matching items normally use two columns of related items, and students are required to match a series of items listed in one column with related items in the other column.  It provides a way to test multiple knowledge simultaneously.  Example   Directions––Listed in the two columns below are words and phrases.  Select the phrase in the right-hand column that defines the word in the left-hand column and place the identifying letter in the blank space provided.  Some of the phrases may be used more than one time, while others may not be used at all.  The first item is answered as an example.   1. - 5.    c   Validity       Reliability       Objectivity       Comprehensiveness       Differentiation       Usability  c.  The ability to measure what it is designed to measure. d.  The degree to which adequate numbers of test items e.  The ability to give consistent results. f.  Often determined by correlating the original test scores a.  The ability to detect small differences in the mastery of b.  The degree to which a test is easy to administer, score, the subject matter. and interpret test results. are used for each objective. with those obtained in a retest. g.  Test score that is not influenced by bias or personal opinion of the score. h.  Assumed to be present when a job is learned to a high level of proficiency within a reasonable time.      Construction Guidelines  Provide clear, concise directions on how to match the items in the two columns. Indicate if the responses may be used more than once or not at all. Limit test items to a single area and the choices to a single subject matter category. Arrange the responses in the same logical order.  AFH 36-2235 Volume 8 1 November 2002 122 Completion     A completion test item requires the students to recall and supply one or more key words that have been omitted from the statement.  The word(s), when placed in the appropriate blanks, make the statement complete, meaningful, and true.    Example   Directions––Complete the sentences below by adding the correct word(s) in the blank spaces that are provided.  1. In a formation takeoff, wingtip clearance to be maintained is ____ to ____ feet.  2.  During formation takeoff, altitude indicated by ____ and _____ should match that of lead throughout the takeoff.   Construction Guidelines  Leave blanks for key words only. Keep items brief. Make all blanks approximately the same size. Grammatical cues to the correct answer, such as articles like "a" or "an" right before the blank, should be avoided. Ensure that only one correct answer fits each block.  AFH 36-2235 Volume 8 1 November 2002 123  Labeling or identification      A labeling or identification test is used to measure a student's ability to recall and label parts in pictures, schematics, diagrams, or drawings.  This form of testing is most often used to measure recognition of equipment components or other concrete objects.  Example   Directions––Identify the numbered items of a CPU-26 computer.  Place the correct answer in the space provided.    Construction Guidelines  Make all sketches, drawings or illustrations clear and of sufficient size.  If possible, use the actual parts of a unit. Provide sufficient information to indicate what the equipment is and which part is to be labeled. The parts to be labeled or identified should be clearly pointed out by using lines or arrows. Ensure that only one definite answer is possible.  AFH 36-2235 Volume 8 1 November 2002 124  Essay      An essay test, which is not normally used in technical training, requires extensive discussion by the student.  These tests should be used only when students are expected to think reflectively or creatively, to organize knowledge in the solution of a problem, and to express their solution in writing.  Example   Direction––Complete the essay question in 500 words or less.  Use only one side of the paper to write your response.  Test Item––Describe the effect wind shears have on an aircraft during the landing approach.   Construction Guidelines  State essay item clearly so students know exactly what type of discussion is expected. The essay item should ask for comparisons, decisions, solutions, cause-effect relationships, explanations and summaries. When possible, use more essay items and limit the discussion on each. Set limits on essay questions, such as time or number of words. Determine how the question will be scored for objective grading.  AFH 36-2235 Volume 8 1 November 2002 125  Procedures to develop performance tests       Constructing performance tests / checklists   When developing performance tests, follow these steps.  Step 1 2 3 4 5   Description List steps/activities/behavior (process) or characteristics (product). Note common errors that are made when performing the checklist. Arrange the steps or activities and characteristics in proper order. Review the checklist for accuracy and completeness. Publish checklists in adequate quantities.   Performance tests, which require the student to perform a task, usually take the format of a checklist.  The checklist is developed to correspond to the steps or activities of the task being performed.  During the performance test, an evaluator observes the student performing a series of steps or activities while rating the steps on a checklist (process evaluation).  An evaluator may also rate the end product of a performance on a checklist (product evaluation).  For operational flying, reaction time is often considered as well as the correct action to take.  As the students gain experience, they are expected to improve.  AFH 36-2235 Volume 8 1 November 2002 126  When a performance test requires the steps or activities to be rated, a process checklist is used.  The checklist should contain all of the essential steps or activities for successful performance.  The actual weapon system checklist should be used when possible, especially since different techniques can provide the same results.  The courseware developed with the help of the SME may have to select only one technique for training and testing.  The flying evaluation grade sheet is also used to record proper performance and to brief students on how well (or poorly) they did.  Items are repeated in a phase of instruction until the desired standard is achieved.  Constant monitoring by the student's flying supervisor allows for remedial action, when necessary.  Following is an example of a process checklist.  Example  PROCESS CHECKLIST MISSION BRIEFING ON OVERALL ROUTE, RADAR NAVIGATION AND APPROACH PROCEDURE Performance test using a process checklist             Step or Activity  Check          Guidelines for Construction and Use  1.  Route and type profiles 2.  Ground speed 3.  Bank angles 4.  Terrain obstacles/hazards 5.  FLIR INS procedures 6.  Fuel figures 7.  Emergency airfields 8.  Airborne radar approach Description of error        Use when the performance of steps or activities of a task are to be evaluated.  The step or activity must be observable.  Define all of the steps or activities of the task being performed. Sequence steps or activities in order of performance. Provide space for checking the performance of each step or activity. Provide space for recording and describing errors.  AFH 36-2235 Volume 8 1 November 2002 127 Performance test using a product checklist          When a performance test requires the product of a process or a task to be evaluated, you will probably want to use a product checklist.  The checklist should identify criteria or characteristics of product acceptability.  Following is an example of a product checklist.  Example  PRODUCT CHECKLIST –  INTERPRETS AIRCRAFT INSTRUMENTS  Yes  No         Product Criteria / Characteristics 1.  States degrees and direction of A/C pitch and roll within ± 5 degrees 2. IAS within ± 10 Kts  3.  Altitude within ± 50 ft 4.  Vertical velocity in correct direction and rate of change within ± 100 ft Comments      Guidelines for Construction and Use  Use a checklist when the objective requires the student to produce something. Use a checklist when the product can be readily evaluated. Use a checklist when there are no fixed or set procedures. Identify the characteristics of the product. Provide space on the checklist for product rating. Provide space on checklist for comments about the product.   AFH 36-2235 Volume 8 1 November 2002 128 Section E Review Existing Materials Introduction  Benefits  Sources of materials  Types of existing material  How to select materials   Developing training materials can be expensive and time-consuming.  Therefore, after developing objectives and tests, review existing training materials to determine if materials exist that will support the course objectives.  If materials are found, it is possible that they will not totally support the objectives.  If so, don't hesitate to modify the material to fit your need.  You may even use portions of existing materials to your advantage.  The time you spend reviewing existing material will be time well spent.   Using existing materials saves time, human resources, materials, and money.   There are several sources for existing training materials, such as:  DoD Other military branches Interservice organizations  Other federal agencies Commercial Colleges and universities   When reviewing existing training materials, you may find them in some of the following forms:  Printed materials (e.g., textbooks, technical orders, handbooks, job aids) Slides Video Audio cassettes Computer-based [ICW, Computer-Assisted Instruction (CAI)] Training aids   To standardize the process and allow comparison between materials under review, use a job aid such as the one below to help select existing materials.  AFH 36-2235 Volume 8 1 November 2002 129   Job Aid Material Review Form 1.  Objective(s) the material supports _____________________________ 2.  Type of Media ____________________________________________ Date _____________ Poor       Good       Excellent 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 6.  Can the material be modified to improve its utility?  If so, what should be done?   Evaluator _____________________        Evaluation of Material Accuracy  Currency Level 3.   Content   Structure Organization Sequence   Suitability Supports Objective User-friendly Pace Guidance Feedback Motivational Measurement  4.  What do I like about the material? 5.  What do I dislike about the material?           AFH 36-2235 Volume 8 1 November 2002 130 Modifying existing materials  Additional information   Sometimes you may find that existing material is available for use but it needs modification.  When modifying existing materials you may:  Add new material such as information on the operation of a particular piece of equipment that was not covered in the existing material available. Expand existing materials to include more detailed information, diagrams, illustrations and examples. Delete materials that do not support your objective(s). Update material to include current information, such as revised procedures, data, and to correct inaccuracies. Re-sequence material to make it compatible with your training design.   For additional information on reviewing existing materials, see:  AFMAN 36-2234, Instructional System Development, Design chapter. Dick, W. and Carey, L. (1990). The Systematic Design of Instruction (3rd Ed.). Glenview, Illinois: Harper Collins Publishers. Knirk, F. G. and Gustafson, K. L.  (1986). Instructional Technology: A Systematic Approach to Education.  New York: Holt, Rinehart, and Winston.   AFH 36-2235 Volume 8 1 November 2002 131 Section F Design Training Introduction  Where to read about it        If you cannot find existing materials that fit your needs, you should proceed to design your own training.  Your design will determine the effectiveness and efficiency of the training system.  Training design should be determined by what best meets the training needs rather than just using a particular training design that has always been used in your training organization.   This section covers three topics.   Determine Training Strategies Design Training Activities Select Media  Page 133 136 138 Topic AFH 36-2235 Volume 8 1 November 2002 132 Introduction  training strategies Determining     Determine Training Strategies  After the objectives and related tests have been developed and you know what behaviors are required, you should plan and develop the most effective and efficient learning environment using principles of learning. This process starts by determining the training strategies or plan you intend to use. A training strategy is necessary in order to manage the design of the training activities and the learning process.   Training strategies can be determined from the following issues:  Student participation Student feedback Student pacing Training sequence  Student Participation  Active student participation is essential for learning to take place.  Students learn by doing, thinking, and feeling–through answering questions, discussing, computing, manipulating, and putting ideas together.  The training strategy ensures that students are active in the learning process and that they can apply or demonstrate what they have learned.  Learning is a process in which students gain skills and knowledge, and form attitudes through their own activities, experiences, and motivations.  For each type of learning, consider using the following strategies.  Type of Learning Skill  Training Strategies Demonstrate task that the student is to perform. Have student do each step of the task following the demonstration. Have student perform the entire task with minimum guidance.   AFH 36-2235 Volume 8 1 November 2002 133  Determining training strategies (Continued)  Type of Learning Knowledge   Attitude Training Strategies Provide drill and practice to reinforce recall. Use examples and non-examples to reinforce understanding. Provide opportunity to practice the knowledge in context. Use human modeling to shape student attitude. Use guided discussions for effective lessons. Give or withhold rewards.    Student Feedback  Students participating in the teaching/learning activity will need feedback on how well they are doing. The kind of feedback, as well as the scheduling of feedback, depends on the kind of learning.  Feedback not only informs the students of how well they are doing, but also serves as a valuable source of motivation.  The training strategy should provide each student with feedback, whether it is the result of a written test or instructor comments during the performance of a task.  Ask frequent questions and provide feedback to address any possible student misunderstanding about the content.  Student Pacing  Pacing is the rate students go through the training.  There are several ways to pace students' progress.  Group-Paced – Students progress through the training together as a group at the same rate.  This is a very effective pacing method when training large numbers of students.  However, it does not allow individuals to deviate from the group rate.  Group Lock-Step – The group progresses through the training at a predetermined pace, completing training on schedule.  This is normally the most costly form of pacing.  However, in cases where timing is critical, it is very effective. Self-Paced – Students are allowed to work at their own pace through the training within certain parameters.  This form of   AFH 36-2235 Volume 8 1 November 2002 134  pacing is very effective in courses using CBT, interactive video and self-study. Combination Pacing – Any of the above-mentioned forms of pacing may be used in combination.  Instructional Sequence  The following sequence considerations also determine training strategies.  Proficiency Advancement – This technique is used to advance students who have prior knowledge, practical experience, or are exceptionally fast learners.  Students show their proficiency by passing a criterion test and may bypass the training in which they have passed the criterion test.   Multiple Track – A training sequence may be divided into various tracks to allow students to go through training best suited to their abilities and needs.  The best track for a student is determined by a pretest.  Modular Scheduling – Training is divided into different modules and students are pre-tested to determine which modules of training they need.  Modular scheduling is normally used only when the learning sequence is not critical. Determining training strategies (Continued)      AFH 36-2235 Volume 8 1 November 2002 135 Design Training Activities Introduction  sequencing Reasons for  Sequencing training   Design of the training activity depends largely on two factors –training sequence and size of the training segment.  Effective and efficient training depends on how well you sequence and size the training.   Properly sequenced training provides:  Student motivation – Students learn best when they are motivated.  Motivation often depends on a proper sense of direction.  Early stages of training should provide a sense of direction.  Properly sequenced training will provide direction and gives students a "mental" roadmap of where they are going and how they are going to get there. Meaningful relationship – Proper sequence provides students with a pattern of relationships so that each training activity has a purpose.  If the training is meaningful to the students, they will learn more easily and quickly. Content consistency – Proper sequencing helps avoid inconsistencies in the training content.  Carefully sequenced training will help eliminate gaps and duplication.  Consistency of content ensures that skill progression is orderly and prerequisite skills and knowledge have been acquired prior to introduction of advanced subject matter content or performance of more complex tasks.     There are several methods of sequencing training. Job performance order – This method sequences training in the order in which the tasks and subtasks are performed.  Tasks and subtasks arranged in this order are very effective, since it is an orderly process, which builds learning on learning and adds realism to the training. Psychological order – This method of sequencing arranges the training content based on the ease of learning.  Students are taught easier tasks first, and then the more complex tasks are taught.  This method also includes the process of going from what students already know and proceeds to new information. AFH 36-2235 Volume 8 1 November 2002 136 Sequencing training (Continued)  Guidelines for sequencing  training unit Optimum size of   Logical order – Normally, training activities should be designed to proceed from the simple to the complex or from known to unknown.  However, training activities will not always lend themselves to this design method.  In such a case, training activities may be designed using both perfor-mance and psychological order.  This arrangement normally includes the whole-part-whole concept.   Various sequencing methods can be used to design training activities based on training content and resource availability.  When selecting methods, consider doing the following:  Place easily learned objectives early in the training sequence. Provide common or "core" training early in the training sequence. Sequence sub-objectives with the objective they support. Place skills and procedures within each objective in job order when possible. Introduce concepts at the point where understanding the concepts is a prerequisite for successful performance. Provide prerequisite skills training before the time it is required for the job. Introduce skill or knowledge in the task where it is most likely or frequently used. Provide practice of skills and concepts in areas where transfer of like or related skills is not likely to occur. Place complex and cumulative skills later in the training sequence.   There are no hard and fast rules on the proper size or increment of training.  There is a natural tendency to play it safe and provide more training than is necessary.  This costs valuable resources and normally causes boredom on the part of the student.  A good approach to designing the size or increment of training is to start with minimal training and rely on validation to show if more training is needed.  This method, combined with evaluation feedback, will indicate if more training is required.  If you provide more training than is necessary in the beginning, it will be difficult to determine until feedback is received from the field.  AFH 36-2235 Volume 8 1 November 2002 137 Select Media Introduction  Definition   Types of media               Although selection of training methods and selection of media are discussed individually, they can't always be considered separately.  No single medium is the most appropriate choice for every training situation.  A proper medium ensures that information is presented to the students by the most effective and efficient means possible.  Note:  As a reminder, the media selected for the training system have a direct impact on resources such as personnel, facilities, and funds that were identified during the planning stage of the ISD project.   Media are the means, instruments, or materials used to communicate information–in other words, the means used to give information to the students.  Examples of media include classroom instructor, study guides, CBT, satellite training, interactive video, or numerous other types of media.   There are several types of training media, as shown below.  Types of Media  Instructor/Tutor  Traditional Audio/Visual Devices Examples Lecturer Demonstrator Tutor/Coach Chalkboards Transparencies Overhead projectors Slides Pre-narrated slides Microfiche Film strips (silent/pre-narrated) Video tapes Slide/workbook/tape recorder combinations   AFH 36-2235 Volume 8 1 November 2002 138 Types of media (Continued) Types of Media  ICW Print   Characteristics of media Printed materials Examples CBT (traditional) IVD CMI Workbooks Study guides Job aids Training manuals Programmed instruction booklets Actual equipment trainers Flight training simulators Part-task trainer Computer simulation Training Devices and Simulators   Media have various characteristics that are either suitable or unsuitable for particular training situations.  Consider these characteristics carefully to ensure that the appropriate medium is selected for the training system.  The following table lists the advantages and limitations of each of the following mediums.  Printer materials Overhead transparencies Audiotape recordings 35-mm Slide Series Multimedia Video and Film Interactive Courseware (ICW)  Include common types of materials.   Advantages 1. 2.  Have wide variety of applications. 3.  Simple types quick to prepare.  Limitations 1.  Sophisticated types more costly to prepare. 2.  Require suitable reading ability.  AFH 36-2235 Volume 8 1 November 2002 139  Advantages 1.  Can present information in systematic, developmental sequences. 2.  Use simple-to-operate projector with presentation rate controlled by instructor. 3.  Require only limited planning. 4.  Can be prepared by variety of simple, inexpensive methods. 5.  Particularly useful with large groups.  Limitations 1.  Require special equipment and skills for more advanced Overhead transparencies Audiotape recordings preparation. 2.  Are large compared with other projectors.   Advantages 1.  Easy to prepare with regular tape recorders. 2.  Can provide applications in most subject areas. 3.  Equipment is compact, portable, and easy to operate. 4.  Flexible and adaptable as either individual elements of instruction or in correlation with programmed materials. 5.  Duplication easy and economical.  Limitations 1.  Have a tendency for overuse, as lecture or oral textbook reading. 2.  Fixed rate of information flow. 3.  Low fidelity of small portable recorders.  Note–If contractor-developed, audio tapes are typically made by a professional narrator; hence, they are not "easy" to prepare or revise.  AFH 36-2235 Volume 8 1 November 2002 140 35-mm slide series Multimedia   Advantages 1.  Require only filming, with processing and mounting by film laboratory. 2.  Result in colorful, realistic reproductions of original subjects. 3.  Prepared with any 35-mm camera for most uses. 4.  Easily revised and updated. 5.  Easily handled, stored, and rearranged for various uses. 6. Increased usefulness with tray storage and remote control by presenter. 7.  Can be combined with tape narration for greater effectiveness. 8.  May be adapted to group or to individual use.  Limitations 1.  Require some skill in photography. 2.  Require special equipment for close-up photography and copying. 3.  Can get out of sequence and be projected incorrectly if slides are handled individually.   Advantages 1.  Can demand attention and create strong emotional impact on viewers and/or users. 2.  Condenses time and materials required to upgrade skills by using a number of training technologies. 3.  Effects rapid skill transfers by providing the ability to implement changes to meet new or revised training requirements. 4.  Computer-based multimedia manages feedback and evaluation by tracking information as the user interacts with the media. 5.  Computer-based multimedia presents information in different content formats such as motion video, audio, still images, graphics, animation, text, etc.  Limitations 1.  Require additional equipment, complex setup, and careful coordination during planning, preparation, and use. 2.  Equipment and production costs may be high for complex programs.  AFH 36-2235 Volume 8 1 November 2002 141 Video and film Interactive Courseware (ICW)   Advantages 1.  Particularly useful in describing motion, showing relationships, and giving impact to topic. 2.  Allow instant replay of video recording. 3.  Videotape is reusable. 4.  Easy to record lip sync on videotape. 5.  May include special filming techniques (animation, time-laps).6.  Combine still and motion on videodisc. 7.  Standardized film projector available everywhere.  Limitations 1.  High cost for studio production equipment. 2.  High cost in time and resources for revision. 3.  Resolution limited with video for fine detail. 4. 5.  Value of investment in motion picture equipment reduced as Incompatibility of particular video format types. video replaces film.  Note – Videotape and videodisc are fast replacing 16mm film medium.   Advantages 1.  Presents text information and graphic images. 2. Interacts with learners on individual basis through asking questions and judging responses. 3.  Maintains record of responses. 4.  Adapts instruction to needs of learner. 5.  Controls other media hardware. 6.  Can interface computer and video for learner-controlled programs.  Limitations 1.  Requires computers and programming knowledge, except for low to mid-level CBT. 2.  Requires essential hardware and software for development and use. Incompatibility of hardware and software among various systems. 3.  AFH 36-2235 Volume 8 1 November 2002 142 Guidelines for selecting media Media selection for integrated activities   Several guidelines for media selection are:  Select media that do not conflict with the specific training environment. Select media that effectively meet the training objectives. Select media that support the training strategy. Select media that allow individualization of training to the greatest extent possible. Select media with development time, know-how, and dollar resources in mind. Select media that are effective and cost-efficient.   Most types of complex skills involve multiple objectives from different domains of learning.  A skill that involves two or more objectives from different learning domains involves integrated learning activities.  Media selection for integrated learning activities must take into consideration the enterprise and the learner's schema, metaskills, and experience.  Enterprise.  An enterprise is an integrated, purposeful activity that usually leads to accomplishment of a goal.  For example, an individual might have an enterprise to build and fly an airplane.  An individual does not have to have all the prerequisite skills to engage in an enterprise.  The importance of an enterprise is that it is purposeful and relevant to the learner.  This motivates the learning behavior necessary to complete the component tasks. Schemas.  A schema is an individual's organization of knowledge.  Schemas may take the form of scripts (a kind of story or scenario that organizes information) or frames (a structure that looks like a table or matrix into which information fits).  Different levels of learners have different types of schemas.  A novice learner (in a particular subject area) has a very sketchy schema or structure into which new information can be placed.  An experienced learner (one who has had some training in a subject area) has a better structure and therefore is a quicker learner than the novice.  Finally, an expert has a very highly developed schema and is probably capable of rapid learning with very little learning support.   AFH 36-2235 Volume 8 1 November 2002 143 Media selection for integrated activities (Continued)   These are simple definitions of 'enterprise' and 'schema,' and address little about how this effects media selection.  Enterprise and schema are more concerned with sequencing and whole-task practice than with media.  Metaskills.  Metaskills are cognitive strategies that an individual applies to the processing of new information in a novel situation (a scenario not previously experienced).  These skills include chunking or organizing new information, recalling relevant schemas, adding the new information to the old schemas, and creating new schemas.  Although metaskills are probably subject-independent, different individuals have different metaskill capabilities depending upon their experience with a particular subject content.  For example, an expert has developed metaskills and can relate better to a novel situation than a novice.  An expert is more efficient at processing new information and applying it to the novel situation.    Learner Experience.  It is helpful to know how experienced a learner is when selecting media or teaching strategies.  The more experience, the higher the level of metaskills and strategies the learner will be able to employ.  Experienced learners can deal with larger steps of instruction and more complex learning environments.  Novices, on the other hand, require simplification of complex contexts so they don't experience information overload while learning. AFH 36-2235 Volume 8 1 November 2002 144 Section G Update Training Development Plan Introduction  Why update the plan?  Who is responsible?  What should be updated?   The training development plan that was developed during initial ISD project planning may have already been updated during the analysis phase.  This plan, which serves as the roadmap for training development, may also need to be updated at the end of the design phase if there have been changes that impact it.  If the plan is to be an effective tool for managing training development and implementation, it will need to be periodically updated.   The training development plan should be updated at the end of the design phase, as applicable; to reflect any changes that impact the training development process resulting from the activities in the design phase.  If the training development plan is not updated, it will not be an effective tool for managing the training development process.   Keeping the training development plan updated with current and accurate information is the responsibility of the curriculum developers.  They should update the plan at the end of the design phase, as applicable.   At the end of the design phase, the training development plan should be updated with new or revised information, such as:  Changes to the overall training development Refinements to project definition Revisions to the resource requirements Changes in resource constraints New or revised milestones Addition or deletion of tasking  AFH 36-2235 Volume 8 1 November 2002 145 Chapter 6 DEVELOPMENT Overview Introduction  Where are you in the process?   Once the objectives and tests have been developed and training strategies and activities have been selected, you are ready to implement your design in the development phase.  Some of the tasks in this phase include writing training materials, producing training media, and developing ICW.  As a curriculum developer, this is where all your efforts from the earlier phases of ISD start to come together.   Figure 12 depicts the ISD model with the development phase highlighted.  Figure 12 Development Phase    The objectives of this chapter are to:  Write tasks involved in developing training. Prepare installation of training information management systems. Review and critique plans that need to be updated. Plan and execute the training validation process. Objectives    AFH 36-2235 Volume 8 1 November 2002 146 Where to read about it           This chapter contains seven sections.  Section A B C D E F G  Title  Prepare Course Documents Develop Training Materials  Develop Training Alternatives Install Training Information Management System Update ISD Evaluation Plan Update Training Development Plan Finalize Training Materials  Page 147 157 161 162 163 165 166 AFH 36-2235 Volume 8 1 November 2002 147 Introduction  Where to read about it    Additional information  Section A Prepare Course Documents  Each aircrew-training course normally has a course control document and several other related documents that are used to control, standardize and support presentation of the training course.  These documents are essential to an effective, cost-efficient training program.   This section covers two topics.   Course Syllabus Other Syllabus-Related Documents   For additional information on course documents, see:  Page 148 154 Topic AFMAN 36-2234, Instructional System Development, Development chapter. AFMAN 36-2236, Handbook for Air Force Instructors.  AFH 36-2235 Volume 8 1 November 2002 148 Introduction  Purpose  Who is  responsible?  What is in the course syllabus?     Course Syllabus  A course syllabus is a blueprint for conducting training in a given course.  In some training organizations, a Plan of Instruction (POI) is used for the same purpose; in DoD Inst. 29612, it is called the Course Control Document.   A course syllabus is used to standardize training and control its quality.  Additionally, the syllabus allows training and support managers to plan resources such as aircraft, fuel, bombs and ranges.   Training managers have the ultimate responsibility while curriculum developers are responsible for preparing the course syllabus.  They are also responsible for keeping it current and accurate.   Since aircrew operational training affects so many different USAF agencies, a standardized model for the syllabuses is required and is shown below.  Chapter 1 Title Course Accounting Content Course title Course number Course purpose Course description Course prerequisites Status upon graduation Multiple tracks identified, if applicable Academic and flying inventory    AFH 36-2235 Volume 8 1 November 2002 149  What is in the course syllabus? (Continued)       Course syllabus format   2 3 4 5 6 (Optional)   Course Training Standards Academic Inventory Aircrew Training Devices Flying Training Criterion-Referenced Objectives Training elements to be accomplished Standards of accomplishment for each element Course flowmap Training schedule matrix Lectures and description Workbook and description Sound/slides and description ICW lessons and description Mission statement and objective for each device training session Special instruction on flying portion of training Explanation of sorties  Mission outline Elements to be accomplished Terminal objectives   The format of course syllabuses is flexible.  They should be usable, document the plan for training, and meet the needs of the training organization as well as others.  Examples of a course syllabus cover and first pages are included on the following pages.  AFH 36-2235 Volume 8 1 November 2002 150 2 JULY 1992 INTRODUCTION E-3 MISSION CREW COMMANDER ACC SYLLABUS Course No. AB00000 USAF OPERATIONAL TRAINING COURSE Example –  Course Syllabus   DEPARTMENT OF THE AIR FORCE Headquarters, Air Combat Command Langley Air Force Base, Virginia  23665-5001            This syllabus prescribes the overall training strategy and average amount of instruction required for a student having the entry and prerequisites to attain the course goals.  Units tasked to implement this syllabus are responsible for ensuring that each student demonstrates the knowledge and skill proficiencies set forth in the course training standards.  Within syllabus and other directive constraints, the amount and level of training devoted to mission elements, events, subjects or phases should be adjusted, as required, to meet the needs of individual students.  Instructions governing publication and revision of ACC syllabuses are contained in ACCR 8-1.     OFFICIAL       JOHN SMITH, Colonel, USAF Director of Information Management                     JOHN DOE, General, USAF                    Commander                   AFH 36-2235 Volume 8 1 November 2002 151 Example –  Course Syllabus (Continued)  CHAPTER 1 COURSE ACCOUNTING (1)  Must be a Mission Ready ASO, AFSC 1744D a.  Track 1, Air Surveillance Officer (ASO) upgrade to MCC. SECTION A – COURSE DESCRIPTION  1-1.  COURSE TITLE/NUMBER.  E-3 Mission Crew Commander (MCC) Basic Qualification Course, E3000BQOBX.  1-2.  PREREQUISITES.  These requirements will be completed prior to the formal course start date for the track being entered.  The MCC course is divided into three tracks                   c.  Track 3, Personnel with no prior experience in AFSC 17XX or 17XX (3)  Must have 300 flight hours as an ASO in the E-3 aircraft (3)  Must have 300 flight hours as a WD and SD combined. (1)  Must be a Mission Ready SD, AFSC 1711/1716. b.  Track 2, Senior Director (SD) upgrade to MCC. (2)  Must be a Major or Major selectee                 (2)  Must be a Major or Major selectee personnel with no E-3 experience. (1)  Current Flight Physical Class III.  Physical must be current for a minimum of six months after reporting to Tinker AFB. (2)  Current Physiological training, including rapid decompression, before departure from present duty location.  Training must remain current for one year from reporting date at Tinker AFB. (3)  Must complete Basic Survival Training Course S-V80, and either Water Survival Course S-V86-A or S-V90-A before departing from present duty location.  AFH 36-2235 Volume 8 1 November 2002 152  Example –  Course Syllabus (Continued)         (4)  Complete Life Support Training at Tinker AFB. (5)  Must have a Top Secret and SCI security clearance. (6)  Graduate of accredited Weapons Control school and have AFSC 1711/1716 with one year experience in Automated Radar Systems (Sage, Buic, 407L, 412L).    (7)  Those personnel not satisfying the requirements of item (6) will complete the academic and simulator portion of the ASO/WD Basic Qualification Course (E3000BQOGX/E3000BQOBX).  (8)  Must be Major or Major selectee.    1-3.  PURPOSE/STATUS UPON GRADUATION.  To train personnel meeting course prerequisites to basic qualification (BQ) status in the Mission Crew Commander crew position in the E-3.  Graduates receive an E-3 rating of BQ IAW ACCM 51-60.  1-4.  LOCATION.  522 AWAC Division, Tinker AFB, Oklahoma.  1-5.  DURATION.     a.  Track 1 will be 43 training days divided into 13 ground training days and 30 flying training days. b.  Track 2 will be 45 training days divided into 15 ground training days and 30 flying training days. c.  Track 3 will be 64 training days divided into 20 ground training days and 44 flying training days. a.  Academic Training Hours b.  Aircrew Training Device Hours c.  Flying Sorties/Hours TRACK 1 86 11 8/64 TRACK 2 92 11 8/64 TRACK 3 117 27 13/114      1-6.  AMOUNT.          AFH 36-2235 Volume 8 1 November 2002 153 Example –  Course Syllabus (Continued)  SECTION B – FLYING INVENTORY  1-7.  The following charts show the average number of effective sorties required by each student.  (ILLUSTRATION) (ILLUSTRATION)     TRACKS 1 and 2 TRACK 3 SECTION C – AIRCREW TRAINING DEVICE INVENTORY  1-8.  MISSION SIMULATOR.     TRACK 1                TRACK 2  TRACK 3 AFH 36-2235 Volume 8 1 November 2002 154 Other Syllabus-Related Documents Introduction  Who is responsible?  Types of related documents Instructor guide   The syllabus is a control document for managers of the training program.  Other syllabus-related documents that support training are developed by curriculum developers and are used by the instructor and students.   Curriculum developers are responsible for ensuring that each component or related document is available to support the training program.   There are many syllabus-related documents that support the various aircrew-training programs.  Some programs may have requirements for specific syllabus-related documents that other programs do not need.  Several syllabus-related documents that every aircrew program will likely need are:  Instructor guide Student study guide Phase manual Mission guides  A brief description of the above-mentioned documents is provided below.   The instructor guide is the lesson plan that instructors use to present the material to the students.  It can be written as a narrative, an outline, or both.  A narrative is an aid for the new or substitute instructor or for material that is not presented often enough for the instructor to maintain proficiency.  Instructors who are proficient with the instructor guide in the narrative form may choose to use only the outline.  The parts of the instructor guide may vary in presentation order but it normally contains the:  Introduction – motivation, preview, review (perhaps a war story)  AFH 36-2235 Volume 8 1 November 2002 155  Instructor guide (Continued)   Student study guide Phase manual Presentation – material presented with spot summaries and transitions Summary – summary of what was just presented Instructor Guidance – information on what students will be doing next, i.e., test or quiz, reading assignment and next class assignment  The format of the instructor guide may change as a result of validation or to meet the training need.   The student study guide can be presented in any format that provides assistance about the material to be covered.  The objectives are given to the students so they will know what is expected of them.  The format of the student study guide may change as a result of validation or to meet the training need.   The Dash One and other TOs may confirm all the information the students need but they are often written at a level that may be difficult for new students.  Thus, phase manuals are developed to extrapolate the information for specific areas of flying.  Phase manuals are normally segmented into:  Conversion or transition Air to air Air to surface Mission guides   These manuals must be technically correct and will normally require coordination with the operational flying manager to ensure accuracy.   Mission guides may be developed for each flying event.  They identify:  Strategy for the mission What support is needed Type of instructor required, if any  AFH 36-2235 Volume 8 1 November 2002 156 Mission guides (Continued)  Format guide  Routes to be taken Goals of the mission Number and types of bombs, bullets, or others  The mission guide can be included in the syllabus (Chapters 4 and 5) or can be developed as a separate document.   Perhaps the most important requirement for syllabus-related documents is consistency within the training program.  All instructor guides, student study guides, phase manuals and mission guides should follow a predetermined format.  A format or style guide is a good way to ensure that all developers are providing professional-looking products that are effective and cost-efficient.   AFH 36-2235 Volume 8 1 November 2002 157 Section B Develop Training Materials  In the design phase, the training method and medium best suited for the training need were selected.  At this point, you are ready to start developing the medium to implement the training design.  Developing training materials is a time-consuming and exacting task regardless of the medium you use.  It is essential that a quality medium be developed, since it is the vehicle that carries to the students the information that is to be learned.   Media are the means, instrument, or material used to communicate information to the student.  Examples of media range from the classroom instructor using a lesson plan to study guides, CBT, satellite training, interactive video, or numerous other types.   Training can be delivered using media such as:  Introduction  Description  materials Types of training  Factors in media development  Instructor Print-based  Slide/tape Audio/video tapes CBT Interactive video Training devices/aids Satellite (i.e., distance learning)   Several factors affect the development of training media and materials.  The relative importance of each of these factors depends on the medium selected.  These factors are:  Development personnel required Development time required Development cost required           Medium Print Transparencies Slide/Tape Videotape CBT Interactive Video Development Activity Draft/write material Edit material Publish material Draft transparency Generate reproducible transparency Reproduce transparency Storyboard/script slide or tape Shoot and edit slide/tape Narrate audio Print slide/tape Storyboard/script  Shoot and edit video Develop audio Storyboard/script Develop graphics Program/code computer Storyboard/script Shoot and edit video Develop graphics Develop audio Program/code computer AFH 36-2235 Volume 8 1 November 2002 158 Development activities  Training material development requires many activities.  The type and number of activities depend upon the type of training materials being developed.  Some of the most common development activities are listed below.  Who is responsible?    Developing training materials normally involves teamwork and requires various skills.  Curriculum developers are responsible for planning, scheduling, and making sure the training materials are produced.  Team members required for production of different media are listed below.  AFH 36-2235 Volume 8 1 November 2002 159  Who is responsible? (Continued)  Print  Medium Development Role Transparencies Slide/Tape Videotape CBT Interactive Video        Guidelines for developing materials SME Curriculum Developer Editor Graphic Artist SME Curriculum Developer Graphic Artist Editor Script Writer SME Photographer Narrator/Editor Script Writer SME Video Producer, Editor, Photographer Sound Technician Script Writer SME Graphic Artist CBT Author Script Writer SME Video Producer, Editor, Photographer Graphic Artist Sound Technician CBT Author   When developing training materials, make sure they:  Support the objectives. Are student-centered. Meet the design that was specified in the design phase. Use techniques that are consistent with the principles of effective learning Are appealing to the students.  AFH 36-2235 Volume 8 1 November 2002 160  Guidelines for developing materials (Continued)  Are constructed so students will be attentive.  This can be done by making the material interesting and meaningful to the students.  Require the student's attention.  One way to accomplish this is to require specific responses or actions. Lead students in the direction of the behavior specified in the objective and guides them toward mastery of the task.  With the proper stimuli and reinforcement, the desired student behavior can be shaped. Are developed using experts such as programmers, photographers, graphic artists, scriptwriters, and editors in order to develop quality training materials. Are checked for items such as technical accuracy, completeness, programming errors, or blurred slides prior to publication or production to ensure quality. Have vocabulary at the appropriate level for the target population. Are properly paced–not too fast or too slow. Are easy to understand. Include the appropriate safety precautions. Support the human relations concepts to which the Air Force is committed.   Additional information on training development can be found in:  AFMAN 36-2234, Instructional System Development. AFMAN 36-2236, Handbook for Air Force Instructors. Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992). Instructional Design Strategies and Tactics. Englewood Cliffs, New Jersey: Educational Technology Publications. Additional information    AFH 36-2235 Volume 8 1 November 2002 161 Introduction  Purpose  Who is responsible?  When should the alternative be developed?  What should be done?  Section C Develop Training Alternatives  In the design phase, training alternatives were selected to support the primary training delivery method, as applicable.  If training alternatives were selected in the previous phase, the alternatives will need to be developed in this phase just as the primary training is developed.   The purpose of developing alternative training is to ensure that there is a "back-up" system ready to support the training system by delivering critical training in the event the primary delivery system is unavailable or not usable.   Curriculum developers are responsible for developing alternative training.  Managers in the training organization ensure that the necessary alternative is developed in this phase and is ready for use, if necessary, in the implementation phase.   Ideally, alternative training should be developed at the same time training is developed so it is ready at training implementation.  However, resources such as time, personnel, and equipment may not allow alternative training to be developed at the same time.  If not, it should be developed as soon as the resources permit.   Curriculum developers follow the same procedures for developing alternative training as they do to develop the primary training for the system.   AFH 36-2235 Volume 8 1 November 2002 162 Install Training Information Management System Section D Introduction  Who is responsible?  look for? What should you  As a curriculum developer, you may never be involved in designing or redesigning a training information management system.  However, you may be involved in the installation.  This section discusses the basic issues of installing a training information management system.   A project manager normally has overall responsibility for installing a new training information management system, revising an existing system, or modifying the software of an existing system.  Many individuals from various training organizations will likely be involved in testing the system.  For example, an instructor may be asked to update student records and a curriculum developer may be asked to update training materials in the system.   If you are involved in installing or testing a training information management system there are several questions you should consider.  Is the hardware user-friendly? Is the software user-friendly? Are there adequate terminals for all system users to do their jobs in a timely manner? Is the information management in your area accurate? Is the information management in your area complete? Is the system reliable?   AFH 36-2235 Volume 8 1 November 2002 163 Section E Update ISD Evaluation Plan Introduction  Why update the evaluation plan?  Who is responsible?  Tasks involved  An evaluation plan is the "metric" or standard for evaluating the ISD process and products.  It is developed initially in the planning stage and is updated in the analysis and design phases. To ensure that the ISD evaluation plan is effective throughout the life cycle of the project, you need to update it again in the development phase.   In order to have an evaluation tool that is effective throughout the life cycle of the system, you will need to keep the plan updated.  It should be maintained current and accurate to reflect the actual status of the evaluation effort.  Updating the plan will keep the development effort on track and ensure quality.   Curriculum developers are responsible for keeping the evaluation plan updated, while management within the training organization is responsible for ensuring that the plan is effective and that it is implemented.   An ISD evaluation plan should be updated at the conclusion of each ISD phase if significant changes have occurred during the process.  Updating an evaluation plan involves the following tasks.  Document changes such as:    Development products to be evaluated such as training materials, lesson plans, and ICW Procedures for evaluating development process and products Evaluation standards  Revise the evaluation schedule, including: Quantity of products to be evaluated When the development process and products will be evaluated AFH 36-2235 Volume 8 1 November 2002 164  Tasks involved (Continued)  Tracing the quality process  Record development phase evaluation results. Provide rationale for changes made to the ISD evaluation strategy during the development phase. Document lessons learned during evaluation of the development phase.   You should document the quality process to the point that it is traceable throughout the entire ISD process.  However, document only what is necessary, and no more.   AFH 36-2235 Volume 8 1 November 2002 165 Section F Update Training Development Plan  Introduction  Why update the plan?  Who is responsible?  What should be updated?   The training development plan is the "blueprint" for managing the training development process.  It is important for you, as a curriculum developer, to have a blueprint of how it all fits together as a training system.  The plan is a valuable management tool for you, as well as for managers in the training organization.   Updating the training development plan at the end of the development phase ensures that it will remain an effective tool for managing the training system.  Update the plan as necessary to reflect the most current status of training system development.   Keeping the training development plan updated is the responsibility of the curriculum developer.  Training organization management has the overall responsibility for ensuring that development of the training system is documented in the plan and the plan reflects the current status.   At the end of the development phase, the plan should be updated to include new or revised information, such as:  Changes to training development strategy New or revised milestones resulting from the development phase Refinements to project definition Changes in resource requirements and constraints Deletion or addition of information resulting from the development phase   AFH 36-2235 Volume 8 1 November 2002 166 Section G Finalize Training Materials Introduction  Purpose  Who is responsible?  What needs to be updated?  Quality checklist   After you have validated the training, you are ready to finalize the training materials.  During this step, you make sure that all necessary changes are made to the training materials and they are ready for implementation.   The purpose of finalizing training materials is to ensure that they:  Have been revised to include the most current and accurate information Are complete Are ready for training implementation   Curriculum developers and instructors are responsible for finalizing training materials and ensuring that they are ready for implementation.   When updating finalized training materials, you should update:  Plans that have been developed Course control documents Training materials activity   The following list of questions may help to ensure that everything is ready for implementation.  Training Development Plan  Has the training development plan been updated? Is the training development plan complete? Has the training plan been approved? Has the training development plan been distributed, as re-quired?  AFH 36-2235 Volume 8 1 November 2002 167 Quality checklist (Continued)  Training Standard  Has the training standard been revised/changed? Has the training standard revision/change been approved? Has the training standard been published? Has the course syllabus been updated? Is the course syllabus complete? Has the course syllabus been approved? Has the course syllabus been published and distributed?  Course Syllabus   Training Materials  Print Materials Have the student workbooks been updated? Are the student workbooks complete? Have the student workbooks been published? Have the instructor lesson plans been updated? Are the instructor lesson plans complete? Have the instructor lesson plans been approved and published?  Audiovisual Have the transparencies been updated? Are the transparencies complete? Are the transparencies ready for use? Have the slides been updated? Are the slides complete? Are the slides ready for use? Has the program been updated? Is the programming complete? Has the ICW been operationally tested?  ICW  AFH 36-2235 Volume 8 1 November 2002 168 Chapter 7 IMPLEMENTATION Overview Introduction  Where are you in the process?   At this point in the ISD process, the course has been validated and you are ready to implement the training.  As the curriculum developer, you need to ensure that everything is ready to support implementation prior to actually implementing the training. That is, the system functions are in place, adequate resources are available, the training system itself is ready, and you are prepared to continue the evaluation process.   An ISD model, with the implementation phase highlighted, is presented in Figure 13 to help visualize the process.  Figure 13 Implementation Phase  Objectives    The objectives of this chapter are to:  Evaluate training system functions. Develop and test final preparations for course implementation.Design training implementation activities. Implement an operational evaluation process.  AFH 36-2235 Volume 8 1 November 2002 169 Where to read about it        This chapter contains four sections.  Section A B C D   Title  Implement Training System Functions  Conduct Training Conduct Operational Evaluation Revise Courseware Page 170 177 183 207  AFH 36-2235 Volume 8 1 November 2002 170 Introduction  Where to read about it      Additional information  Section A Implement Training System Functions   It is unlikely that you, as a curriculum developer, will ever be required to implement the training system functions. These functions were probably put in place when the  training organization was established.  Your job, along with developing training, will be to ensure that these functions are being performed and to clearly understand how the system functions interface with the training system in order to support, operate, and maintain training courses.  The four system functions are management, administration, delivery, and support.   This section covers four topics.   Management Function Support Function Administration Function Delivery Function  Page 172 173 175 176 Topic  For additional information on training system management functions, see: AFMAN 36-2234, Instructional System Development, Implementation chapter. Bills, C. B. and Butterbrodt, V. L. (1992). Total Training Systems Design Function: A Total Quality Management Application. Wright-Patterson AFB, Ohio. Fishburne, R. P., Williams, K. R., Chatt, J. A., and Spears, W. D. (1987). Design Specification Development for the C-130 Model Aircrew Training System: Phase I Report. Williams AFB, Arizona: Air Force Human Resources Laboratory (AFHRL-TR86-44).   AFH 36-2235 Volume 8 1 November 2002 171 Additional information (Continued)   JWK International Corp. (1990). Final Training System Baseline Analysis Report (EWOT). Dayton, Ohio.  JWK International Corp. Williams, K. R., Judd, W. A., Degen, T. E., Haskell, B. C., and Schutt, S. L. (1987). Advanced Aircrew Training Systems (AATS): Functional Design Description. Irving, Texas: Seville Training Systems (TD-87-12).  AFH 36-2235 Volume 8 1 November 2002 172 Management Function Introduction  Definition  Who is responsible?  Management activities  Relationship to implementation   The management function is a key training system function.  The management function involves managing the entire training system, including management of the training development process and the day-to-day operation of the system.  It is essential that it be in place and working if the training system is to be operated effectively and adequately maintained.  Lack of effective management can result in failure of the training system.   The management function is the practice of directing and controlling all aspects of the training system from initial project planning to the day-to-day operation of the system.   Organizational management has the overall responsibility for seeing that the management function is in place and carried out in support of the training system.  However, some activities may be delegated to the curriculum developer.   Activities within the training system management function are:  Planning for the design, development, delivery, support, operation, and maintenance of the training system. Organizing resources, which includes identifying, arranging, and bringing together resources required for the training system, such as personnel, equipment, and facilities. Coordinating activities between training and support organizations, such as civil engineering, resource management, and audiovisual services. Evaluating the effectiveness and efficiency of each element of the project, such as personnel, budget, production, and graduates' performance. Reporting project status to organizational management or Headquarters (HQ).   Every aspect of the training system depends on the system management function.  All of its planning, organizing, coordinating, evaluating, and reporting activities work toward successful implementation of a training program.  Without this function, the system would be ineffective and inefficient.  AFH 36-2235 Volume 8 1 November 2002 173 Support Function Introduction  Definition  Who is responsible?  Examples of training support tasks   The importance of the support function in the training system cannot be overstressed.  In most cases, you will find that the support function already exists within the base or training organization.  Although the support function may already be established, each time a training system is developed, the support requirements for that specific course must be established to ensure that adequate support is available to support the system.   The support function can be defined as those long-range as well as day-to-day tasks performed by organizations in support of the training system.  Examples of support functions are:  Maintain equipment and facilities. Supply materials for the courses. Provide services such as audiovisual or publication.   As with the other training system functions, organizational management has the overall responsibility for ensuring that the training systems within the organization are adequately supported.  Curriculum developers must identify to management any lack of support for their training systems.   Base organizations perform many tasks in support of the training system.  Examples of some of the organizations and the tasks they perform are listed below.  Civil Engineering   Constructs training and support facilities such as classrooms. Modifies existing facilities, such as adding new electrical outlets, air conditioning.  Resource Management Provides personnel such as instructors, computer programmers, and graphic artists. Manages training and support equipment. Provides funding for day-to-day operation.  AFH 36-2235 Volume 8 1 November 2002 174  Examples of training support tasks (Continued)  Relationship to implementation    Information Management Edits training material, such as student workbooks, student study guides. Produces training material, such as student handbooks, course syllabuses.   Contracting or Contractor Logistic Support (CLS) Develops contracts for maintenance and other services. Processes local purchase forms to procure equipment, supplies.  Visual Information Develops and controls visual material such as slides, filmstrips, video. Manages visual equipment, such as televisions, VCRs, slide projectors.   Implementing a training system requires planning and preparation.  A part of that effort is to ensure that the support functions are in place and ready to support the system.  Every training system component requires some type of support or maintenance in order to effectively contribute to the training system.  AFH 36-2235 Volume 8 1 November 2002 175 Administration Function Introduction  Definition  Who is responsible?  Administration activities  Relationship to implementation   Often overlooked, the administration function plays a vital role in the day-to-day operation of a training system.  You may not be directly involved in any of the administrative activities, but you should still be aware of what is being done by other organizations to support and maintain the training system.   Administration is the part of management that performs day-to-day tasks such as typing support, personnel schedules, and student records.   Organizational management has the overall responsibility for ensuring that the organization has an active administration function to support the training system.  However, curriculum developers have the responsibility for ensuring that each of the courses they are responsible for receives adequate support.   Administration activities include items such as:   Providing documents such as course syllabuses, instructor and student materials. Maintaining personnel, training, and equipment records. Typing reports, letters, and messages.  Administering student support, which includes tasks such as processing student records, mailing courses to students. Administering staff support tasks such as leave processing, preparation and maintenance of personnel records, and ad-ministration of personnel programs. Scheduling resources such as scheduling students for classes and establishing equipment utilization schedules. Tracking students and equipment.   The training system administration function must be in place before you can successfully implement a training system.  For example, training materials must be produced and available, reports typed, exportable courses mailed, and student records updated.   Remember, you always need administrative support for your training system. AFH 36-2235 Volume 8 1 November 2002 176 Delivery Function Introduction Definition Who is responsible?  Relationship to implementation    As curriculum developers designed the training system, special care was taken to ensure that the appropriate delivery method was selected for the course.  Now that the system is ready to implement, they must ensure that the delivery function is ready to support the system.  The delivery function, like the other two functions that have been discussed, is also critical to the training system.     The delivery function is defined as the means or methods by which training is provided to the students.  Examples of delivery include:  Instructors Computers, which include ICW, CAI, or CMI Training devices including simulators, part-task trainers, and mock-ups Satellite Programmed text Exportable courses Job aids   Within a training organization, there are several individuals responsible for the training delivery function:  Organizational managers – responsible for ensuring adequate planning and analysis before the delivery method is selected.  Once the method has been selected, management must make sure there is adequate support. Curriculum developers – responsible for selecting the appropriate method for training delivery. Instructors – responsible for using the selected method and evaluating the effectiveness of it.   Simply stated, without the delivery function the training cannot be implemented.  The delivery system is the vehicle for getting the training to the students.  AFH 36-2235 Volume 8 1 November 2002 177 Section B Conduct Training Introduction  Where to read about it     Additional information  To this point, organizational management has spent considerable time planning the training system, securing resources, managing the training development process, and arranging for system support.  Curriculum developers have analyzed the tasks, designed the training system, and developed and validated the training.  Now the course is ready to implement.  During implementation, management continues to plan and manage the training system while the curriculum developers provide the training and continually evaluate its effectiveness.   This section covers two topics.   Preparations for Conducting Training Conducting Training   For additional information on conducting training, see AFMAN 36-2236, Handbook for Air Force Instructors.  Page 178 181 Topic AFH 36-2235 Volume 8 1 November 2002 178 Preparations for Conducting Training Introduction  Why prepare?  Who is responsible?   Although preparations for conducting training started in the initial planning and have continued throughout the analysis, design, and development phases of ISD, final preparations need to be made prior to implementing the course.  Adequate preparation is essential to successful training.   Adequate preparations contribute to the overall success of a training program, while inadequate preparation can result in its complete failure.   Everyone involved in the support, operation, and maintenance of a training system shares some of the responsibility for making last-minute preparation to implement the training system.  For example:  Instructors must ensure that:         Classrooms are ready. Training materials are available in adequate quantities. All equipment is available and operational. Curriculum developers must ensure that: Course syllabuses are complete. Exportable training packages are ready. Organizational managers must ensure that:  Resources are available and ready for implementation (i.e., equipment, facilities, and personnel). Training system functions are ready to support implementation. Instructors have been trained and certified. Support organization must ensure that: Support and services are available to support implementation of training. AFH 36-2235 Volume 8 1 November 2002 179 When should you prepare?  What should be checked?   Throughout this handbook, adequate planning and preparation have been a constant theme.  However, at this point, the focus is on the last-minute preparations that may be required before the training is implemented.  If possible, before the training is implemented, check each component of the training system to make sure everything is ready.   The type of training system being implemented will determine what should be done to get ready to implement the training.  For example, an exportable course will not have the same items to be checked as a course taught in a classroom environment.  Some of the last-minute preparations may include checking to ensure the following:  Equipment Training and support equipment is available in adequate numbers and in operational condition. Equipment support, such as maintenance, is available. Facilities Training and support facilities are available. Any facility modifications such as electrical and air conditioning are complete. Is a "back-up" system available if the primary system is unavailable or not usable? Personnel Personnel are available, including qualified instructors, administrative and maintenance personnel. Students have been scheduled for the classes. Money Time Adequate funds are available to meet implementation costs and the costs associated with the daily operation of the course. Curriculum developers have had adequate time to develop effective and efficient training. Instruction has been scheduled efficiently and effectively.     AFH 36-2235 Volume 8 1 November 2002 180 What should be checked? (Continued)    Materials and Supplies Training materials are available in adequate quantities to support training. Training and office supplies are available in adequate quantities to support training implementation. AFH 36-2235 Volume 8 1 November 2002 181 Conducting Training Introduction  Why conduct training?  Who is responsible?   If adequate preparations have been made throughout the training development process and a last minute check of each training system component was made, there should be few problems, if any, encountered when implementing the training.  However, conducting the training remains a critical phase of ISD.   The purpose of conducting training is to impart the necessary skills, knowledge, and attitudes required for the graduate to meet job performance requirements.   There are many individuals and organizations directly or indirectly involved in conducting training.  Each has the responsibility for performing their respective tasks.  For example:  Organizational managers have the overall responsibility for the daily operation of a training system.  This responsibility includes managing the support, operation, and maintenance of the system.  Support organizations have the responsibility of providing the necessary logistic support and services to support, operate, and maintain the system.  Curriculum developers have the responsibility for designing and developing the training and continuously evaluating its effectiveness and correcting identified deficiencies, as applicable.  AFH 36-2235 Volume 8 1 November 2002 182 Conducting the training     Once the training is implemented and becomes operational, it will normally remain so until there is no longer a need for the training.  When conducting training, the focus is on conducting effective and efficient training that produces graduates who can meet job performance requirements.  During the operation of the training system, ongoing activities ensure system integrity, as listed below.  Resource management is probably the single most critical issue for training organization management as well as curriculum developers.  Resources must be well managed to gain maximum benefit.  Staff development is an activity that goes on continually while training is being conducted.  Curriculum developers as well as other training staff should periodically attend courses that help them develop professionally in curriculum development.  Conducting training is the center of system integrity.  No matter what has been done to this point, the training system can fail or be rendered ineffective or inefficient if the training is not properly conducted.  Some items that are helpful to remember when conducting training are:  Training should always be student-centered. Training should always follow the course syllabus. Curriculum developers or instructors must always perform professionally in the teaching/learning environment.   Training staff should be qualified to perform their duties.   Evaluation maintains the quality of training implementation.  During the conduct of training, operational evaluation is continually performed to ensure the quality of a training program.  It is covered in detail in the next chapter of this handbook. AFH 36-2235 Volume 8 1 November 2002 183 Section C Conduct Operational Evaluation  Introduction  Objectives  Where to read about it       After the formative and summative evaluations have been completed, and the system functions are in place, the instructional system is ready for implementation.  Once the system is implemented and starts producing graduates, it's time to begin conducting operational evaluation.  Operational evaluation is a continuous process that assesses how well course graduates are meeting the established job performance requirements.   The objectives of this chapter are to:  Analyze the operational evaluation process. Plan and execute internal evaluation. Plan and execute external evaluation. Topic   This section covers three topics.   Operational Evaluation  Internal Evaluation External Evaluation  Page 184 186 193 AFH 36-2235 Volume 8 1 November 2002 184 Operational Evaluation Introduction  Definition  Purpose  What should you look for?   Evaluation is a continuous activity that is integrated throughout each stage of ISD, beginning with analysis and continuing throughout the life cycle of the system.  It focuses on quality improvement and the graduate's ability to meet job performance requirements.  The last stage of the evaluation process is operational evaluation.   Operational evaluation is the process of gathering and analyzing internal and external feedback data to ensure that the system continues to effectively and cost-efficiently produce graduates who meet established requirements.  It is a quality improvement activity.   The two main purposes of operational evaluation are to:  Ensure that graduates continue to meet established job performance requirements. Continually improve system quality.   When evaluating, look for both strengths and weaknesses in the system.  Focus on:  How well graduates are meeting job performance requirements. Whether unneeded training is being provided. Whether any needed training is not being provided. How well each system component is contributing to overall system quality. Ways to improve the graduate's performance as well as the system.  AFH 36-2235 Volume 8 1 November 2002 185 Operational evaluation activities  Additional information   The two operational evaluation activities are:  Internal evaluation – gathers and analyzes internal feedback and management data from within the training environment to assess the effectiveness and quality of the training process. External evaluation – gathers and analyzes external feedback data from the field to assess graduates' on-the-job performance in an operational environment.   For additional operational evaluation information, see:  AFMAN 36-2236, Handbook for Air Force Instructors. Briggs, L. J. and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.).  Glenview, Illinois: Harper Collins Publishers.  AFH 36-2235 Volume 8 1 November 2002 186 Internal Evaluation Introduction   Definition  Purpose  Possible causes for problems   Internal evaluation activities begin with implementation of the training system and continue throughout the life cycle of the training system.  Some organizations call this evaluation activity a "course review."  Internal evaluations look at the instructional system from within to determine system effectiveness and quality.  Internal evaluation is the acquisition and analysis of internal feedback and management data, such as test data, student critiques, instructor comments, and data correlation from within the training system.   The purpose of internal evaluation is to improve the effectiveness and quality of the training system.   Although training systems are validated prior to implementation, students may still have difficulty with the training during day-to-day system operation.  Possible causes of problems are:  Instructors do not follow the course syllabus. The developed course is different from the course that is actually implemented. Resources required to support, operate, and maintain the system are inadequate. Training materials are not correlated. Students do not meet course requisites.  Periodic internal evaluations will identify weaknesses (problems) as well as strengths of the training system.  Internal evaluations should be conducted with sufficient frequency to ensure the quality of the training system.  For example, a mature course that is seldom changed would not likely require an internal evaluation as often as a course that is constantly being updated or revised.  Some training organizations conduct internal evaluations annually on all courses, regardless of their status.  AFH 36-2235 Volume 8 1 November 2002 187 Data collection Data Collection Methods Review Course Control Document Review Resources  Visit Training Facilities Evaluate Instructor Performance Evaluate Student Performance Monitor Measurement Program  Conducting an internal evaluation    Several methods of collecting internal evaluation data are explained below.     Purpose To determine if there are any discrepancies between the planned course and the course that was actually implemented. To determine if resources are available and adequate: Facilities (training and support). Equipment (training, support, and test) and supplies. Personnel (curriculum developers, instructors, students). Time (course length, sufficient time to maintain course). To evaluate the quality of implemented training (ensure that the visit is long enough to observe representative instruction). To check equipment, training media, training aids and devices for condition, operation, and appropriateness. To check training literature such as study guides and workbooks for quality and availability. To check if instructor:  Follows the syllabus, uses training media properly, responds to student needs, and is qualified to teach. Noted weaknesses on the evaluation forms and if they have been corrected. To determine if students are meeting standards. To monitor flying and simulator missions. To ensure quality. To check the measurement program for compromise.  If a test has been compromised, it cannot provide useful feedback. Evaluate training in terms of student performance.  Use performance measures to determine students' achievement of objectives.   Collect sufficient internal evaluation data for the analysis.  Insufficient data will skew the analysis results; possibly leading to incorrect decisions being made.  An example of a job aid used to gather internal evaluation data is shown below.   AFH 36-2235 Volume 8 1 November 2002 188  Conducting an internal evaluation (Continued)                                            Check   Data Source Does the course syllabus reflect the operational course? Is the course syllabus current and accurate? Does the course syllabus provide adequate guidance? Does the lesson plan and course syllabus agree? Does the lesson plan reflect what is being taught in the course? Is the lesson plan current and accurate? Do training materials support the lesson plan and course syllabus? Do training facilities meet system requirements? Do support facilities meet system requirements? Does training equipment meet system requirements? Is the training equipment adequately maintained? Does support equipment meet system requirements? Are instructors teaching according to the lesson plan? Are instructors adequately trained? Do tests adequately measure the objectives? Is the test data thoroughly analyzed? Check grade sheets–are students meeting standards in the trainers (simulators, part-task)? Check grade sheets–are students completing all elements to stated standards? Are failed rides excessive? Can improvements be made in the course? AFH 36-2235 Volume 8 1 November 2002 189 Student reaction     The following are examples of questions designed to obtain student feedback.  A questionnaire using this information should also include a section for identification (period / date / instructor / learner) and a section for comments, explanations, or recommendations.   1.  Prior to this training, my experience in this area was     2.  Did your knowledge of the subject increase as a result of the __________   extensive __________  moderate __________ little or none training? __________ yes __________  no    3.  If your knowledge increased as a result of the training, to what extent did it increase? __________  not applicable (my knowledge didn't increase) slightly too advanced __________  __________  about right __________ too elementary __________ __________  moderately __________  extremely     4.  Based on my experience, the level of training was     5.  The organization of the training was     6.  The lecture outline (main points of training) was      __________ __________  helpful __________  not very helpful __________ __________  helpful __________  not very helpful very helpful very helpful AFH 36-2235 Volume 8 1 November 2002 190  Student reaction (Continued)   valuable __________  meaningful __________ __________  not helpful __________  not applicable (no questions asked) somewhat helpful __________  of great value __________ __________  of little or no value __________  not used, but could have helped __________  not used and not needed   7.  Audiovisual aids were       8.  Answers to student questions were      9.  Should the subject matter covered be changed?    10.  Should the method of training be changed?  yes (please explain below)   11.  Overall, the training was      12.  Instruments (including tests) to evaluate student performance __________  outstanding __________  good __________ fair __________  poor __________ __________  no  yes (please explain below) __________ __________  no were __________  outstanding __________  good __________ fair __________  poor       AFH 36-2235 Volume 8 1 November 2002 191 Data analysis   Before beginning analysis of the data, ensure that:  Data has been collected from each component of the training system. Adequate data samples are collected in order to validate the reliability of the findings.  Following are some methods of analyzing the internal evaluation data.  Compare the training standard with the course syllabus to determine if the requirements of the standard are being met. Compare course syllabus with operational course to determine if the planned and operational courses are the same. Review course syllabus, lesson plan, and training material to determine if they are current, adequate, and in agreement. Compare stated resource requirements with actual resources to determine if adequate resources are available to support, operate, and maintain the training system. Review records to determine if instructors are qualified to teach the course. Review test data to ensure that students are meeting course objectives. Analyze test data to determine if test items are valid and reliable. Review grade sheets to ensure that students are performing to standards (check instructor's comments). Review failed rides report for indications of a problem and whether problem is related to academics, trainers, flying missions or the student's inability or unwillingness to complete the course.  This may require an investigation of the student's grade book and interviews with instructors.  AFH 36-2235 Volume 8 1 November 2002 192 Revising the training system   After internal evaluation data is collected and analyzed, the next stage is to correct deficiencies in the training system.  If revisions can be made to correct identified problems, they should be made in a timely manner in order to receive the greatest benefit from the changes.  Revisions resulting from the analysis may require reentry into an earlier phase of the ISD process to correct the problem(s).  The need to reenter an earlier phase of ISD is determined by the nature and scope of the revision.  For example, changing a test item or adding time to a unit of training would not require reentry.  However, adding a new piece of equipment to the course would more than likely require you to do so.    Section D in this chapter discusses revising courseware.  AFH 36-2235 Volume 8 1 November 2002 193 External Evaluation Introduction  Definition  Purpose  Possible causes for problems  Collecting data       How well graduates meet job performance requirements is learned through external evaluation.  This evaluation activity relies on input from the field to determine how well graduates are performing.   External (field) evaluation is the process of gathering and analyzing data from outside the training environment in order to determine how well recent graduates are meeting job performance requirements.   The purpose of external evaluation is to determine if recent graduates of the course: Can meet job performance requirements. Need all of the training they received. Need any training they did not receive.   Some possible problems that may be identified during external evaluations are: Criterion test did not measure graduates' ability to meet job performance requirements. Objectives do not reflect job performance requirements. Job performance requirements were incorrectly identified during task analysis. Job performance requirements changed after task analysis. Sequence of training did not take into account the task decay rate.   Several methods of collecting external evaluation are listed below.    Methods of External Evaluation Questionnaires Field Visits Other Sources of Evaluation Input  Page 194 203 206 AFH 36-2235 Volume 8 1 November 2002 194 Questionnaires Introduction Purpose Advantages   Disadvantages    Questionnaires are effective, cost-efficient evaluation tools. The discussion on questionnaires will focus on:  Advantages and disadvantages of questionnaires Types of questionnaires  How to prepare and distribute questionnaires Analysis of data gathered using questionnaires   The purpose of using questionnaires is to:  Determine the ability of recent graduates to perform specific tasks on which they received training. Identify the specific nature of any deficiency. Determine what tasks graduates are actually performing. Identify what training is not needed for on-the-job performance.   The advantages of questionnaires are:  They are comparatively inexpensive to administer. They can be used to collect large samples of graduate and supervisor data. They yield data that can be easily tabulated and reported.   Respondents give their opinions freely.   Disadvantages of questionnaires include:  They may not be the most reliable form of evaluation–data validity depends on preparation and distribution. Communication is one-way–respondent may not understand some of the questions. They may not ask the most relevant questions. They collect only opinions, which may not be as reliable as other methods of collecting external data. Developing effective and reliable questionnaires may be costly and require extensive experience. Low return rates and inappropriate responses affect accuracy.  AFH 36-2235 Volume 8 1 November 2002 195 Types of questionnaires  Preparing questionnaires        Two types of questionnaires can be used to collect external evaluation data.  One is for the graduates' immediate supervisor.  However, responding may be delegated to the graduates' trainer. The other questionnaire is for the graduates.  This questionnaire is designed to find out what graduates think about the training they received.   Well-constructed questionnaires that are properly administered are extremely important to the field evaluation process.  The following table identifies the five basic stages of questionnaire development.  Stage   Activity 1  2  3   Define purpose of questionnaire.  Focus only on relevant information. Determine specific information to be collected.  Specify exactly what is needed in a list of objectives. Develop questions that ask for specific information: What conditions/equipment are required to do the job. Exact action to accomplish the performance.Standards of performance. Results of the performance. AFH 36-2235 Volume 8 1 November 2002 196 Preparing questionnaires (Continued)  Stage  4    Activity   Guidelines for developing questions   5   Consider motivational factors when developing questionnaires.  You want the respondent to answer fully and conscientiously.  Questionnaires will motivate if you: Explain the purpose of the questionnaire. Tell the respondents how they can benefit from answering the questionnaire. Write clear and concise instructions. Make questionnaire format uncluttered and easy to answer.  For example, using boxes for check marks will make the questionnaire easier to answer. Arrange the questionnaire in logical order. Ask specific questions. Test the questionnaire on sample respondents.  Ask them to: Evaluate the cover letter. Check instructions and questions for clarity. Explain how they feel about answering the questions. Revise the questionnaire, if necessary, before distribution.  Note: Questions can be taken directly from the task statements in the standard.   Guidelines for developing effective questions are: Use closed-end questions when you want the respondent to choose answers from a small number of possibilities.  This makes tabulation easy but may not give the range of answers desired. Use open-end questions when you don't know all the possible answers.  The respondent will probably suggest possibilities. Word questions to the respondent's level of understanding.  Use vocabulary and concepts that are easy for the respondent to understand. Limit each question to one aspect of a topic. AFH 36-2235 Volume 8 1 November 2002 197 Guidelines for developing questions (Continued)  Guidelines for constructing questionnaires    Decide on the logical order of the questions (task order, general to specific).  Each question increases the respondent's frame of reference and further establishes upcoming responses. Avoid questions that make it easier to answer one way or another. Avoid questions that show biases or exceptions. Word questions so they will not threaten the respondents. Supplemental "information-seeking" questions may be used.  Such questions may ask how much time the graduate spends on individual tasks or what equipment or materials the graduate uses.  When constructing a questionnaire, several factors should be considered. Provide short, concise, and specific directions for completing the questionnaire.  The directions should be printed in heavy, bold type, if possible. Provide space for the respondent's name, title, organization, and location. Number the questionnaires to allow for administrative control. Whenever possible, allow the respondent to use the same type marking for all questions.  For example, one of the best methods is to allow use of check marks for responses. Arrange "yes" and "no" responses vertically rather than horizontally. Correct  Yes  ___ ___ No     Incorrect   Yes  ___ No  ___ Number each page of the questionnaire. The questionnaire should be easy to read and mark, and should be printed. Print on both sides of the pages to conserve materials, if possible. •  Continued on next  page      AFH 36-2235 Volume 8 1 November 2002 198 Guidelines for constructing questionnaires (Continued) Guidelines for preparing cover letters  Before you distribute the questionnaire  Distribution of questionnaires    Send self-addressed return envelope with the questionnaire. Fold the questionnaire in such a manner that the respondent can refold it the same way to place it in the return envelope after completion.   Each questionnaire should have a cover letter.  The cover letter should:  Explain the purpose of the questionnaire and its importance to improving training. Include a statement that assures the respondent that the information will be treated confidentially. Provide information on how to return the questionnaire. Indicate the approximate time required to complete the questionnaire. Show the date the questionnaire was mailed and the recommended return date. Use appropriate letterhead stationery signed by a responsible authority.   Before distributing the questionnaire, it should be administered to a small number of select individuals to:  Provide valuable feedback on the quality of the questionnaire. Prevent acquiring misinformation resulting from the administration of a faulty questionnaire. Allow correction of problems in the questionnaire before distribution.   Distribution of the questionnaire is a critical aspect of external evaluation.  You just don't pick a few graduates' names and drop a questionnaire in the mail to them.  You plan distribution to ensure that the data collected is valid and reliable.  When distributing the questionnaire, you should:  Decide to whom you are sending the questionnaire–recent graduates, their supervisors, or both.  You may collect important information from both. AFH 36-2235 Volume 8 1 November 2002 199      Select a representative sample to ensure valid results.  Graduates may perform different tasks or their job requirements may vary depending on the major command, geographic location, or organization level.  Therefore, questionnaires should be distributed to each area as evenly as possible. Decide when to distribute the questionnaires.  Timing is critical.  Usually, questionnaires should be sent to the graduates within three to six months after graduation.  Beyond six months, it may be impossible to tell whether the graduate learned the skill or knowledge in the course or on the job.  If the questionnaire is sent too early, the graduate may not have had time to perform many of the tasks that were taught in the course. Determine how many questionnaires you need to mail out.  That decision is based on: Expected response rate. Level of confidence (a statistical consideration which means the size of the sample required for you to be, say, 95 percent sure the sample truly represents the larger population).  The graduate sampling chart on the following page shows how to determine the number of questionnaires you need based on this consideration. Develop points of contact at the gaining units who can administer the questionnaires and ensure that they are completed and returned.  Note: To ensure that sufficient numbers of the questionnaires are returned for analysis, contact nonrespondents and encourage their response.  Distribution of questionnaires (Continued)          AFH 36-2235 Volume 8 1 November 2002 200 GRADUATE SAMPLING CHART Course Graduates (During Sampling Period) 10 20 40 60 80 100 120 160 200 250 300 350 400 450 500 600 700 800 900 1,000 1,500 2,000 2,500 3,000 3,500 4,000 4,500 5,000 10,000 25,000 100,000    Sample Size 95% Confidence* 10193652678092114133154171187200212222240255267277286316333345353358364367370383394398 Sample Size 90% Confidence 10193549627383101115130142153161169176186195202208213229238244248251253255257263268270 Sample Size 80% Confidence 9183244546269819099106112116120123129133136139141148151154155157157158159161163164AFH 36-2235 Volume 8 1 November 2002 201 Distribution of questionnaires (Continued) Data analysis   HOW TO USE THIS TABLE  The table can be used as shown in the following example:  Annual course production is 4,000 - 95% confidence level desired.  Estimated return rate of usable questionnaires is 85%.  From the table, 364 usable questionnaires are required.  Therefore, this figure should be 85% of the questionnaires to mail out.  The number of questionnaires to mail is computed as follows:                        85    * It is recommended that the 95% confidence level be chosen.  This is the level commonly used in business decisions.  X = 364 x 100 = 404 = number of questionnaires to mail  85% = 364 100%     X   When a sufficient number of completed questionnaires have been returned, you should begin analyzing the data.  In this process, the data is:  Compiled Collated Analyzed (data from each command should be analyzed together)  Pay special attention to:  Notes made by respondents on the questionnaires   Answers to supplemental questions that were included in the questionnaire  Use with caution any data that contain such obvious errors as:  Halo effect – indiscriminate rating of all items positively Central tendency – indiscriminate rating of items in the center of the scale  Examine the responses to ensure, insofar as possible, that the information accurately reflects the opinion of the graduates and their supervisors.  AFH 36-2235 Volume 8 1 November 2002 202 Reporting findings   After completing data analysis, the findings should be reported.  The report should include information such as:  Background information on the course that was evaluated Scope of the evaluation Tasks evaluated Analysis results Recommendations Milestones for corrective actions, if applicable  Now that the report is complete, your last action is to distribute the report.  AFH 36-2235 Volume 8 1 November 2002 203 Field Visits Introduction  Purpose Advantages  Disadvantages  Field visits are a very effective method of conducting external evaluations.  An evaluator, often assisted by a curriculum developer or instructor, normally conducts them.  Ideally, field visits should include specialists who are familiar with the graduates' jobs.  However, in most cases this is not possible due to limited TDY funds, scheduling constraints, and number and variety of graduates to be interviewed.   The purpose of a field visit is to get first-hand information on the graduates' assignment, utilization, and proficiency on the job, and to validate information gained from other evaluation activities.   Advantages of field visits are:  Guidance and information about the evaluation is given directly to graduates and supervisors. Information is gathered first-hand by the evaluator.  Any questions or assumptions can be clarified. Field visits help validate questionnaire data. External evaluations build rapport between the training activity and the user. Additional information can be gained by observing nonverbal messages and asking leading or probing questions.     Disadvantages of field visits are: They are time-consuming.  Travel to several different bases requires considerable time.  Interviews and observations also require a lot of time if they are done correctly. The sample is limited.  Since the evaluator only visits a few bases, the number of interviews and observations conducted is limited. AFH 36-2235 Volume 8 1 November 2002 204  The cost is high.  Field visits require evaluators to spend limited TDY funds to travel to the various bases. Information gathered by the evaluator can be subjective and biased. Graduates may feel they are being scrutinized. Evaluators are not always skilled at interview and observation.Disadvantages (Continued)  Data collection  Preparing for the field visit  Conducting field visits   Two methods of collecting data are:  Interviews Observations  Evaluators should interview recent graduates and their supervisors and observe the graduates' on-the-job performance when possible.  However, observations are almost useless unless the observer is familiar with the tasks being performed.   Visits to the field to collect evaluation data should be adequately planned.  Adequate planning will ensure that useful data is gathered.  To prepare for the visit, you should:  Develop a list of questions to get honest, pertinent answers and keep the discussion focused. Determine the bases to be visited. Establish the schedule for the visit. Select the individuals to be interviewed and observed.   Tasks to be performed during the field visit are:  Inform graduates and supervisors of the purpose of the visit.  Tell them that their answers will furnish valuable information for improving the training. Interview the recent graduates and their supervisors.  Supervisors should know how well the graduate has performed on the job.   Determine the graduates' proficiency. Determine how the skills learned during training are being used.  AFH 36-2235 Volume 8 1 November 2002 205 Conducting field visits (Continued)   Data analysis  Reporting findings  Find out how the graduates are progressing on OJT. Guide the interviews with your list of questions.  (As the interview progresses, you may need to add, delete, or revise questions.) Take accurate and complete notes, especially on information that is freely given. Have the supervisor rate the graduates' performance.   Observe graduates perform tasks.  This may not be possible if the evaluator does not have job or task knowledge.  Take careful notes on the performance.  After the task has been completed, ask questions to clarify any of the actions taken by the graduates during task performance.   Data collected from interviews and observations is analyzed in the same manner as questionnaires–that is, compiled, collated, and analyzed by major command or defense system.   The results of the field visits and questionnaires should be combined and reported.  The information gathered during field visits is not normally used or reported independently.  The analysis results of the questionnaires and field visits are compared in order to validate the findings.  AFH 36-2235 Volume 8 1 November 2002 206 Other Sources of Evaluation Input Other data sources  Revising the training system    Other external data sources that can be used to evaluate the graduates' job performance are:  Inspection team (IG) reports – AF and MAJCOMs periodically inspect  instructional activities to determine their effectiveness.  Other inspections conducted by these teams may also discover related problems.  Use this source of data to determine if graduates are meeting their job performance requirements.  Take appropriate action to correct deficiencies.  One example of an IG report is the Functional Management Inspection (FMI).   Standardization/evaluation team findings – Standardiza-tion/evaluation teams periodically inspect training activities to determine their effectiveness.  Analyze findings indicating a problem and take appropriate action to correct the deficiency. Accident board reports – These reports provide data that determine cause of accidents and recommendations.  Often inadequate training or conflicting data in the syllabus-related documents is identified as contributing to the cause.  These reports can be included in the training program to motivate students to adopt a correct pattern or technique or discontinue an incorrect one.   After the external evaluation data is collected and analyzed, the next stage is to correct the deficiencies in the training system, once the corrections have been coordinated and approved.  If revisions can be made to correct identified deficiencies, they should be made in a timely manner in order to receive the greatest benefits from the changes.  The next section of this chapter discusses revising courseware.  AFH 36-2235 Volume 8 1 November 2002 207 Section D Revise Courseware Introduction  Outcomes of courseware revisions  Change/update process  A major portion of the aircrew training ISD activity is the review and revision of existing courseware.  The training environment is constantly changing and the courseware must be changed accordingly.  In order to guard against obsolescence, it is necessary to periodically review all units of instruction and to revise the courseware as required.  Through this review each phase of the ISD process is reached.  Courseware revisions can be initiated at any level within the aircrew training organization.   The outcomes of courseware revisions are numerous.  For example:  Task list and Established Training Requirements (ETRs) are brought up-to-date. Objectives and tests are brought up-to-date. Courseware control documents are updated. Media are updated or changed. Instructor guides are brought up-to-date. Sufficient copies of the coursebook are on inventory. Academics and flying squadron learning center libraries are updated. Total time spent in revising each unit of instruction, e.g., SME, Curriculum Development Manager (CDM), and administrative time, is reported. Courseware revision checklist is completed.   The change/update process used to revise courseware may vary depending on the nature and scope of the revision and the training organization itself.  An actual change/update process used by an aircrew training organization is provided in Example 1.  Another example of the change/update process is included in Example 2.  AFH 36-2235 Volume 8 1 November 2002 208 Example 1:  Change/update process   SME:  CDM:  Prepare course change/update using redline copy of coursebook.  Start the redline copy when courseware is printed so it is available for posting when an error or typo is found by you or the students. Initiate Courseware Revision Checklist (see page 211), recording reason for revision. Record time spent on the revision checklist. Plan for word processing turnaround time.  Allow two weeks for initial drafting (assuming 100-page coursebook). Complete items 2-7 of the revision checklist.  Be sure to review to determine if classified material has been used anywhere in the courseware. Prepare legible redline copy of courseware in proper format with errors and typos corrected. Deliver redline copy to CDM and review it with CDM. After word processing, review, proofread and edit. Approve final product before master copy is prepared for reproduction. Review printed copy to ensure that text and illustrations are legible. Monitor inventory. Attend monthly crew specialty instructor meetings. Coordinate with SME on all matters affecting courseware and record time spent on a Time Accounting Record Sheet. Receive Courseware Revision Checklist and review redline copy with SME, checking format, legibility of draft, and posting of changes. Submit request for updating graphics/photos as required. Ensure that errors and typos are corrected and then submit to Administration for word processing using AF Form 1509, Word Processing Work Request (Appendix A). Record date submitted and date returned on revision checklist. Review all materials for completeness and accuracy.  Record date on checklist if returned to word processing. Have SME review, proofread, and edit as required.  Record date of review on checklist, as well as date returned to word processing, if applicable. AFH 36-2235 Volume 8 1 November 2002 209 Example 1:  Change/update process (Continued)   Example 2: Change/update process  CDM (Continued): Record date on checklist of decision by you and SME to accept final product. Prepare Master copy and record date on checklist.  NOTE:  Cover, illustrations, text, etc., should all be original or copy prepared by graphics or photo lab.  Prepare Printing and Collating Sequence form, and submit with master copy to ISD Tech.  Record on checklist the date Master copy is sent to reproduction. If courseware has not been delivered within two weeks, check with ISD Tech. Ensure that copies of revised coursebook have been sent to flying squadrons for learning center library update.  Total SME time spent for the unit of instruction and report it to the Management Analysis Tech.  ISD Tech: Review courseware to ensure that the Master copy is ready for reproduction and that the ISD process is complete. Prepare DD Form 844, Request for Local Duplicating Service. Pick up Master copy at pubs office and return it to the CDM. Post and maintain redlined courseware. Conduct operational evaluation of redlined courseware.  Submit formal courseware change plans (design) Review redlined courseware. Review operational evaluation data.  Revise courseware (development) Modify courseware.  Review development process and products.  Randomly review courseware during the revision process. Advise personnel in application of ISD process as required.  CEM:   Collect data (analysis) AFH 36-2235 Volume 8 1 November 2002 210 Example 2: Change/update process (Continued)  Courseware revision checklist    Conduct training (implementation) Implement changes in courseware. Perform configuration activities. Conduct operational evaluation.   Once a revision cycle is initiated, it is suggested that a checklist be used to ensure that the revision process is effectively managed.  An actual courseware revision checklist is provided on the following pages as an example.  AFH 36-2235 Volume 8 1 November 2002 211  COURSEWARE REVISION CHECKLIST #  Item OPR D05C 1.  Reason for revision   change #________________________ a.  Inventory depleted, date ______________________ b.  Technical Order, name _______________________   c.  Regulation, name ___________________replaced  ______________________________ d.  Manual, name ______________________________   replaced by_________________________________ e.  Other (specify)  by          SME notify CDM of change and reason. Date CDM notified  Has SME completed items 2-7 of this checklist insuring that changed material is drafted into a red line copy of courseware?   1._____________   2._____________3._____________  4._____________   5._____________6._____________     Date materials sent to CDM for review and word processing.         Date materials sent by CDM to word processing.         Date materials return from word processing.       1. _____________2._____________3._____________  4. _____________5._____________6._____________ 1. _____________2._____________3._____________  4. _____________5._____________6._____________  Date YES  NO  NA                   AFH 36-2235 Volume 8 1 November 2002 212 COURSEWARE REVISION CHECKLIST (Continued) OPR D05C Date YES  NO  NA                                            Item #  Materials review for completeness of word processing and returned to:  a.  D05 CDM for completeness and accuracy b.  SME for review, proofreading, and editing. 4._______________5._______________6._____________ 1._______________2._______________3._____________                Date of decision on final product. ________________ SMS 1._______________2._______________3._____________ 4._______________5._______________6._____________   __________________ CDM _________________    Date Master created. ______________________________    Date materials sent to reproduction. ___________________  2.  Courseware review  Is all changed/corrected material included, ERRATA, date changes? Does Table of Contents agree with the lesson codes, titles, and page #'s? Do the materials for a specific lesson meet the objective(s)? Do the sample Criterion Test items reflect the criterion test question type? Are the lesson procedures correct? Are all required lesson resources listed? Are all exercise items referenced? Are there sufficient instructions to complete the exercises? Are the answers to the exercises included in the coursebook? Are there enough answer blocks/blanks and can they contain a write-in answer? Is the revised material complete and accurate?      AFH 36-2235 Volume 8 1 November 2002 213 COURSEWARE REVISION CHECKLIST (Continued) OPR D05C # Item  Note: Be sure to review to determine if classified material has been used anywhere in the courseware. 3.  Education/Training Requirements review    Have necessary changes/additions been made? Do the objectives in the course book and on ETR pages read word-for-word the same? 4.  Media review    Have the required changes been made to scripts? Have arrangements been made to create or change tape/slide packages? Have instructions been given for use of tapes with out-of-date materials? 5.  Criterion Tests review     Are any test items to be changed? Are the test instructions complete and correct? Are the answer keys correct and properly referenced? 6.   Instructor guide review  Is any material to be changed? Are the objectives, lesson procedures, and resources listed and do they agree with the coursebook(s)? Is there a separate page for each lesson start? 7.  a.  Rechecked contents for classified materials?   Name ______________________________     Date ______________________________   b.  SME log time spent in revising this courseware  (see item 10). 8.  CDM Master Construction    Is the date and title on the cover correct? Is the utilization letter current? Are correct symbols used for the lesson identifier on the title page?   Does each lesson start on an odd-numbered page?   Has a printing and collating sequence sheet been made up? Date YES  NO  NA                                                                         AFH 36-2235 Volume 8 1 November 2002 214 COURSEWARE REVISION CHECKLIST (Continued) #  Item OPR D05C 9.  General  Were two copies of the latest academic coursebooks sent to the flying squadrons to maintain their learning center library?  (CDM) SUMMARY OF CHANGES: Date YES  NO  NA               10.  Cost Accounting:  SME indicate time to nearest tenth (.10) of an hour spent revising the unit of instruction.  Label time by grade (O- /E-) and process such as writing, reviewing/editing, graphics and reproduction. Pay Grade  Writing       Editing    Graphics  Reproduction  Other          AFH 36-2235 Volume 8 1 November 2002 215 Chapter 8 EVALUATION Overview Introduction  the process? Where are you in   Evaluation is integrated throughout each activity of the instructional development process.  It starts in the planning stage with development of an evaluation plan and continues through the life cycle of the training system.  The focus of evaluation is continuous improvement in training system quality.  This chapter stresses the importance of the evaluation process by summarizing the evaluation activities discussed in earlier chapters of this handbook.  For example, formative and summative evaluations are discussed in Chapter 6 and operational evaluation is described in Chapter 7.   The ISD model, with the evaluation phase highlighted, is depicted in Figure 14.  As depicted in the model, each stage in the ISD process involves evaluation activities.  Figure 14 Evaluation    AFH 36-2235 Volume 8 1 November 2002 216 Objective  Continuous evaluation process         The objective of this chapter is to summarize a:  Formative evaluation Summative evaluation Operational evaluation Validate Training   As previously mentioned, evaluation is an ongoing process.  It begins during ISD planning and continues as long as the instructional system is operational.  The process includes formative, summative, and operational evaluations.  Each of these forms of evaluation is discussed in subsequent sections of this chapter.  A brief overview of the evaluation process is provided in the following table in order to acquaint the user of this handbook with the continuous evaluation process.  Continuous Evaluation Process Form  Formative Period From initial ISD planning through small-group tryout  Summative  Operational Operational tryout (normally 2 or 3 classes) – real student throughput, full instructional system operation From completion of the operational tryout continuing for the life cycle of the instructional system  Purpose Checks design of individual components of the instructional system for integration (accomplished periodically – is focused on the components – high data collection – make changes when it is least expensive to revise). Checks full system integration and its components (intense – high data collection – short-term – first time everything is working together). Checks day-to-day system integration and its components (periodic, less data collection, life of system – continuous improvement). AFH 36-2235 Volume 8 1 November 2002 217 Where to read about it       Additional information    This chapter contains four sections.  Section A B C D Title  Validate Training Formative Evaluation Summative Evaluation Operational Evaluation Page 218 230 234 235   For additional information on evaluation, see:  Previous chapters in this handbook. AFMAN 36-2236, Handbook for Air Force Instructors. Briggs, L. J. and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.). Glenview, Illinois: Harper Collins Publishers. Dick, W. and Carey, L. (1990).  The Systematic Design of Instruction.  (3rd Ed.).  Glenview, Illinois: Harper Collins Publishers. O'Neil, H.F. Jr., and Baker, E.L. (1991).  Issues in Intelligent Computer-Assisted Instruction:  Evaluation and Measurement.  In T. Gutkin and S. Wise, (Eds.)  The Computer and the Decision Making Process.  Hillsdale, New Jersey: Laurence Erlbaum Associates.  AFH 36-2235 Volume 8 1 November 2002 218 Section A  Validate Training Introduction  What is validation?  validate? When should you  Where to read about it         ISD is more an art than a science.  It offers no fixed formulas that guarantee outcomes, only guidelines.  Validation is a quality improvement tool that helps identify problems in the training during development so that revisions can be made.  Validation consists of internal reviews, individual tryouts, and small-group tryouts, which are conducted as a part of formative evaluation and operational (field) tryouts, which make up summative evaluation.   Validation is a process that assesses the effectiveness of the training as it is being developed with the intention of improving the training product.  It is a process of repetitive cycles of development, tryouts, and revisions until evidence shows that the training is effective.   It is important that validation be conducted during – not after – training development and should be part of the total development plan developed during the management phase. The purpose is to correct all mistakes or problems before you spend too many resources on a flawed product.  Validation should be done as segments, units, or blocks of training are being developed.     This section covers five topics.   Plan Validation Conduct Internal Reviews Conduct Individual Tryouts Conduct Small-Group Tryouts Conduct Operational (Field) Tryouts  Page 219 220 223 226 228 Topic AFH 36-2235 Volume 8 1 November 2002 219 Plan Validation Introduction  Purpose  Who is responsible?  validation plan? What is in a   Planning is a key factor in developing a training system.  Without adequate planning and preparation, it is unlikely that the training system would effectively and efficiently produce graduates who have the skills, knowledge, and attitudes to perform the job under operational conditions.  A part of that necessary planning is how you intend to validate training as it is developed.   As with any other plan, a validation plan provides curriculum developers and instructors with a roadmap for validating the training.  A validation plan adds structure and creditability to the validation process.   Planning validation is the responsibility of the curriculum developer.  In some cases, this responsibility may be delegated to other individuals, such as an instructor.  Managers in the training organization have overall responsibility for ensuring that the validation process is adequately planned for the training system being developed.  The validation plan is included as part of the ISD evaluation plan.   Validation plans may contain the following information:  Description of the training to be validated (objectives, method, media) Validators or measures of validation Validation procedures Validation schedules Number of tryouts to be conducted Number of students to be used in the small-group and field tryouts  Sources and how the results will be documented How problems will be resolved     Remember, include only necessary information in the validation plan and keep it simple. AFH 36-2235 Volume 8 1 November 2002 220 Conduct Internal Reviews Introduction  Purpose  Who should review?  What should be reviewed?                The first step in the validation process is internal reviews, which is a formative evaluation activity.  These reviews identify technical inaccuracies and instructional weaknesses in the training.  Qualified individuals from within the training organization normally conduct the review before a selected target population tries out the training.   The purpose of internal reviews is to verify the accuracy of the training materials that have been developed by cross-checking them against data sources such as technical orders, checklists, and job aids.   The following individuals perform internal reviews:  SMEs make sure the content is technically accurate and the coverage is adequate. Curriculum developers make sure the material follows sound instructional principles and the training method, medium, and activities used are appropriate to the content and specified target population.   Training materials should be submitted for review as they are developed.  If several different authors created the lessons for a unit, the draft materials for the entire unit should be reviewed as a unit before any media are created.  Materials to be validated for each type of media are listed below.    AFH 36-2235 Volume 8 1 November 2002 221 Type of Media Print-based materials (workbooks, handouts, training manuals) Classroom training Traditional audiovisual materials (videotapes, film slide) Interactive courseware (IVD, CMI) Materials to be Validated List of objectives Test items Draft of printed materials List of objectives Test items Plans of instruction Drafts of support materials (transparencies/slides) List of objectives Test items Storyboard/script List of objectives Test items Storyboard/script Graphics/video produced Screens developed   During an internal review, you should:  Cross-check technical data for agreement and concurrency. Use a focus group (SME, supervisors, instructor, and curriculum developer) to review the training material.  What should be reviewed? (Continued)     During a review                   AFH 36-2235 Volume 8 1 November 2002 222    During a review (Continued)  Common problems identified After a review    Questions You May Ask During Internal Reviews Is the content of the training materials accurate? Is the training material current? Is the training material complete? 1. 2. 3. 4.  What are the "good" parts of the training? 5.  What are the "bad" parts of the training? 6. Is the sequence of the material effective? Are the practice exercises adequate? 7. 8. Are the review exercises adequate? 9.  Do the training materials/lessons effectively teach the behavior specified in the objective? Is the objective adequately evaluated? Is the content of the training materials compatible? 10. 11. 12.  Can the training materials be improved?  If so, how?    Some of the common problems that you may identify during the internal reviews are:  Lack of agreement in the material Inaccuracies in the material Incomplete material Weaknesses in the material Lack of continuity in the material   Upon completion of the internal reviews, you should:  Determine the changes to be made to improve the training materials. Decide the best way to make the changes to the training materials. Make the changes to the training materials, as applicable. AFH 36-2235 Volume 8 1 November 2002 223 Conduct Individual Tryouts Introduction  Purpose  Select students for tryouts   During the individual tryouts, which are a formative evaluation activity, a curriculum developer tests the training materials on individual students.  Several students should be used for comparison in order to obtain more valid and reliable results.   The purpose of individual (one-on-one) tryouts is to determine the effectiveness of small segments or units of training and materials as they are developed.   When selecting students for individual tryouts, consider these issues.  Students selected should represent the target population in:     Aptitude Skills Attitude Prior knowledge Background experience Select students for the first tryouts from the upper percentage ranges in aptitude and background.  The reasons for selecting students from the upper-percent level for the first tryout are: Above-average students often point out and analyze weaknesses in the training or training materials. If better students cannot learn the material, less capable students will not be able to. If lower-level students are used in the individual test and they do well, it will be hard to assess the relative difficulty of the lesson. It is easier to add material in order to make a lesson easier and more effective than to identify and delete existing material that is unnecessary or confusing in order to achieve the optimum level and quality of instruction. AFH 36-2235 Volume 8 1 November 2002 224 Before a tryout  During a tryout  Common problems identified  Before conducting individual tryouts, a curriculum developer should prepare the students for the tryout.  Students need to know:  Purpose of the tryout. Their role in the tryout. That they are not being evaluated–the training is. That their active participation is essential if the tryout is to be successful. That their feedback is necessary to determine adequacy of the training material.  If instructors are involved in the individual tryouts, they should be made aware of their role and the role of the student.   During the individual tryouts, a curriculum developer should:  Use a pretest to identify entry behavior. Use a post-test to assess learning as a result of the tryout. Observe where each student seems to have problems or uncertainties. Give assistance only when essential to the student's progress.Determine which exercises or tasks result in errors; type of errors; how many students make the same error(s). Get each student's view about the difficulties encountered during the tryout. Ask each student for suggestions on how to improve the training.   Some of the common problems that you may identify during individual tryouts are:  Improper sequencing of the training Gaps in the training Inadequate explanation of content Wrong assumption about target population's prerequisite knowledge Confusing test items Test items that do not measure objectives Insufficient practice time  AFH 36-2235 Volume 8 1 November 2002 225 After a tryout  After individual tryouts, analyze the data to determine if there are error patterns or problems.  If so, change or revise the training or training material as appropriate.  If the training or materials require significant changes, it would be wise to conduct additional tryouts to make sure that changes made were effective in solving the problem.  AFH 36-2235 Volume 8 1 November 2002 226 Conduct Small-Group Tryouts Introduction  Purpose  Select students  Time is critical factor   Up to this point, the success of the training has been based on a limited sampling of students of higher aptitudes.  Since training is developed for average students, small-group tryouts that focus on an average group of students are required.  Small-group tryouts conclude the formative evaluation process.   The purpose of conducting small-group tryouts is to determine if the training works for the average target students.   Students selected to participate in the tryouts should be representative of the target population.  If possible, they should have:  Even distribution between low and high aptitudes Varying skill levels Backgrounds that are representative of the target population  The number of students in the small groups is determined by several factors, such as:  Actual task requirements (for example, some tasks may require students to work in teams of two; if so, the small-group size should be a multiple of two) Planned group size of the actual course Availability of equipment Availability of facilities   During small-group tryouts, students should be given a reasonable amount of time to learn, practice, and perform the skills trained.  The average time required by the groups will help establish the time to be allotted in the course for that unit of training.  AFH 36-2235 Volume 8 1 November 2002 227 Before a small-group tryout  During a tryout  After a tryout   Before a small group tryout, you should:  Determine the number and availability of students and groups to be used in the tryouts. Select representative students from the target population. Ensure that training has been revised in accordance with results from individual tryouts. Ensure that student materials are available in adequate quantities. Establish the number of trials a student will be permitted to achieve the criterion performance.   During a small-group tryout, you should:  Record the time each student takes to complete the material.  This information will be used to validate unit time, course lengths, and course content. Record student responses.  This information will help determine the weaknesses of the training.  Don't supplement the training.  Supplementing the training will skew the results of the tryout.   After a tryout, you should analyze the results to:  Identify the average completion time for each segment or unit of training.  This information determines the exact time for lessons, segments, units, or modules of training. Determine the need to revise equipment requirements, change the facilities and adjust the personnel authorizations. Revise the training and materials.  If the training requires significant changes, it may be necessary to conduct additional tryouts to determine if the revisions will be effective.  AFH 36-2235 Volume 8 1 November 2002 228 Conduct Operational (Field) Tryouts Introduction  Purpose  Select students  Before a tryout    The final stage of validation is operational (field) tryouts, which comprise the summative evaluation process.  Field tryouts provide a learning situation that is intended for the training materials.  All of the materials and media should be complete and ready to go.  For example, if it is classroom training, an instructor should be used to teach the class.  If it is CBT, the students should sit at the computer and interact with the program.   Field tryouts serve several purposes:    They determine if the training actually works under operational conditions. They provide feedback from a large sample of the target population for final revisions or refinements of the training. They identify implementation problems, such as equipment and facilities.   For field tryouts, students are assigned to the tryout group using the normal class assignment procedures.   Before a field tryout, make sure that:  Resources are available, such as equipment, facilities, and instructors. Training has been revised based on the results of small-group tryouts. Training materials are available in adequate quantities. Students have been scheduled and informed of the purpose of the tryout. AFH 36-2235 Volume 8 1 November 2002 229 During a tryout  Data collection      After a tryout    In many respects, a field tryout is like the normal day-to-day operation of a training course.  During a field tryout, you should:  Ensure that the training is conducted under normal operating environment. Collect validation data such as time requirements, measurement results, instructor and student comments, and problem areas. Use adequate class samples to ensure that the data is both valid and reliable.   During an operational tryout, you may collect data from students before, during, and after the training.  The field data collection is summarized.   Stage Before During After  Data To Be Collected Student entry skill/knowledge level Number of errors students make Questions raised by students Student work samples Duration of the training Student learning gains Student views about the training Data Collection Methods Pretest Oral examination Student interviews Observations Recording student questions Collecting work sam-ples Post-test Student interviews Student critiques   After a field tryout, you should:  Analyze the data gathered during the tryouts. Revise the training and materials as necessary.  AFH 36-2235 Volume 8 1 November 2002 230 Introduction  Definition  Formative evaluation activities  Section B Formative Evaluation  The formative evaluation process begins during analysis and continues through small-group tryouts in the development stage of ISD.  Within each stage–analysis, design, and development–formative evaluation seeks to improve the quality of the ISD activities (process) and products.  In some organizations, formative evaluation is equated to four stages of validation: internal reviews, individual tryouts, small-group tryouts, and operational tryouts.   Formative evaluation is designed to collect data and information that is used to improve the ISD activities and products while the system is still being developed.   Formative evaluation includes the following activities.  Process Evaluation Process evaluation ensures quality in the analysis, design, and development activities.  It checks each activity against the standards or metrics established during ISD project planning, to assure process quality, while continually seeking improvements within each activity.  Process evaluation enables curriculum developers to form an effective and efficient training system based on quality principles.  Product Evaluation Product evaluation is an integral part of each stage of ISD.  Product evaluation focuses on the products of the analysis, design and development activities such as task lists, objectives, tests, plans of instruction and training materials.  During product evaluation the focus is again on quality.  Products are measured against established standards or metrics to ensure quality.  Product evaluation also helps form a total quality training system.  AFH 36-2235 Volume 8 1 November 2002 231 Formative evaluation activities (Continued)    Two activities of product evaluation are:  Validation, which takes place during training development and is the final activity in the formation evaluation process.  This component forms the training system by trying out instruction on individuals and small groups.  Validation identifies quality improvements that should be made to the training prior to implementation.  Quality control starts in the initial stages of ISD planning, with the strategy for controlling quality, and continues throughout training analysis, design and development.  This process ensures that each activity, such as equipment acquisition and facility construction, is based on quality principles.  Development Test and Evaluation (DT&E) DT&E is an active part of training system development.  As a formative evaluation activity, it is conducted to demonstrate that training system equipment design and  development is complete, design risks have been minimized, and the system meets performance requirements.  It ensures the effectiveness of the manufacturing process, equipment, and procedures.  Operational Test and Evaluation (OT&E) OT&E completes the formative evaluation process for training system equipment.  This formative evaluation activity evaluates the system's operational effectiveness, maintainability, supportability, and suitability to identify any operational and logistic support deficiencies, and to identify the need for modification.  In addition, OT&E provides information on organizational structure, personnel requirements, support equipment, doctrine, training, and tactics.  It should also provide data to verify operating instructions, maintenance procedures, training programs, publications, and handbooks.   Site Readiness Reviews The site readiness review is a formative evaluation activity that focuses on evaluating readiness of the "bed-down" site for the training system.  This evaluation ensures that the site, including training facilities and support equipment, is ready for OT&E of the system.  Site readiness reviews ensure successful training system implementation.  AFH 36-2235 Volume 8 1 November 2002 232 Relationship of the activities Period of formative evaluation   Formative evaluation activities contribute to the overall quality of the training system. They combine to ensure that:  Training development activities are effective and cost-efficient.Products of each development activity meet quality standards. Training meets requirements. Equipment satisfies operational, training, and support requirements. Facilities meet operational, training, and support requirements.  Planning for formative evaluation begins in the initial planning stage of ISD, with evaluation activities actually beginning during analysis and continuing through small-group tryout in development.   AFH 36-2235 Volume 8 1 November 2002 233 Section C Summative Evaluation Introduction  Definition  Summative evaluation activity  Evaluating the integrated system  Period of summative evaluation   With the conclusion of small-group tryouts, formative evaluation activities are complete.  Summative evaluation is the next stage in the continuous evaluation process. This stage of evaluation involves trying out the training on the target population in an operational environment.  In some organizations, summative evaluations are conducted after the instructional system becomes operational.  Summative evaluation includes two components: internal and external evaluation.   Summative evaluation is designed to collect data and information during the operational (field) tryouts in order to determine the "summed" effect of the training under operational conditions.   The only activity within summative evaluation is the operational tryouts.  Operational tryouts:  Determine if the instructional system works under operational conditions. Provide feedback from a large sample of target population in which to base revisions prior to implementation of the training system. Identify possible implementation or operational problems.   Summative evaluations are conducted on fully integrated training systems–that is, each component of the system is functioning as a whole.  This form of evaluation is essential in determining the effectiveness of the system and correcting any deficiencies prior to implementation.   Summative evaluation is focused on the period of operational tryouts.  These tryouts begin after the small-group tryouts have been completed and continue until the training system is implemented.  Normally, the operational tryout period is limited to two or three classes.   AFH 36-2235 Volume 8 1 November 2002 234 Section D Operational Evaluation Introduction  Definition  Operational evaluation activities  Relationship of activities  As previously mentioned, the evaluation process is continuous.  Once formative and summative evaluation activities have been completed and the training system is implemented, operational evaluation begins.  Operational evaluations continue as long as the system is operational.  In some organizations, this form of evaluation is  called summative evaluation.   Operational evaluation is designed to gather and analyze internal and external feedback data to ensure that the system continues to effectively and cost-efficiently produce graduates who meet established training requirements.   Operational evaluation activities include internal and external evaluation.  Internal Evaluation Internal evaluation focuses on evaluating the training system internally.  This form of evaluation continuously evaluates feedback data, such as instructor comments, students' critiques and test results, in order to continuously improve the system and ensure quality.  External Evaluation External evaluation focuses on evaluating the training system externally.  This form of evaluation continuously evaluates feedback data from the field, such as inspection and evaluation reports, to ensure that graduates meet the established job performance requirements.   Each operational evaluation activity contributes to the overall quality of the training system by ensuring that:  Each system component continues to contribute to the overall effectiveness and cost-efficiency of the system. Graduates of the course continue to meet established job performance requirements.  AFH 36-2235 Volume 8 1 November 2002 235  Operational evaluation begins with implementation of the training system and continues for the life cycle of the system.  A schedule of review should be established for all training materials and be included in the ISD evaluation plan.  Courseware review boards, with all relevant people in attendance, can also be an effective way of evaluating the program.  RICHARD E. BROWN, Lt General, USAF DCS/Personnel Period of operational evaluation            AFH 36-2235 Volume 8 1 November 2002 236 GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION  Attachment 1 AFPD 36-22 AFI 36-2201 AFI 36-2301 AFMAN 36-2234 AFMAN 36-2236 AFH 36-2235        Military Training   Developing, Managing and Conducting Military Training   Professional Military Education  Instructional System Development   Handbook for Air Force Instructors  Vol 1 Vol 2  Vol 3  Application to Acquisition Vol 4   Manager's Guide to New Education and Training Technologies Vol 5  Advanced Distributed Learning:  Instructional Technology and Information for Designers of Instructional Systems (12 Volumes) ISD Executive Summary for Commanders and Managers ISD Automated Tools/What Works Distance Learning         Vol 6  Guide to Needs Assessment Vol 7  Design Guide for Device-based Aircrew Training Vol 8  Application to Aircrew Training Vol 9  Application to Technical Training Vol 10  Application to Education Vol 11  Application to Unit Training Vol 12 Test and Measurement Handbook AFH 36-2235 Volume 8 1 November 2002 237   Walter Dick, Lou Carey, James O. Carey (2000).  The Systematic Design of Instruction 5th Edition.  Addison-Wesley Pub Co.  ISBN 0321037804.  Patricia Smith & Tillman Ragan (1999).  Instructional Design, 2nd Edition.  John Wiley & Sons, 399 pages. ISBN 047136570X.  4   Ruth Clark (1999).  Developing Technical Training 2nd Edition:  A Structured Approach for Developing Classroom and Computer-based Instructional Materials.  International Society for Performance Improvement. 238 pages.  ISBN 1-890289-C7-8 M. David Merrill (1994).  Instructional Design Theory.  Educational Technology Pub.   ISBN 0-87778-275-X.    M. David Merrill, Robert D. Tennyson, and Larry O. Possey (1992).  Teaching Concepts:  An Instructional Design Guide 2nd Edition.  Educational Technology Pub.  ISBN 0-87778-247-4  Robert M. Gagné, Leslie J. Briggs, and Walter W. Wager.  (1992).  Principles of Instructional Design 4th Edition.  Wadsworth Pub. Co.  365 pages.  ISBN 00300347572  Robert M. Gagné (1985).  The Conditions of Learning and Theory of Instruction 4th Edition.  Holt, Rinehart and Winston. ISBN 0-03-063688-4  Jeroen J. G. van Merriënboer (1997).  Training Complex Cognitive Skills.  Educational Technology Publications.  Ruth Clark (1998).  Building Expertise:  Cognitive Methods for Training and Performance Improvement.  International Society for Performance Improvement.  204 pages.  ISBN 1890289043.    Bernice McCarthy (1996).  About Learning.  Excel, Inc.  452 pages.  ISBN 0-9608992-9-Malcolm Fleming & W. Howard Levie (Editors) (1993).  Instructional Message Design:  Principles from the Behavioral and Cognitive Sciences 2nd Edition.  Educational Technology Publications.  331 pages.  ISBN 0-87778-253-9.  Ellen D. Gagné, Frank R. Yekovich, and Carol Walker Yekovich (1993).  The Cognitive Psychology of School Learning.  2nd Edition.  Addison Wesley Longman, Inc. 512 pages.  ISBN 0673464164.  Marcy P. Driscoll (1999).  Psychology of Learning for Instruction 2nd Edition.  Allyn & Bacon.  448 pages.  ISBN 0205263216 AFH 36-2235 Volume 8 1 November 2002 238 Ann E. Barron & Gary W. Orwig (1997).  New Technologies for Education:  A Beginner's Guide 3rd Edition.  Libraries Unlimited.  ISBN 1563084775  Robert Heinich & Michael Molenda (1998).  Instructional Media and Technologies for Learning 6th Edition.  Prentice Hall.  428 pages.  ISBN 0138591598.  Diana Laurillard (1993).  Rethinking University Teaching:  A Framework for the Effective Use of Educational Technology.  Routledge.  ISBN 0415092892.  Tom Boyle and Tim Boyle (1996).  Design for Multimedia Learning.  Prentice Hall.  275 pages.  ISBN 0132422158.  William W. Lee and Diana L. Owens (2000).  Multimedia-Based Instructional Design:  Computer-Based Training, Web-Based Training, and Distance Learning.   Jossey-Bass.  304 pages.  ISBN 0787951595.  Margaret Driscoll & Larry Alexander (Editor) (1998).  Web-Based Training:  Using Technology to Design Adult Learning Experiences.  Jossey-Bass Inc.  288 pages.  ISBN 0787942030.  Brandon Hall (1997).  The Web-Based Training Cookbook.  John Wiley & Sons.  496 pages.  ISBN 0471180211.  Stephen M. Alessi & Stanley P. Trollip (2000).  Computer Based Instruction.  Allyn & Bacon.  432 pages.  ISBN 0205276911.  Andrew S. Gibbons & Peter G. Fairweather (1998).  Computer-Based Instruction.  Educational Technology. 570 pages.  ISBN 0877783012.  Douglas M. Towne (1995).  Learning and Instruction in Simulation Environments.  Educational Technology.  351 pages.  ISBN 0877782784.  Thomas M. Duffy & David H. Jonassen (Editors) (1992).  Constructivism and the Technology of Instruction:  A Conversation.  Lawrence Erlbaum Associates.  221 pages.  ISBN 0805812725.  Brent G. Wilson (1995).  Constructivist Learning Environments:  Case Studies in Instructional Design.  Educational Technology Publications.  ISBN 0877782903.  Roger C. Schank (Editor) (1997).  Inside Multi-Media Case Based Instruction.  Lawrence Erlbaum Associates.  451 pages.  ISBN 080582538X.  David H. Jonassen, Wallace H. Hannum & Martin Tessmer (1999).  Task Analysis Methods for Instructional Design.  Lawrence Erlbaum Associates.  275 pages.  ISBN 0805830863.  AFH 36-2235 Volume 8 1 November 2002 239 Allison Rossett (1999).  First Things Fast:  A Handbook for Performance Analysis.  Jossey-Bass. 241 pages.  ISBN 0787944386.  Charles M. Reigeluth (Editor) (1983).  Instructional-Design Theories and Models: An Overview of their Current Status.  Lawrence Erlbaum Associates.  487 pages. ISBN 0-89859-275-5.  Charles M. Reigeluth (Editor) (1999).   Instructional-Design Theories and Models:  A New Paradigm of Instructional Theory.  Vol. II.  Lawrence Erlbaum Associates.  715 pages.  ISBN0-8058-2859-1.  Tennyson, Robert D., Schot, Franz, Norbert, Seel & Dijkstra, Sanne. (Editors)  (1997).  Instructional Design International Perspective Vol. 1 Theory, Research, and Models.  Lawrence Erlbaum Associates. 475 pages.  ISBN 0-8058-1397-7.  Sanne Dijkstra, Norbert Seel, Franz Schott & Robert D. Tennyson (Editors) (1997).  Instructional Design:  International Perspective Vol. 2:  Solving Instructional Design Problems.  Lawrence Erlbaum Associates. 418 pages.   ISBN 0805814000.  George M. Piskurich, Peter Beckschi, and Brandon Hall (Editors) (1999). The ASTD Handbook of Training Design and Delivery:  A Comprehensive Guide to Creating and Delivering Training Programs -- Instructor-led, Computer-based.  McGraw Hill. 530 pages. ISBN 0071353105.  Charles R. Dills & A. J. Romiszowski (Editors) (1997). Instructional Development Paradigms.  Educational Technology Publications. 882 pages.  ISBN 08777882954.  Sanne Dijksra, Bernadette van Hout Wolters & Pieter C. van der Sijde (Editors) (1990).  Research on Instruction: Design and Effects.  Educational Technology Publications.  ISBN 0877782210.  David H. Jonnassen (Editor) (1996).  Handbook of Research on Educational Communications and Technology. Macmillan.   1267 pages.  ISBN 0028646630.  Byron Reeves & Clifford Nass (1996).  The Media Equation:  How People Treat Computers, Television, and New Media Like Real People and Places.  Cambridge University Press.  305 pages.  ISBN 1-57586-053-8.   Donald A. Norman (1990).  The Design of Everyday Things.  Doubleday & Company.  256 pages.  ISBN 0385267746,  Bills, C. G. and Butterbrodt, V. L. (1992). Total Training Systems Design Function: A Total Quality Management Application. Wright-Patterson AFB, Ohio.  Briggs, L. J.  and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.).  Glenview, Illinois: Harper Collins Publishers. AFH 36-2235 Volume 8 1 November 2002 240  Carlisle, K. E. (1986). Analyzing Jobs and Tasks. Englewood Cliffs, New Jersey: Educational Technology Publications.  Davies, I. K. (1976). Objectives in Curriculum Design. London: McGraw Hill.  Dick, W. and Carey, L. (1990). The Systematic Design of Instruction (3rd Ed.). Glenview, Illinois: Harper Collins Publishers.  Fishburne, R. P., Williams, K. R., Chatt, J. A. and Spears, W. D. (1987). Design Specification Development For The C-130 Model Aircrew Training System: Phase I Report. Williams AFB, Arizona: Air Force Human Resources Laboratory (AFHRL-TR86-44).  Gagné, R. M. (1985).  The Conditions of Learning (4th Ed.). New York: Holt, Rinehart and Winston.  Gagné, R. M., Briggs, L. J., and Wager, W. W. (1992). Principles of Instruction (4th Ed.). New York: Harcourt Brace Jovanovich College Publishers.  Gagné, R. M. and Merrill, M. D. (1990). Integrative Goals for Instructional Design. Englewood Cliffs, New Jersey: Educational Technology Publications. 38(1), 1-8.  JWK International Corp. (1990). Final Training System Baseline Analysis Report (EWOT). Dayton, Ohio: JWK International Corp.  Kaufman, R., Rojas, A. M. and Mayer, H. (1993).  Needs Assessment: A User's Guide.  Englewood Cliffs, New Jersey: Educational Technology Publications.  Keller, J. M. (1987).  The Systematic Process of Motivational Design.  Performance and Instruction, 26(9), 1-8.  Kibler, R. J. (1981). Objectives for Instruction. Boston: Allyn and Bacon.  Knirk, F. G. and Gustafson, K. L. (1986). Instructional Technology: A Systematic Approach to Education. New York: Holt, Rinehart, and Winston.  Leshin, C. B., Pollock, J., and Riegeluth, C. M. (1992). Instructional Design Strategies and Tactics. Englewood Cliffs, New Jersey: Educational Technology Publications.  Mager, R. F. (1962). Preparing Objectives for Instruction (2nd Ed.). Belmont, California: Fearon Publishers.  AFH 36-2235 Volume 8 1 November 2002 241 Merrill, M. D., Tennyson, R. D. and Posey, L. (1992).  Teaching Concepts: An Instructional Design Guide (2nd Edition). Englewood Cliffs, New Jersey: Educational Technology Publications.  Modern Training Model Concepts for Aircrew Training. AFHRL PR 87-34.  O'Neil, H. F., Jr., and Baker, E. L. (1991).  Issues in Intelligent Computer-Assisted Instruction: Evaluation and Measurement.  In T. Gutkin and S. Wise (Eds.),  Hillsdale, New Jersey:  Lawrence Erlbaum Associates.  Reigeluth, C. M. (1983). Instructional Design: What Is It and Why Is It?  In C.M. Reigeluth (Ed.), Instructional Design Theories and Models?  An Overview of Their Current Status.  Hillsdale, New Jersey:  Erlbaum Associates.  Reiser, R. A. and Gagné, R. M. (1993).  Selecting Media for Instruction. Englewood Cliffs, New Jersey: Educational Technology Publications.  Rossett, A. (1987).  Training Needs Assessment.  Englewood Cliffs, New Jersey: Educational Technology Publications.  Shrock, S. and Coscarelli, W. C. (1990). Criterion-Referenced Test Development.  Reading, Massachusetts: Addison-Wesley Publishing Company.  Spears, W. D. (1983). Processes of Skill Performance:  A Foundation for the Design and Use of Training Equipment. (NAVTRAEQ-VIPCEN 78-C-0113-4). Orlando, Florida: Naval Training Equipment Center.  Swezey, R. W. and Pearlstein, R. B. (1975). Guidebook for Developing Criterion-Referenced Test. Arlington, Virginia: U. S. Army Research Institute for the Behavioral and Social Sciences.  Tennyson, R. D. and Michaels, M. (1991). Foundations of Educational Technology: Past, Present and Future.  Englewood Cliffs, New Jersey: Educational Technology Publications.  Williams, K. R., Judd, W. A., Degen, T. E., Haskell, B. C., & Schutt, S. L. (1987). Advanced Aircrew Training Systems (AATS): Functional Design Description. Irving, Texas: Seville Training Systems (TD-87-12).  Wolfe, P., Wetzel, M., Harris, G., Mazour, T. and Riplinger, J. (1991). Job Task Analysis: Guide to Good Practice. Englewood Cliffs, New Jersey: Educational Technology Publications. AFH 36-2235 Volume 8 1 November 2002 242  ACRONYMS AND ABBREVIATIONS AATS A/C ACC ADI AETC AF AFH AFI AFMAN AFPAM AFPD AFS AFSC CAI CBI CBT CDM CLS CMI CRO DoD DT&E ETR FEB FMI HQ HSI HUD IAW ICW IFR IG ISD JPR KTAS LP MAJCOM MCP OFT OJT OMS OPR OSR OT&E Advanced Aircrew Training Systems Aircraft Air Combat Command Attitude Direction Indicator Air Education and Training Command Air Force Air Force Handbook Air Force Instruction Air Force Manual Air Force Pamphlet Air Force Policy Directive Air Force Specialty  Air Force Specialty Code Computer-Assisted Instruction Computer-Based Instruction Computer-Based Training Curriculum Development Manager Contractor Logistic Support Computer-Managed Instruction Criterion-Referenced Objective Department of Defense Development Test and Evaluation Established Training Requirements Flying Evaluation Board Functional Management Inspection Headquarters Heading Situation Indicator Heads Up Display In Accordance With Interactive Courseware Instrument Flight Rules Inspector General Instructional System Development Job Performance Requirements Knots True Air Speed Lesson Plan Major Command Military Construction Project Operational Flight Trainer On-the-Job Training Occupational Measurement Squadron Office of Primary Responsibility Occupational Survey Report Operational Test and Evaluation AFH 36-2235 Volume 8 1 November 2002 243 PAS POI QAF QI SKA SME SMS SPO STP TDW TDY TNA TO TPR TPT USAF VCR VFR Primary Alerting System Plan of Instruction Quality Air Force Quality Improvement Skill, Knowledge and Attitude Subject Matter Expert Subject Matter Specialist System Program Office System Training Plan Task Description Worksheet Temporary Duty Training Needs Assessment Technical Order Trained Personnel Requirement Training Planning Team United States Air Force Video Cassette Recorder Visual Flight Rules  AFH 36-2235 Volume 8 1 November 2002 244 TERMS  The following list of definitions includes those terms commonly used in aircrew training as it relates to instructional system development and as used in this handbook.  The list is not to be considered all-inclusive.  Acquisition.  The procurement by contract, with appropriated funds, of supplies or services (including construction) by and for the use of the Federal Government through purchase or lease, whether the supplies or services are already in existence or must be created, developed, demonstrated, and evaluated.  Aircrew Training System (ATS).  A contractor produced, operated and maintained ground-based system used to train aircrew members.  It includes training equipment, software, firmware, hardware, devices, courseware, training system support, logistics support, ground-based instruction, media, and facilities.  It typically does not include flight training or aircraft support.  Association.  The connection made between an input (stimulus) and an action (response).  Attitude.  (a) The emotions or feelings that influence a learner's desire or choice to perform a particular task.  (b) A positive alteration in personal and professional beliefs, values, and feelings that will enable the learner to use skills and knowledge to implement positive change in the work environment.  Also see Knowledge and Skill.  Audiovisual Medium.  Any delivery device or system which provides both audio and visual presentations.  Behavior.  Any activity, overt or covert, capable of being measured.  Cognition.  The mental or intellectual activity or process of knowing, includes both awareness and judgment.  Cognitive Strategies.  The capability of individuals to govern their own learning, remembering, and thinking behavior.   Computer-Assisted Instruction (CAI).  The use of computers to aid in the delivery of instruction. A variety of interactive instructional modes are used including tutorial, drill, and practice, gaming, simulations, or combinations.  CAI is an integral part of computer-based instruction (CBI) and computer-based training (CBT).  Computer-Based Instruction (CBI) and Computer-Based Training (CBT).  The use of computers to aid in the delivery and  management of instruction.  CBI and CBT are synonymous and are used interchangeably.  CAI (the delivery of instruction) and CMI (computer-managed instruction) are both elements of CBI and CBT. AFH 36-2235 Volume 8 1 November 2002 245 Computer-Managed Instruction (CMI).  The use of computers to manage the instructional process in CAI or CBT. Management normally includes functions such as registration, pretesting, diagnostic counseling, progress testing, and post-testing.  CMI is also used to schedule and manage training resources such as trainers and actual equipment.    Constraints.  Limiting or constraining conditions or factors, such as policy considerations, time limitations, equipment, environmental factors, personnel, budgetary, or other resource limitations.  Contract Logistics Support (CLS).  A preplanned method used to provide all or part of the logistics support to a system, subsystem, modification, or equipment throughout its life cycle.  CLS covers depot maintenance and, as negotiated with the operating command, necessary organizational and intermediate (O&I) level maintenance, software support, and other operation and maintenance (O&M) tasks.  Contractor.  An individual or organization outside the Government that has accepted any type of agreement or order for providing research, supplies, or services to a Government agency.  Course Chart.  A qualitative course control document that states the course identity, length, and security classification, lists major items of training equipment, and summarizes the subject matter covered.  Course Control Documents.  Specialized publications used to control the quality of the instructional system. Examples are training standards, plans of instruction, syllabi, and course charts.   Courseware.  Training materials such as technical data, textual materials, audiovisual instructional materials, and computer-based instructional materials.  Criterion.  (a) The standard by which something is measured.  (b) In test validation, the standard against which test instruments are correlated to indicate that accuracy with which they predict human performance in some specified area. (c) In evaluation, the measure used to determine the adequacy of a product, process, behavior, and other conditions.  Criterion-Referenced Objective (CRO).  (a) A performance-oriented tool identifying criteria and actions required to demonstrate mastery of a task.  (b) An objective with prescribed levels of performance.  Each CRO contains a behavior (task statement), a condition (available equipment, checklists, and governing directives, or the situation requiring the task), and a standard (regulation, operating instruction) for the task.  Criterion-Referenced Test (CRT).  A test to determine, as objectively as possible, a student's achievement in relation to a standard based on criterion objectives.  During instructional development, the CRT can be used to measure the effectiveness of the AFH 36-2235 Volume 8 1 November 2002 246 instructional system.  The test may involve multiple-choice items, fill-in items, essays, or actual performance of a task. If given immediately after the learning sequence, it is an acquisition test; if given considerably later, it is a retention test; if it requires performance not specifically learned during instruction, it is a transfer test.   Defense System.  Any weapon system, support system, or end item that supports a specific military mission, therefore requiring operations, maintenance, or support personnel training.  Discrimination.  The process of making different responses to a stimulus. A discrimination requires a person to determine the differences among inputs and to respond differently to each.  Distance Learning.  Training that is exported, such as from a resident course to a field location.  Also called Exportable Training.  Duty.  A large segment of the work done by an individual; major divisions of work in a job.   Evaluation.  A judgment expressed as a measure or ranking of trainee achievement, instructor performance, process, application, training material, and other factors (see MIL-HDBK-29612).  It includes  Formative Evaluation; Operational Evaluation; and Summative Evaluation.   Evaluation Plan.  A method or outline of a set of procedures which will be used to gather data and information for the purpose of assessing a course of instruction or other training product.  Exportable Training.  See Distance Learning.  External Evaluation.  The acquisition and analysis of feedback data from outside the formal training environment to evaluate the graduate of the instructional system in an operational environment.  Also called Field Evaluation.  Also see Operational Evaluation.  Feedback.  Information that results from or is contingent upon an action. The feedback does not necessarily indicate the rightness of an action; rather, it relates the results of the action from which inferences about correctness can be drawn.  Feedback may be immediate, as when a fuse blows because a lamp is incorrectly wired; or delayed, as when an instructor provides a discussion pertaining to an exam taken the previous week, or when completed graduate evaluation questionnaires are reviewed.  Fidelity.  The degree to which a task or a training device represents the actual system performance, characteristics, and environment.  Field Evaluation.  See External Evaluation. AFH 36-2235 Volume 8 1 November 2002 247 Formative Evaluation.  An activity that provides information about the effectiveness of training materials to meet training objectives and the trainees' acceptance of training materials as they are being developed.  It is conducted while the instructional system or course is still under development, to gather data on lessons, units, or modules of instruction as they are completed.  The purpose is make improvements to the system or course while development is still in progress.  Also see Evaluation.  Generalization.  Learning to respond to a new stimulus that is similar, but not identical, to one that was present during original learning.  For example, during learning a child calls a beagle and spaniel by the term "dog"; a child who has generalized would respond "dog" when presented with a hound.   Instructional Media.  The means used to present information to the trainee.  Instructional Objective.  See Objective.  Instructional System.  An integrated combination of resources (students, instructors, materials, equipment, and facilities), techniques, and procedures performing effectively and efficiently the functions required to achieve specified learning objectives.  Instructional System Developer.  A person who is knowledgeable of the instructional system development (ISD) process and is involved in the analysis, design, development, implementation and evaluation of instructional systems.  Also called Instructional Designer, Instructional Developer, Curriculum Developer, Curriculum Development Manager, and other terms.  Instructional System Development (ISD).  A deliberate and orderly, but flexible, process for planning, developing, implementing, and managing instructional systems.  ISD ensures that personnel are taught in a cost-efficient way the skills, knowledge, and attitudes essential for successful job performance.  Interactive Courseware (ICW).  Computer-controlled training designed to allow the student to interact with the learning environment through input devices such as keyboards and light pens. The student's decisions and inputs to the computer determine the level, order, and pace of instructional delivery, and forms of visual and aural outputs.  Interactive Video.  Video that uses analog and digital databases to present instructional material in the ICW environment.  Interactive Videodisc (IVD).  A form of ICW instruction that specifically makes use of videodisc technology. Video and audio signals are pressed onto the laser videodisc; programming codes may or may not be pressed onto the disc depending on the IVD level. As a result, motion sequence, still-frame shots, computer-generated graphics, and/or audio may be displayed and heard through a monitor under computer and user control.  AFH 36-2235 Volume 8 1 November 2002 248 Internal Evaluation.  The acquisition and analysis of feedback and management data from within the formal training environment to assess the effectiveness of the instructional system.  Also see Operational Evaluation.  Job.  The duties, tasks, and task elements performed by an individual.  The job is the basic unit used in carrying out the personnel actions of selection, training, classification, and assignment.  Job Aid.  A checklist, procedural guide, decision table, worksheet, algorithm, or other device used by a job incumbent to aid in task performance.  Job aids reduce the amount of information that personnel must recall or retain.  Job Analysis.  The basic method used to obtain salient facts about a job, involving observation of workers, conversations with those who know the job, analysis questionnaires completed by job incumbents, or study of documents involved in performance of the job.  Job Performance Requirements (JPR).  The tasks required of the human component of the system, the conditions under which these tasks must be performed, and the quality standards for acceptable performance. JPRs describe what people must do to perform their jobs.  Knowledge.  Use of the mental processes that enable a person to recall facts, identify concepts, apply rules or principles, solve problems, and think creatively.  Knowledge is not directly observable.  A person manifests knowledge through performing associated overt activities.  Also see Attitude and Skill.  Learning.  A change in the behavior of the learner as a result of experience. The behavior can be physical and overt, or it can be intellectual or attitudinal.  Lesson Plan.  An approved plan for instruction that provides specific definition and direction to the instructor on learning objectives, equipment, instructional media material requirements, and conduct of training.  Lesson plans are the principal component of curriculum materials in that they sequence the presentation of learning experiences and program the use of supporting instructional material.   Media.  The delivery vehicle for presenting instructional material or basic communication stimuli to a student to induce learning.  Examples are instructors, textbooks, slides, and interactive courseware (ICW).  Media Selection.  The process of selecting the most effective means of delivering instruction.  Metrics.  Measurement tools used for assessing the qualitative and quantitative progress of instructional development with respect to the development standards specified. AFH 36-2235 Volume 8 1 November 2002 249 Mockup.  A three-dimensional training aid designed to represent operational equipment.  It may be a scaled or cutaway model and may be capable of disassembly or operational simulation.  Motor Skill.  Physical actions required to perform a specific task. All skills require some type of action.  Multimedia.  The use of more than one medium to convey the content of instruction.  Media available for use may include, but need not be limited to:  text, programmed instruction, audio and video tapes/discs, slides, film, television, and computers.  Norm-Referenced Test.  The process of determining a student's achievement in relation to other students.  Grading "on the curve" involves norm-referenced measurement, since an individual's position on the curve (grade) depends on the performance of other students.  Generally, norm-referenced measurement is not appropriate in the Air Force ISD process.  Objective.  A statement that specifies precisely what behavior is to be exhibited, the conditions under which behavior will be accomplished, and the minimum standard of performance.  Objectives describe only the behaviors that directly lead to or specifically satisfy a job performance requirement.  An objective is a statement of instructional intent.  Operational Evaluation.  The process of internal and external review of system elements, system requirements, instructional methods, courseware, tests, and process guide revision as needed to enhance the continued training effectiveness and efficiency of the training system during full-scale operations.  The process begins at the training system readiness review and continues throughout the life of the training system.  It includes Internal Evaluation and External Evaluation.  Also see Evaluation.  Part-task Trainer (PTT).  Operator trainers that allow selected aspects of a task (fuel system operation, hydraulic system operation, radar operation, etc.) to be practiced and a high degree of skill to be developed independently of other elements of the task.  Perceptual Skill.  The process of information extraction; the process by which an individual receives or extracts information from the environment through experiences and assimilates this data as facts (sight, sound, feel, taste, smell).  Performance.  Part of a criterion objective that describes the observable student behavior (or the product of that behavior) that is acceptable to the instructor as proof that learning has occurred.  Plan of Instruction (POI).  A qualitative course control document designed for use primarily within a school for course planning, organization, and operation.  Generally, criterion objectives, duration of instruction, support materials, and guidance factors are listed for every block of instruction within a course.  Also called Syllabus.   AFH 36-2235 Volume 8 1 November 2002 250 Post-test.  A criterion-referenced test designed to measure performance on objectives taught during a unit of instruction; given after the training.  Pretest.  A criterion-referenced test designed to measure performance on objectives to be taught during a unit of instruction and performance on entry behavior; given before instruction begins.  Quality Air Force (QAF).  A management philosophy and a methodology that work together to produce continuous process improvements.  QAF implements Total Quality Management (TQM) in the Air Force.  Quality Improvement (QI).  The organized creation of beneficial change; improvements made in products, procedures, learning, etc.  Ready for Training (RFT).  The date on which sufficient equipment, training capabilities, personnel, and logistics elements are available to support full operational training.  Reliability.  (a) A characteristic of evaluation which requires that testing instruments yield consistent results.  (b) The degree to which a test instrument can be expected to yield the same result upon repeated administration to the same population.  (c) The capability of a device, equipment, or system to operate effectively for a period of time without a failure or breakdown.  Simulator.  (a) Hardware and software designed or modified exclusively for training purposes involving simulation or stimulation in its construction or operation to demonstrate or illustrate a concept or simulate an operational circumstance or environment.  Training simulators and devices are considered part of the  overall training system that may or may not be identified as part of the parent defense system.  (b) Training equipment that imitates operational equipment both physically and functionally, such as a cockpit procedures trainer, operational flight trainer, or weapon systems trainer.  Also see Training Device.  Simulator Certification (SIMCERT).  The process of ensuring through validation of hardware and software baselines that a training system and its components provide the capability to train personnel to do specific tasks.  The process ensures that the device continues to perform to the delivered specifications, performance criteria, and configuration levels.  It will also set up an audit trail regarding specification and baseline data for compliance and subsequent contract solicitation or device modification.  Skill.  The ability to perform a job-related activity that contributes to the effective performance of a task.  Skills involve physical or manipulative activities that often require knowledge for their execution.  All skills are actions having specific requirements for speed, accuracy, or coordination.  Also see Attitude and Knowledge.  AFH 36-2235 Volume 8 1 November 2002 251 Subject Matter Expert (SME).  (a) An individual who has thorough knowledge of a job, duties/tasks, or a particular topic, which qualifies him/her to assist in the training development process (for example, to consult, review, analyze, advise, or critique). (b) A person who has high-level knowledge and skill in the performance of a job.   Summative Evaluation.  The overall assessment of a program at the completion of the developmental process.  It is designed and used after the instructional system has become operational. Data gathered during this form of evaluation is used to determine the effectiveness of the instructional system.  It identifies how well the system is working–that is, how well graduates can meet job performance requirements.  Also see Evaluation.  Syllabus.  See Plan of Instruction.  System Approach to Training (SAT).  Procedures used by instructional system developers to develop instruction. Each phase requires input from the prior phase and provides input to the next phase. Evaluation provides feedback that is used to revise instruction.  Also see Instructional System Development.   System Training Plan (STP).  The specific document that includes program information and data concerning the system or equipment program, event, or situation that originated the training requirement, and describes the training required and the training programs to satisfy the requirement.  The STP is designed to provide for planning and implementation of training and ensure that all resources and supporting actions required for establishment and support are considered.  Target Audience.  The total collection of possible users of a given instructional system; the persons for whom the instructional system is designed.  Task.  A unit of work activity or operation which forms a significant part of a duty.  A task usually has clear beginning and ending points and directly observable or otherwise measurable processes, frequently but not always resulting in a product that can be evaluated for quantity, quality, accuracy, or fitness in the work environment.  A task is performed for its own sake; that is, it is not dependent upon other tasks, although it may fall in a sequence with other tasks in a duty or job array. Task Analysis.  The process of describing job tasks in terms of Job Performance Requirements (JPR) and the process of analyzing these JPRs to determine training requirements.  Also see Job Performance Requirements.  Task Decay Rate.  A measure of how much delay can be tolerated between the time a task has been taught and the actual performance begins.  Terminal Objective.  An objective the learner is expected to accomplish upon completion of the instruction.  It is made up of enabling (support or subordinate) objectives.  AFH 36-2235 Volume 8 1 November 2002 252 Total Contract Training (TCT).  A training concept that includes contract support for a contractor- operated training system.  It includes instructors, curriculum, courseware, facilities, trainers, aircraft, spares, support equipment, and other support elements of contractor logistics support.  The purpose of a TCT is to produce a trained student.  Training.  A set of events or activities presented in a structured or planned manner, through one or more media, for the attainment and retention of skills, knowledge, and attitudes required to meet job performance requirements.  Training Device.  Hardware and software designed or modified exclusively for training purposes involving simulation or stimulation in its construction or operation to demonstrate or illustrate a concept or simulate an operational circumstance or environment.  Also see Simulator.   Training Fidelity.  The extent to which cue and response capabilities in training allow for the learning and practice of specific tasks so that what is learned will enhance performance of the tasks in the operational environment.  Training Needs Assessment (TNA).  The study of performance and the environment that influences it in order to make recommendations and decisions on how to close the gap between the desired performance and the actual performance.  Training Planning Team (TPT).  An action group composed of representatives from all pertinent functional areas, disciplines, and interests involved in the life cycle design, development, acquisition, support, modification, funding, and management of a specific defense training system.  Training Requirement.  The skills and knowledge that are needed to satisfy the job performance requirements and that are not already in the students' incoming repertoire.  Training Strategy.  An overall plan of activities to achieve an instructional goal.  Training System.  A systematically developed curriculum including, but not necessarily limited to, courseware, classroom aids, training simulators and devices, operational equipment, embedded training capability, and personnel to operate, maintain, or employ a system.  The training system includes all necessary elements of logistic support.    Training System Support Center (TSSC).  A consolidated function which contains the personnel, equipment, facilities, tools, and data necessary to provide life cycle hardware and software support for a training system.  Tutorial.  (a) An instructional program that presents new information to the trainee in an efficient manner and provides practice exercises based on that information.  (b) A lesson design used to teach an entire concept.  (c) Interactive instruction that asks questions based on the information presented, requests trainee responses, and AFH 36-2235 Volume 8 1 November 2002 253 evaluates trainee responses.  Tutorials are self-paced, accommodate a variety of users, and generally involve some questioning, branching, and options for review.  Validation.  The process of developmental testing, field testing, and revision of the instruction to be certain the instructional intent is achieved. The instructional system is developed unit by unit and tested (or validated) on the basis of the objective prepared for each unit.    Validity.  The degree to which a criterion test actually measures what it is intended to measure.  Videodisc.  A medium of audiovisual information storage for playback on a television monitor.    Videotape.  A magnetic tape that can record and play back audio (sound) and video (pictures).    Weapon System.  A combination of one or more weapons with all related equipment, materials, services, personnel, training, and means of delivery and deployment (if applicable) required for self-sufficiency.  For purposes of this handbook, a weapon system is that portion of the system that conducts the mission.  Weapon System Trainer (WST).  A device which provides an artificial training/tactics environment in which operators learn, develop, improve and integrate mission skills associated with their crew position in a specific defense system. 