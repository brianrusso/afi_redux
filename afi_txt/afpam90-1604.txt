BY ORDER OF THE  SECRETARY OF THE AIR FORCE AIR FORCE PAMPHLET 90-1604 4 NOVEMBER 2016 Special Management ASSESSMENT OF OPERATIONS AND STRATEGY     ACCESSIBILITY:   Publications and forms are available on the e-Publishing website at www.e-Publishing.af.mil for downloading or ordering. RELEASABILITY:  There are no releasability restrictions on this publication.  OPR:  AF/A9AA   Certified by: AF/A9A  (Col Jonathan T. Hamill) Pages: 31  This  publication  supports  Air  Force  Policy  Directive  (AFPD)  90-16,  Studies  and  Analyses, Assessments  and  Lessons  Learned  (to  be  reissued  as  AFPD  90-16,  Studies,  Analyses,  and Assessments).  It  provides  informational  guidance  on  developing,  conducting,  and  presenting operational assessments and strategy assessments throughout the Air Force (AF). This Air Force Pamphlet (AFPAM) is for reference and is not directive in nature. It applies to individuals at all levels who conduct operational or strategic assessments, including the Air Force Reserve and Air National Guard (ANG), except where noted otherwise. This publication may be supplemented at any  level,  but  all  supplements  must  be  routed  to  the  Office  of  Primary  Responsibility  (OPR) listed  above  for  coordination  prior  to  certification  and  approval.  Refer  recommended  changes and  questions  about  this  publication  to  the  OPR  listed  above  using  the  AF  Form  847, Recommendation  for  Change  of  Publication;  route  AF  Forms  847  from  the  field  through  the appropriate functional chain of command. Ensure that all records created as a result of processes prescribed  in  this  publication  are  maintained  in  accordance  with  (IAW)  Air  Force  Manual (AFMAN)  33-363,  Management  of  Records,  and  disposed  of  IAW  the  Air  Force  Records Disposition  Schedule  (RDS)  in  the  Air  Force  Records  Information  Management  System (AFRIMS).   Chapter 1— INTRODUCTION TO ASSESSMENT OF OPERATIONS AND STRATEGY   1.1.  Overview. ................................................................................................................  1.2.  Levels of Assessment. .............................................................................................  4 4 5 2 AFPAM90-1604  4 NOVEMBER 2016 Figure  1.1.  Levels of Assessment. .............................................................................................  Chapter 2— ASSESSMENT OF OPERATIONS AND STRATEGY  2.1.  Process Overview. ..................................................................................................  Figure  2.1.  Steps in the Assessment of Operations and Strategy. .............................................  2.2.  Step 1 – Identify Strategy. ......................................................................................  Table  2.1.  Definitions of Ends, Ways, and Means. ..................................................................  2.3.  Step 2 – Develop Criteria. .......................................................................................  Table  2.2.  Attributes of Well-written Criteria. .........................................................................  Table  2.3.  Example Criteria for Effects. ..................................................................................  Table  2.4.  Example Criteria for Performance. .........................................................................  2.4.  Step 3 – Identify Measures and Collect Data. .........................................................  Figure  2.2.  Assessment Levels – Example. ...............................................................................  Table  2.5.  Example Measures – Scale. ....................................................................................  Table  2.6.  Example Measures - Observable/Inferable. ............................................................  Table  2.7.  Example Measures – Clear and Concise. ................................................................  2.5.  Step 4 – Analyze and Present Insights. ...................................................................  Figure  2.3.  Expected Results. ....................................................................................................  Figure  2.4.  High MOP and Low MOE. .....................................................................................  Figure  2.5.  High MOE with Low MOP.....................................................................................  Table  2.8.  Examples High Performance/Low Effect. ..............................................................  Table  2.9.  Examples Low Performance/High Effect. ..............................................................  Chapter 3— CASE STUDY – AIR SUPERIORITY  3.1.  Overview. ................................................................................................................  3.2.  Step 1 – Identify the Strategy..................................................................................  3.3.  Step 2 – Develop Criteria. .......................................................................................  Table  3.1.  Potential End State Criteria. ....................................................................................  Table  3.2.  Potential Ways (Task) Criteria. ...............................................................................  3.4.  Step 3 – Identify Measures and Collect Data. .........................................................  Table  3.3.  List of Potential MOEs. ..........................................................................................  6 8 8 8 9 9 9 10 10 11 11 12 13 13 13 14 14 15 15 16 16 17 17 17 18 18 19 19 20 AFPAM90-1604  4 NOVEMBER 2016 Table  3.4.  List of Potential MOPs. ..........................................................................................  3.5.  Step 4 – Analyze and Present Insights. ...................................................................  Figure  3.1.  Interpretation of Enemy Sortie Generation. ............................................................  Chapter 4— CASE STUDY – UNIT FITNESS  4.1.  Overview. ................................................................................................................  4.2.  Step 1 – Identify the Strategy..................................................................................  Table  4.1.  Possible Ends, Ways, Means of the Strategy. .........................................................  4.3.  Step 2 – Develop Criteria. .......................................................................................  Table  4.2.  Potential End State Criteria. ....................................................................................  Table  4.3.  Potential Ways (Tasks) Criteria. .............................................................................  4.4.  Step 3 – Identify Measures and Collect Data. .........................................................  Table  4.4.  Potential MOEs. ......................................................................................................  Table  4.5.  Potential MOPs. ......................................................................................................  4.5.  Step 4 – Analyze and Present Insights. ...................................................................  Figure  4.1.  Unit Fitness Program Assessment. .........................................................................  Chapter 5— SUMMARY  5.1.  Overview. ................................................................................................................  5.2.  Step 1 – Identify the Strategy..................................................................................  5.3.  Step 2 – Develop Criteria. .......................................................................................  5.4.  Step 3 – Identify Measures and Collect Data. .........................................................  5.5.  Step 4 – Analyze and Present Insights. ...................................................................  5.6.  The key to success in today’s conflicts, and in the future, lies in the ability to adapt – to find a means of gaining continuing advantage. .......................................  Attachment 1— GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION Attachment 2— REFERENCE SHEET STEPS TO ASSESSMENT OF OPERATIONS & STRATEGY    3 20 20 21 22 22 22 22 22 23 23 23 23 24 24 25 26 26 26 26 26 26 26 27 31 4 AFPAM90-1604  4 NOVEMBER 2016 INTRODUCTION TO ASSESSMENT OF OPERATIONS AND STRATEGY Chapter 1 1.1.  Overview. 1.1.1.  Context. In any operation or process, periodic progress reviews are vital to determine progress  toward  achieving  the  desired  objectives.  Those  assessments  evaluate  progress  in creating  the  effects  needed  for  attainment  of  the  objectives.  Evaluations  also  rate  the successful accomplishment of appropriate tasks needed to create those effects. Insights from this review process can support changes to the operation in order to better achieve the desired results  or  better  execute  planned  actions.  The  process  is  accomplished  by  completing  an assessment of operations or strategy. Assessments can be performed at all levels of warfare across  a  wide  array  of  operations,  from  warfighting  campaigns  to  headquarters  staff activities.  When  done  correctly,  an  assessment  consolidates  data  from  many  sources  and summarizes  the  data  clearly,  concisely,  and  in  context.  Assessments  provide  commanders necessary details of the methods used and results produced. They also communicate relevant uncertainty  in  the  data  and  the  associated  risks.  Assessments  can  drive  better  operational planning  and  measurement  of  progress  during  execution,  and  provide  clear,  defensible insights to the operation’s commander and planning staff. 1.1.2.  Meeting  a  commander's  objectives  requires  that  both  planner  and  assessor  select definitive  and  consistent  indicators  to  objectively  show  mission  progress.  Planning  and assessment  personnel  play  critical  roles  in  quantifying  the  duration,  scope,  and  resourcing required  to  achieve  the  desired  effects  and  objectives.  Just  as  it  is  the  assessor's  job  to indicate  the  degree  to  which  the  commander's  objectives  are  being  achieved,  the  planner's role is to build plans in view of the identified success indicators. Neither planner nor assessor should  work  in  a  vacuum.  Due  to  close  coupling  of  the  planner  and  the  assessor,  plans created are readily assessable and thereby support subsequent planning and execution efforts consistent with the overall strategy. That partnership does not imply that the planner and the assessor  should  be  the  same  person,  only  that  their  ongoing  collaboration  throughout  the plan-execute-assess  cycle  benefits  the  efficiency  of  the  cycle  itself  for  commanders, operators,  planners,  and  assessors.  It  is  important  that  assessment  focus  not  get  lost  in  the many  responsibilities  of  the  planning  cell  and  also  that  the  assessors  are  free  to  provide independent  assessment  support  to  the  commander  regarding  the  constructed  plans themselves. 1.1.3.  Goal. The goal of this Air Force Pamphlet (AFPAM) is to describe a widely used and often  useful  approach  to  assessing  operations  and  strategy.  Additionally,  the  detailed examples  included  should  provide  a  starting  point  from  which  to  tailor  other  assessments. This AFPAM is not all-encompassing and should not supplant good judgment. Although the approach outlined in this pamphlet is broadly applicable, it may not fit every situation. 1.1.4.  Scope. Many diverse types of assessment are performed across military commands. In fact,  there  are  over  30  official  Air  Force  (AF)  publications  that  include  the  word “assessment”  in  their  titles.  This  variety  of  uses  gives  the  term  “assessment”  a  vague meaning.  Operational  Assessment  (OA)  can  describe  assessment  as  executed  within  the Strategy  Division  of  the  Air  Operations  Center  (AOC).  Other  operational  assessments  may AFPAM90-1604  4 NOVEMBER 2016 5 include  those  in  test  and  evaluation  with  definitions  described  in  Department  of  Defense Instruction  (DoDI)  5000.02,  Operation  of  Defense  Acquisition  System.  Specifically,  this pamphlet  describes  the  assessment  of  operations  and  strategy.  It  principally  applies  to assessments of warfighting but is also useful for assessments of staff functions and projects. See case studies in Chapters 3 and 4 for respective examples. 1.1.4.1.  Joint  Publication  (JP)  5-0,  Joint  Operation  Planning  provides  guidance  on  not only  planning,  but  also  conducting  analyses  and  assessments.    This  pamphlet  is  not intended to rewrite the recommended assessment procedures and techniques provided in JP  5-0.  It  is  rather  meant  to  supplement  the  methods  described  in  JP  5-0  for  Air  Force assessors.    Air  Force  personnel  are  encouraged  to  understand  how  the  methods  in  this pamphlet  complement  and  apply  those  presented  in  JP  5-0,  and  to  consistently  apply current guidance for Air Force assessments. 1.1.4.2.  This pamphlet also specifically does not address the Air Force Risk Assessment Framework  (RAF),  though  practitioners  may  find  some  methods  in  this  pamphlet  to  be useful in  the context  of risk assessment. Assessment of risk pervades assessments  at  all levels and is more effectively described as an independent methodology in its own right. Risk assessment is of tremendous importance to future Air Force and joint actions. If one imagines  an  operational  or  strategic  plan  and  its  execution  as  a  “window,”  risk assessment essentially looks at the window before passing through it, whereas operational and  strategic  assessment  discussed  in  this  pamphlet  fundamentally  evaluate  success during and after passing through the window. Risk,  and the Air Force Risk  Assessment Framework in particular, looks forward to help senior AF leaders make better decisions for  potential  challenges  for  the  AF,  while  the  methods  in  this  AFPAM  inform  senior leader decisions by looking at the present and past progress and actions toward planned objectives. 1.1.4.3.  This  pamphlet  does  not  alter  existing  responsibilities  of  Air  Force  assessors  of operations  and  strategy  to  take  into  account  the  requirements  of  the  Defense  Critical Asset  Infrastructure  Program  (DCIP),  as  defined  in  DoD  Directive  (DoDD)  3020.40, DoD Policy and Responsibilities for Critical Infrastructure, and account for Joint and Air Force Mission  Assurance Assessment  requirements  and periodicity, as laid out  in DoDI 2000.12,  DoD  Antiterrorism  (AT)  Program,  in  the  Secretary  of  Defense  (SECDEF)  7 May  2012  Mission  Assurance  Strategy  memorandum,  and  in  the  Oct  2013  Mission Assurance Strategy Implementation Framework. 1.2.  Levels of Assessment. 1.2.1.  The  emphasis  of  this  AFPAM  is  operational  and  strategic  assessments;  however, assessments  are  performed  at  all  levels  of  warfare  (i.e.,  tactical,  operational,  and  strategic) and  across  all  types  of  military  commands.  The  focus  of  an  assessment  is  different  at  each level and should be determined by the intent of the commander who directed the assessment. Figure  1.1  lists  some  common  types  of  assessment  and  the  levels  where  each  would  most likely be applied. 6 AFPAM90-1604  4 NOVEMBER 2016 Figure 1.1.  Levels of Assessment.  1.2.2.  Tactical Assessments. Assessments in this context are generally performed at the unit or  component  level  and  measure  task  achievement  or  direct  effects  of  specific  tasks performed.  Effects  are  the  responses  of  the  operational  environment  due  usually  to accomplishment  of  tasks  or  actions.  Accordingly,  time  frames  associated  with  tactical assessments  tend  to  be  relatively  short  with  more  immediate  consequences  to  the  actions considered.  Combat  Assessment  –  an  umbrella  term  covering  Battle  Damage  Assessment (BDA),  non-kinetic  Battle  Damage  Indication  (BDI),  Munitions  Effectiveness  Assessment (MEA), and recommendations for re-attack – is primarily an intelligence function executed at the component level to measure first-order tactical effects of military operations. It is a well-developed approach and  is  described in  greater detail  in  various publications, such as JP 3-60, Joint Targeting. 1.2.3.  Operational  Assessments.  Assessments  at  the  operational  level  begin  to  evaluate complex indirect effects, track progress toward less immediate objectives, and take a broader perspective of the operational environment. OA frequently account for multiple decisions and actions  creating  several  effects  in  different  domains,  all  directed  toward  those  specific objectives.  Time  frames  for  OA  are  typically  longer,  though  they  can  vary  widely  with  the particular  objectives,  domains,  and  capabilities  involved  in  an  operation.  Based  on measurements of progress toward intended effects and objectives, OA often are used to make recommendations  for  adjustments  in  plans,  as  well  as  for  changes  in  resource  usage  and future action extending beyond re-attack. 1.2.4.  Strategic  Assessments.  Strategic-level  assessments  can  address  issues  at  the  service, joint  force and national  levels  and involve a wide array of methodologies, participants, and inputs  with  broad  effects  on  investment  strategy,  budget  allocation,  and  strategic requirements. Assessments often consider, as listed in the 2014 Quadrennial Defense Review (QDR),  economic,  diplomatic,  intelligence,  law  enforcement,  development,  and  military tools  for  accomplishing  strategic  objectives.  This  pamphlet  specifically  focuses  on assessment  of  Air  Force  strategy  in  a  joint  or  coalition  environment.  Air  Force  strategy AFPAM90-1604  4 NOVEMBER 2016 7 supports  achievement  of  the  objectives  listed  above  by  leveraging  its  core  functions,  and deserves  critical  evaluation  of  its  effectiveness.  These  assessments  thereby  can  evaluate strategy at its appropriate level. Longer time frames are necessary to assess the broad aspects of strategy. 1.2.5.  The  time  frames  considered  by  the  various  assessments  may  vary  widely,  and  the relationships  among  the  various  assessments  are  not  linear.  Outputs  from  one  assessment often  feed  multiple  other  assessments.  Appropriately  scoped  to  a  particular  end  state,  the methodologies presented in Chapter 2 of this pamphlet are likewise applicable at any of the levels of assessment. 8 AFPAM90-1604  4 NOVEMBER 2016 Chapter 2 ASSESSMENT OF OPERATIONS AND STRATEGY 2.1.  Process Overview. 2.1.1.  An assessment of operations and strategy is designed to give a commander and his or her  staff  insight  into  whether  the  commander’s  operations  and  strategy  are  effective  and  to measure progress of the operations toward that strategy’s desired end state. This is done by developing an assessment framework. 2.1.2.  The goal of any of these assessments is to develop logically defensible judgments into the  effectiveness  and  conduct  of  an  operation  or  strategy,  to  determine  areas  in  need  of  re-evaluation,  and  then  to  support  the  development  of  COAs  toward  the  current  or  updated process. Given the fluid nature of complex military operations involving high-order effects, judgment  is  an  intrinsic  part  of  any  assessment.  Instead  of  developing  criteria  or  measures that take all judgment out of the process, the goal is to build an assessment framework for the development  of  logically  defensible  judgments.  The  four  steps  are:    1)  identify  strategy,  2) develop  criteria,  3)  identify  measures  and  collect  data,  and  4)  analyze  and  present  insights. Figure 2.1 depicts the steps for conducting an assessment of operations and strategy, wherein ‘MOEs’ are measures of effectiveness and ‘MOPs’ are measures of performance. Figure 2.1.  Steps in the Assessment of Operations and Strategy. 2.1.3.  The  assessment  approach  should  also  account  for  the  cost  for  the  assessment,  in relation to the importance of the desired effect or the level of effort of the operation.  AFPAM90-1604  4 NOVEMBER 2016 9 2.2.  Step 1 – Identify Strategy. 2.2.1.  The  strategy  consists  of  four  components:  problem  identification/diagnosis  and advantages  desired  upon  dealing  with  the  problem  or  challenge  (ends),  guiding  policy  to achieve  the  ends  sought,  and  coherent  actions  designed  to  enact  guiding  policy  using appropriate ways and means. The Ends, Ways, and Means, in particular, are defined in Table 2.1.  Developing  these  components  in  order  to  achieve  the  commander’s  intent  is  critical  to the success of an assessment. The integration of these elements at an acceptable level of risk constitutes a strategy. Table 2.1.  Definitions of Ends, Ways, and Means. 2.2.2.  Assessment  begins  with  the  assessment  team  assisting  in  strategy  and  planning development  to  ensure  they  understand  the  desired  criteria  and  develop  an  appropriate assessment  approach.  Commander’s  intent,  mission  statements,  and  various  forms  of guidance  may  be  used  to  develop  desired  effects.  However,  care  should  be  taken  to  ensure effects  are  not  written  in  terms  of  planned  tasks,  but  are  instead  stated  as  the  desired conditions  created  by  successful  completion  of  tasks.  Those  conditions  then  sufficiently characterize successful achievement of the desired objective that should result from the tasks.  2.3.  Step 2 – Develop Criteria. 2.3.1.  Criteria define the attributes and thresholds for judging progress toward the end state itself and the use of resources (means) in the accomplishment of required tasks (ways) taken to  achieve  those  ends.  Development  of  assessment  criteria  is  the  critical  component  of  the assessment  process  and  should  be  accomplished  before  specific  measures  or  data requirements are defined. Developing measures without a clear understanding of how those measures  fit  into  a  judgment  of  the  effectiveness  of  the  overall  strategy  often  leads  to laborious  data  collection  and  analysis  processes  that  provide  little  value  to  the  decision makers.  Spending  additional  time  to  thoroughly  consider  and  develop  meaningful  and 10 AFPAM90-1604  4 NOVEMBER 2016 relevant  assessment  criteria  will  help  to  avoid  this  pitfall.  Defining  success  and  failure criteria can be extremely useful in focusing the assessment. 2.3.2.  Criteria help focus data collection by ensuring that assessment measures relate clearly to  the  elements  of  the  operations  and  strategy  being  assessed.  As  data  are  collected,  the criteria  translate  the  data  into  meaningful  insights  on  the  operation  or  strategy.  Criteria should be developed for the ends, ways, and means at each level of the assessment. See Table 2.2 for basic attributes of well-written criteria. Table 2.2.  Attributes of Well-written Criteria. Table 2.3 shows examples of criteria that could be used for effects.  Table 2.3.  Example Criteria for Effects.    AFPAM90-1604  4 NOVEMBER 2016 11 Table 2.4 shows examples of criteria that could be used for performance. Table 2.4.  Example Criteria for Performance. 2.4.  Step 3 – Identify Measures and Collect Data.  2.4.1.  Assessment measures are simply the data elements that, when viewed in relation to the criteria,  will  provide  insights  into  the  effectiveness  of  the  operation  or  the  commander’s strategy.  Two  common  types  of  measures  are  Measures  of  Effectiveness  (MOEs)  and Measures of Performance (MOPs). MOEs are used to  assess progress toward the end state, while  MOPs  address  the  ways  and  means  that  are  being  executed  to  help  achieve  the  end state. Simply put, MOEs provide an indication of progress toward achieving desired effects or objectives, while MOPs provide an indication of progress toward accomplishing planned tasks  or  actions.  Developing  these  measures  should  be  straightforward  if  the  criteria  are thorough and meet the basic attributes listed above. If the measures are difficult to develop, the criteria are likely not as mature as they need to be. 2.4.2.  The  relationship  between  MOEs  and  MOPs  only  exists  within  the  context  of  the relevant  commander’s strategy. For example, as shown in Figure 2.2, a measure used as an MOE  for  the  creation  of  a  supporting  commander’s  effect  may  relate  very  closely  to  a supported  commander’s  task  required  to  create  a  higher  echelon  effect.  Dropping  a  bridge might  represent  a  task  –  measured  via  an  MOP  –  within  the  operational  commander’s strategy of cutting enemy lines of communication and preventing enemy  movement.  At the same time, creating the tactical effect of impeded enemy ability to cross the river would be measured via an MOE for the squadron commander tasked with flying the bombing mission. That relationship does not imply that an MOP at one level is an MOE at another level, only that the measures for one commander’s task accomplishment may be closely tied to measures for  the  creation  of  another  commander’s  effect.  This  guidance  does  not  supersede  the guidance  in  AFI  99-103,  Capabilities-Based  Test  and  Evaluation,  on  the  development  of MOEs  and  MOPs  related  to  the  testing  processes  associated  with  the  development  of  a materiel solution. 12 AFPAM90-1604  4 NOVEMBER 2016 Figure 2.2.  Assessment Levels – Example.  2.4.3.  Availability  of  data  is  an  important  consideration  when  developing  measures  but cannot  be  assumed.  A  range  of  sources  –  to  include  the  various  divisions  in  a  planning  or headquarters staff, intelligence reports, and inputs from supported and supporting commands – should be used to gather the required data. Much of this information may be available from relevant  Communities  of  Practice  (COPs)  or  Communities  of  Interest  (COIs),  program  and enterprise architectures, and other shared resources. These sources may generate a wide array of data in various formats and quality: quantitative and qualitative, objective and subjective, observed and inferred. Assessors should coordinate with owning organizations to determine the  quality  and  quantity  of  data  those  organizations  can  provide  prior  to  the  start  of  the operation.    This  coordination  is  crucial  to  developing  useful  and  measurable  assessment criteria. 2.4.4.  Developing good measures is an art, though there are some general guidelines that can aid in developing high-quality measures. 2.4.4.1.  Measures should represent a scale, not a goal. Assessors may be tempted to write goals  or  criteria  as  measures.  Instead,  goals  should  be  included  in  the  criteria  in accordance  with  (IAW)  the  commander’s  risk  tolerance  and  thresholds.  Operators  and planners should establish these goals in coordination with the assessors. Table 2.5 shows examples of measures that could be used. AFPAM90-1604  4 NOVEMBER 2016 13 Table 2.5.  Example Measures – Scale.  2.4.4.2.  Data  satisfying  a  measure  should  be  observable,  or  at  least  inferable.  The measurements  may  be  either  quantitative  (numerical)  or  qualitative  (non-numerical).  In general,  objective  measures  are  usually  preferred  because  they  reduce  uncertainty  and variability in the final product. Assessors should avoid blindly using rates, numbers, and other  quantitative  metrics,  especially  in  assessing  effects,  since  these  can  often  lack  a direct linkage to the objectives or ends outlined in the strategy. Table 2.6 shows examples of good and bad measures for both qualitative and quantitative measures. Table 2.6.  Example Measures - Observable/Inferable. 2.4.4.3.  Measures should be clear and concise. They should be written in plain language so  that  someone  with  no  prior  knowledge  of  the  measures  can  still  understand  the  data requirements. See Table 2.7 for examples.  Table 2.7.  Example Measures – Clear and Concise.  2.4.5.  Measures  may  need  to  be  refined  or  amended  as  the  operational  situation  changes. Selection of assessment measures is an iterative, ongoing effort. 2.4.6.  All elements of the strategy need to be considered; however, not all the elements need be  measured.  Attempting  to  assess  too  many  measures  can  paralyze  the  assessment  effort. Consider the value to the end result before adding more measures. After assessors have built the entire set of measures, they should conduct a final review to identify those measures that have  less  relative  importance/contribution  or  take  inordinate  effort  relative  to  the  insight provided, and remove them from the set. Assessment teams should prioritize their efforts to best  support  the  commander’s  decision  making  needs.  Prioritization  of  assessment  efforts 14 AFPAM90-1604  4 NOVEMBER 2016 should  also  account  for  the  cost  in  dollars  and  manpower  to  collect  and  conduct  the assessment, as compared to the cost of the overall effort being assessed. 2.5.  Step 4 – Analyze and Present Insights. 2.5.1.  The  purpose  of  an  assessment  is  not  merely  to  report  on  the  measures,  but  rather  to interpret  the  realization  of  the  measures  and  their  criteria  to  provide  analytically  supported insights  into  the  effectiveness  of  the  operation  and  strategy.  The  overall  assessment  can generally  be  broken  into  two  major  pieces:  the  effect  and  performance  assessments.  The effect  assessment  (based  on  the  MOEs)  should  provide  the  commander  with  the  overall picture  of  progress  toward  objective  or  end  state  achievement  or  of  the  creation  of  effects contributing to those ends. The performance assessment (based on MOPs) should provide the commander with  an overall picture of how well planned tasks (i.e., the strategy’s ways and means)  are  being  executed.  Figure  2.3  provides  a  framework  with  which  to  compare evaluations of effects and performance results. 2.5.2.  The  relationship  between  the  effect  assessment  and  performance  assessment  can  be characterized in two ways: expected results or unexpected results. 2.5.2.1.  Expected  Results.  In  the  first  case,  similar  effect  and  performance  assessments suggest  the operation is  succeeding or failing  as  expected.  In other words, the tasks are being  performed  as  planned  with  the  desired  effects  being  achieved,  or  assessment  of poor  task  performance  is  correlated  with  a  lack  of  generation  of  desired  effects.  Either one would constitute an expected result. Note that an observed correlation between effect and  performance  does  not  necessarily  imply  causality.  Assessors  should  continue  to monitor for any changes to the apparent equilibrium. Figure 2.3.  Expected Results. 2.5.2.2.  Unexpected Results.  2.5.2.2.1.  Disconnects  between  effect  and  performance  assessments  indicate  that portions  of  the  plan  may  require  further  examination.  A  high  performance  value paired with a low effect value, as shown in Figure 2.4, is an indication the completion of planned tasks is not leading to the desired effects. Numerous issues may be driving the result, including data latency, delayed effects, or a misunderstanding of the enemy system. See Table 2.8 for examples. AFPAM90-1604  4 NOVEMBER 2016 15 Figure 2.4.  High MOP and Low MOE. 2.5.2.2.2.  Conversely, a high value for effect paired with a low value for performance (desired effects are being achieved without the completion of corresponding tasks), as shown  in  Figure  2.5,  indicates  there  may  be  a  reason  to  re-evaluate  the  chosen strategy.  Again,  numerous  issues  including  data  latency,  enemy  deception,  good fortune,  and  misunderstanding  of  the  enemy  system  could  lead  to  this  apparent contradiction. Table 2.9 provides examples.  Figure 2.5.  High MOE with Low MOP. 2.5.2.2.3.  The  assumptions  about  the  direct  links  between  the  achievement  of  tasks and the effects they support may be flawed. In this situation, the primary focus of an assessment should be to identify and highlight these imbalances to the strategists and planners, so they can recommend changes to the strategy, plan, or operation.  16 AFPAM90-1604  4 NOVEMBER 2016 Table 2.8.  Examples High Performance/Low Effect. Table 2.9.  Examples Low Performance/High Effect.   2.5.3.  Understanding  the  relationships  between  effect  and  performance  is  critical  to interpreting progress in the strategy and to revealing options for planners and commanders to modify their approach accordingly. Identifying these opportunities will allow the commander to  execute  operations  more  effectively  and  efficiently.  However,  it  is  also  critical  to understand  why  there  is  an  unexpected  result,  so  commanders  and  planners  can  most appropriately fine tune the operations or strategy. 2.5.4.  The final step is to share with other organizations the results and lessons identified in the  assessment:  through  the  Joint  Lessons  Learned  Information  System  (JLLIS)  at https://www.jllis.mil,  COPs,  COIs,  enterprise  architecture,  or  the  Air  Force  Operations Assessment  Working  Group  (OAWG),  or  other  means.  This  critical  step  helps  to  ensure lessons  identified  in  the  assessments  become  lessons  learned  and  shared,  so  they  do  not become lessons forgotten. 2.5.4.1.  While the conceptual results are important to developing overall insights into the effectiveness  of  the  strategy  being  assessed,  those  insights  can  be  lost  if  they  are  not presented in a succinct, easy-to-understand form  for the commander. Many presentation options  exist,  from  showing  the  current  state  of  the  assessment  to  including  trending information  that shows progress  over the course of the operation. The specific form for displaying  these  results  can  vary  widely  based  on  the  specific  operation  and  the preference of the commander. 2.5.4.2.  For reference, attachment 2 of this AFPAM is a concise, single page summary of the steps to assessment of operations and strategy. AFPAM90-1604  4 NOVEMBER 2016 17 Chapter 3 CASE STUDY – AIR SUPERIORITY 3.1.  Overview.  This example is an assessment of air superiority in a generic scenario. It is not specific to any theater or Operation Plan (OPLAN); specific OPLANs would require their own assessment criteria and list of key tasks and objectives based on the details of those plans. This example  only  considers  one  small  portion  of  the  overall  effort  that  the  assessment  team  would have to undertake in providing support to all elements of the commander’s strategy. Despite the example’s specificity for air superiority, the concepts and processes discussed can be applied to any  assessment  and  provide  a  concrete  starting  point  for  an  analyst  tasked  with  providing  an assessment of operations and strategy. Assessments are never completely straightforward with a single  correct  solution.  The  best  approach  is  the  one  that  best  provides  logically  supported, relevant, and timely feedback to the commander. 3.2.  Step 1 – Identify the Strategy. 3.2.1.  For a given theater of operations, one of the commander’s objectives might be to gain and maintain air superiority. Given this objective, planners would develop ways and means, and consult with assessment experts to determine reasonable end states. 3.2.2.  Definition.  The  assessment  should  use  accepted  doctrinal  definitions  to  the  greatest extent  possible.  Thus,  this  example  uses  JP  1-02’s  definition  of  air  superiority  as  “That degree of dominance in the air battle by one force that permits the conduct of operations at a given time and place without prohibitive interference from air and missile threats.” 3.2.3.  Scope. 3.2.3.1.  When scoping the assessment, keep in mind that efforts are typically constrained by many outside influences such as regulations,  standards, and laws. A major driver for many assessments is the limiting factors of existing equipment, software, processes, and available  data.  Other  influences  may  include  security  or  safety  concerns.  Thus, identifying  and  managing  the  scope  of  any  assessment  is  an  important  step  early  in  the process. 3.2.3.2.  For  this  example,  the  Joint  Force  Air  Component  Commander  (JFACC)  is  the commander  who  is  supported  by  the  assessment.  The  JFACC’s  responsibilities  include gaining  and  maintaining  air  superiority  in  order  to  enable  Joint  Force  Commander’s (JFC)  objectives.  Note  from  the  definition  above  that  air  superiority  enables  other operations.  Therefore,  the  assessment  should  consider  the  operations  it  enables.  This could include air, land, and sea operations and would require consultation with supported commanders.  While  theater  ballistic  missile  defense  is  sometimes  included  under  air superiority, it is not considered in this example. 3.2.4.  Statement  of  Strategy.  Once  applicable  definitions  are  clear  and  the  assessment  is scoped  to  the  appropriate  level,  assessors  should  work  with  the  JFACC  and  other  staff elements to develop the end state, ways and means that will be assessed. This is an iterative process,  working  to  converge  on  a  reasonably  assessable  end  state,  ways,  and  means.  The strategy laid out below is clearly much simpler than it would be in a real scenario, where the ways and means should specify the specific tasks (e.g., numbers and types of missions flown) 18 AFPAM90-1604  4 NOVEMBER 2016 and the resources tied to those tasks (e.g., aircraft, support forces). While an important aspect of  strategy,  the  means,  are  not  used  in  this  example.  Delving  into  means  is  crucial  for conducting root cause analysis, but not necessarily needed here to interpret the effects versus performance described in subsequent steps. 3.2.4.1.  End  State.  All  friendly  air,  land,  and  sea  operations  are  free  from  prohibitive interference  by  opposing  Integrated  Air  Defense  Systems  (IADS),  and/or  opposing  air, space, and cyber capabilities. 3.2.4.2.  Ways.  Offensive  Counterair/Defensive  Counterair,  Suppression  of  Enemy  Air Defenses, and Intelligence, Surveillance, and Reconnaissance (ISR) missions. 3.3.  Step 2 – Develop Criteria. 3.3.1.  Based on the strategy developed in step 1, criteria should next be developed to specify the  attributes  and  standards  that  must  be  observed  in  order  to  determine  criteria  categories. The number of categories, their titles (e.g., good, acceptable, marginal, or poor) and how they are  presented  (e.g.,  stoplight  chart,  slider  bars)  all  depend  on  the  specific  operation  being assessed and the preferences of the commander. 3.3.2.  The end state criteria should directly address the end state: whether enemy air has had prohibitive interference on friendly operations. Possible criteria are shown in Table 3.1. Table 3.1.  Potential End State Criteria. 3.3.2.1.  Any  nonstandard or confusing terms  should  be fully defined in  order to  ensure consistent application of the criteria. For example, the term “significant friendly attrition” should  be  defined  relative  to  the  JFACC’s  risk  tolerance  and  to  attrition’s  impact  on sustaining planned operations per the plan being executed. 3.3.2.2.  Similarly, criteria for the ways (i.e., tasks taken to achieve air superiority) should take form similar to the examples shown in Table 3.2.  AFPAM90-1604  4 NOVEMBER 2016 19 Table 3.2.  Potential Ways (Task) Criteria.  3.3.3.  Many outcomes could potentially result from any operation, and it may be difficult to capture all of these outcomes within the criteria categories. For example, many air superiority tasks may exist making it difficult to partition their levels of accomplishment into a few clear categories.  A  useful  way  to  partition  the  space  of  potential  outcomes  within  the  criteria statements is to break the tasks into broad categories. In the example above, these categories were operational level missions and critical operational level missions. This partition allows the  assessors,  through  consultation  with  planners  and  Subject  Matter  Experts  (SMEs),  to group missions into these broad categories. Be aware, however, that too many categories can produce  overly  complex  assessment  results  that  are  difficult  to  understand,  while  too  few categories might oversimplify. 3.4.  Step 3 – Identify Measures and Collect Data. 3.4.1.  The  next  step  in  the  assessment  process  is  to  develop  measures  and  collect  data  to fulfill those measures. Metric development should flow easily from well-developed criteria. MOEs  are  those  measures  necessary  to  assess  progress  toward  the  end  state  criteria,  while MOPs measure task performance based on the ways and means of the strategy. 3.4.2.  MOEs. 3.4.2.1.  Based on the end state criteria outlined above, Table 3.3 could be a potential list of MOEs for the assessment. 3.4.2.2.  Notice  that  these  measures  would  require  consultation  with  the  supported commanders  whose  missions  (air,  land,  and  sea)  are  enabled  by  air  superiority.  It  is important  to  ensure  these  commands  provide  adequate  supporting  details  with  their feedback,  so  that  the  air  commander  and  planning  staff  can  make  concrete  strategy adjustments if necessary. 3.4.2.3.  MOEs  2,  3,  and  4  in  Table  3.3  need  to  be  interpreted  by  air,  land  and  naval SMEs who can translate the raw numbers into useful information on what certain levels of attrition mean to their operations and objectives. 20 AFPAM90-1604  4 NOVEMBER 2016 Table 3.3.  List of Potential MOEs. 3.4.3.  MOPs.  3.4.3.1.  The ways/means criteria suggest the MOPs listed in Table 3.4 to judge how well the assigned tasks are accomplished. Table 3.4.  List of Potential MOPs. 3.4.3.2.  Like the MOEs above, the MOP values reported need to be interpreted by SMEs to yield actionable feedback to the JFACC.  3.5.  Step 4 – Analyze and Present Insights. 3.5.1.  Analysis.  Analysis  can  take  many  forms  within  the  assessment,  ranging  from statistical  analysis  of  quantitative  data  to  gathering  questionnaire  data  to  compiling  SME inputs. The type of analysis used will depend on a number of factors, including the type of data  being  analyzed  and  how  much  time  is  allowed  for  the  assessment  process  (an  80% solution  in  time  to  inform  a  commander’s  decision  is  better  than  a  100%  solution  after  the decision has already been made). 3.5.1.1.  For  example,  MOEs  2  and  3  involve  SME  judgment  from  the  supported commands; thus, complex statistical analysis would not be appropriate. Simply reporting the  SME  judgment  with  supporting  details  might  be  the  appropriate  level  of  analysis. However, for MOEs 1 and 4, more intricate analysis on the percentages and their trends might  be  in  order.  In  the  end,  analysis  should  serve,  rather  than  drive,  the  assessment process. 3.5.2.  Conceptual  Results.  Assessors,  in  consultation  with  planners,  strategists,  and  SMEs, should  use  their  analysis  to  develop  the  conceptual  results  of  the  assessment  in  order  to provide useful insights to the commander on the effectiveness of the strategy. 3.5.2.1.  As described in Chapter 2, comparing the effect assessment (via the MOEs) and the  performance  assessment  (via  the  MOPs)  can  yield  various  insights  into  the  strategy being  assessed.  Figure  3.1  displays  these  insights  for  one  portion  of  the  air  superiority objective: offensive strike missions on enemy airfields to limit enemy sortie generation. The desired effect is that the enemy aircraft are not able to fly; the task is to destroy the enemy runways. 3.5.2.2.  These  four  basic  results  can  be  used  to  provide  insights  to  the  effectiveness  of the strategy and progress toward accomplishment of JFACC objectives. Note that while Figure 3.1 shows four distinct  quadrants,  the outcomes will most likely not  be as clear-AFPAM90-1604  4 NOVEMBER 2016 21 cut.  Sharing  the  assessment  conclusions  with  various  elements  of  the  planning  staff  is vital  to  providing  a  useful  judgment  of  strategy  success  to  the  commander  and  to influencing potential subsequent changes to planned COAs. Figure 3.1.  Interpretation of Enemy Sortie Generation. 3.5.3.  Assessment Results Presentation and Lessons Learned Dissemination. 3.5.3.1.  Process observations through the lessons learned process IAW AFI 90-1601, Air Force Lessons Learned Program, and CJCSI 3150.25F, Joint Lessons Learned Program, AFI 10-1301, Air Force Doctrine Development, and within the OAWG.  3.5.3.2.  Share    lessons    identified    in    the    assessment    with    the    broader    joint  community the  JLLIS  website  at https://www.jllis.mil.   by  uploading  all lessons identified into 22 AFPAM90-1604  4 NOVEMBER 2016 Chapter 4 CASE STUDY – UNIT FITNESS 4.1.  Overview.  This example is an assessment of an Air Force unit’s overall fitness level. This is an oversimplified example of how to apply the methodology described above, but the concepts and processes discussed can be applied to any assessment and provides a concrete starting point for  an  analyst  tasked  with  providing  an  assessment  of  operations  and  strategy.  As  with  any assessment,  the  best  approach  is  the  one  that  best  provides  logically  supported,  relevant,  and timely feedback to the commander. 4.2.  Step  1  –  Identify  the  Strategy.  All  unit  commanders  are  concerned  with  ensuring personnel are mission ready. Since part of being mission ready requires a minimum fitness level, unit commanders typically have a goal of 100% of personnel achieving a passing score on their (bi)annual fitness evaluation. Given this objective, unit fitness monitors would develop ways and means, and consult with assessment experts to determine measures that would indicate progress. 4.2.1.  Definition.  In  order  to  assess  unit  fitness,  there  has  to  be  an  acceptable  definition  of physical  fitness.  Physical  fitness  generally  refers  to  a  person’s  ability  to  be  healthy,  resist diseases, work effectively, and respond to emergency situations. The Air Force’s objective is to ensure members are physically prepared to meet expeditionary mission requirements. For this assessment, we will consider members fit if they are able to earn a composite score of 75 or  greater  and  meet  the  minimum  component  scores  as  identified  in  AFI  36-2905,  Fitness Program. 4.2.2.  Scope. For this example, the scope of the assessment is  constrained to  the personnel within the fitness  monitor’s unit. The commander being supported by the assessment is  the fitness monitor’s unit commander, whose responsibilities include maintaining readiness. 4.2.3.  Statement  of  Strategy.  Once  applicable  definitions  are  clear  and  the  assessment  is scoped to the appropriate level, assessors (unit fitness monitor in this case) should work with the unit commander to develop the end state, ways and means that will be assessed.  This is an  iterative  process,  working  to  converge  on  a  reasonably  assessable  end  state,  ways  and means. Possible ends, ways, and means for this strategy are listed in Table 4.1. Table 4.1.  Possible Ends, Ways, Means of the Strategy. 4.3.  Step 2 – Develop Criteria.  4.3.1.  Based on the strategy developed in step 1, criteria should next be developed to specify the attributes and standards that must be observed in order to produce certain categories. 4.3.2.  The  end  state  criteria  should  directly  address  the  end  state:  whether  100%  of  unit personnel pass the test or not. Table 4.2 lists potential end state criteria. AFPAM90-1604  4 NOVEMBER 2016 23 Table 4.2.  Potential End State Criteria.  4.3.3.  Any  nonstandard  or  confusing  terms  should  be  fully  defined  in  order  to  ensure consistent  application  of  the  criteria.  For  example,  “unit  improvement”  could  be  better defined as a specific percentage increase in the unit’s passing rate. The commander and unit fitness  monitor  would  have  to  decide  how  much  of  an  increase  indicates  the  unit  is  truly improving. 4.3.4.  Similarly,  criteria  for  the  ways  (or  the  tasks  taken  to  achieve  a  100%  passing  rate) should also be developed as exemplified in Table 4.3. Table 4.3.  Potential Ways (Tasks) Criteria.  4.3.5.  As  above,  “high  participation”  should  be  defined.  For  example:  High  Participation means all members who have failed bi-annual fitness test participate at least 90% of time and other members participate at least 75% of the time. 4.4.  Step 3 – Identify Measures and Collect Data. 4.4.1.  The  next  step  in  the  assessment  process  is  to  develop  measures  and  collect  data  to fulfill those measures. Metric development should flow easily from well-developed criteria. MOEs  are  those  measures  necessary  to  assess  progress  toward  the  end  state  criteria,  while MOPs measure task performance based on the ways and means of the strategy. 4.4.2.  Measures of Effectiveness. 4.4.2.1.  Based  on  the  end  state  criteria  outlined  above,  Table  4.4  is  a  potential  list  of MOEs for the assessment. Table 4.4.  Potential MOEs. 4.4.3.  Measures of Performance.  4.4.3.1.  The ways/means criteria suggest the MOPs shown in Table 4.5. 24 AFPAM90-1604  4 NOVEMBER 2016 Table 4.5.  Potential MOPs.  4.4.3.2.  Like the MOEs above, the MOP percentages reported need to be interpreted by SMEs to yield actionable feedback to the commander. 4.4.4.  The  data  collection  task  should  not  overwhelm  the  assessment  effort.  Spending  time understanding the strategy, developing criteria and identifying MOEs/MOPs should focus the collection on only the data truly necessary to produce useful assessment results. 4.5.  Step 4 – Analyze and Present Insights. 4.5.1.  Analysis.  The  approach  to  analysis  within  an  assessment  will  vary  according  to  the particular requirements of the assessment. Analytic techniques useful in assessments include, among  others,  statistical  analysis  of  quantitative  data,  gathering  questionnaire  data,  and compiling  SME  inputs.  The  type  of  analysis  used  will  depend  on  a  number  of  factors including  the  type  of  data  being  analyzed  and  the  amount  of  time  available  to  conduct  the assessment (an 80% solution in time to inform a commander’s decision is better than a 100% solution after the decision has already been made). For example, MOE 2 can provide useful insights into the unit’s fitness, however, at the end of the year the only test that matters is the official  test  result  for  each  individual.  In  the  end,  analysis  should  serve,  not  drive,  the assessment. 4.5.2.  Conceptual  Results.  Assessors,  in  consultation  with  the  unit  commander  and  the fitness monitor, should use their analysis to develop the conceptual results of the assessment in order to provide useful insights to the commander on the effectiveness of the strategy. 4.5.2.1.  As described in Chapter 2, comparing the effect assessment (via the MOEs) and the  performance  assessment  (via  the  MOPs)  can  yield  various  insights  into  the  strategy being assessed. Figure 4.1 displays these insights for looking at the effect the commander desires (100% fitness test pass rate) against the tasks performed (unit fitness program). 4.5.2.2.  These four basic results can be used to provide insights to the commander on the effectiveness of the strategy and progress toward accomplishment of objectives. Note that while Figure 4.1 shows  four distinct quadrants,  the outcomes will most likely not  be as clear-cut.  Results  may  have  to  be  vetted  through  various  SMEs  in  order  to  better understand the assessment results. AFPAM90-1604  4 NOVEMBER 2016 25 Figure 4.1.  Unit Fitness Program Assessment. 4.5.3.  Assessment Results Presentation and Lessons Learned Dissemination. 4.5.3.1.  Process observations through the lessons learned process IAW AFI 90- 1601, Air Force Lessons Learned Program, CJCSI 3150.25F, Joint Lessons Learned Program, AFI 10-1301, Air Force Doctrine Development, and within the OAWG.  4.5.3.2.  Share    lessons    identified    in    the    assessment    with    the    broader    joint  community the  JLLIS  website  at https://www.jllis.mil.   by  uploading  all lessons identified into 26 AFPAM90-1604  4 NOVEMBER 2016 Chapter 5 SUMMARY 5.1.  Overview.  Whether  working  in  the  AOC,  Task  Force,  NAF,  MAJCOM,  or  HAF,  an analyst needs to have an assessment strategy to support the respective commander. While every situation is unique, the basic approach described in this pamphlet provides a useful structure for accomplishing these assessments. The process focuses on understanding the underlying strategy and developing the criteria that support critical elements of the strategy. From there, developing metrics  is  straightforward.  Determining  the  conceptual  results  of  the  assessment  will  require judgment and interpretation from various staff divisions and SMEs, while displaying the results will  depend  on  the  particulars  of  the  operation  being  assessed  and  on  the  preferences  of  the commander. These four basic steps to the assessment of operations and strategy are summarized below. 5.2.  Step 1 – Identify the Strategy.  Assessments should flow from a keen understanding of the commander’s intent. Developing end states, ways, and means to achieve the commander’s intent at an acceptable risk level is critical to the success of an assessment. 5.3.  Step  2  –  Develop  Criteria.  Criteria  define  the  attributes  and  thresholds  for  judging progress toward the end state and accomplishment of required tasks. They help ensure that only relevant and necessary data are collected and that consistent and logical feedback is provided to the commander. 5.4.  Step  3  –  Identify  Measures  and  Collect  Data.  Assessment measures – including MOEs and MOPs are drafted as part of operational design and planning, and should relate directly to the criteria they are supporting. Data for assessments exist throughout the operational environment, and collection requires the concerted efforts of those responsible for a given level of assessment, along  with  other  agencies,  governments,  and  partners.  COPs,  COIs,  program  and  enterprise architectures, and other shared resources have a wealth of information. 5.5.  Step  4  –  Analyze  and  Present  Insights.  Analysts  should  look  critically  at  the  data emerging  from  data  collection.  Well-defined  assessment  criteria  and  measures  should  ensure relevant  data  are  being  collected.  Analysts  should  continually  evaluate  the  usefulness  of  the collected  data  to  the  assessment.  Based  on  analyses,  strategists  and  planners  may  make recommendations  ranging  from  a  simple  re-attack  on  a  task  to  the  major  re-direction  of  a campaign.  Successful  adaptation  requires  constant  review  of  assessment  criteria,  analyses,  and recommendations for future action to commanders at all levels. 5.6.  The key to success in today’s conflicts, and in the future, lies in the ability to adapt – to find a means of gaining continuing advantage.  Assessments can help guide this adaptation by providing  meaningful  feedback  to  commanders  at  all  levels  on  the  effectiveness  of  their operations and strategy.  KEVIN E. WILLIAMS, SES, DAF Director, Studies, Analyses and Assessments AFPAM90-1604  4 NOVEMBER 2016 27 GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION Attachment 1 References DoDD 3020.40, DoD Policy and Responsibilities for Critical Infrastructure, 14 January 2010 (As Amended Through 21 September 2012) DoDI 2000.12, DoD Antiterrorism (AT) Program, 1 March 2012 (As Amended Through 9 September 2013) DoDI 5000.02, Operation of Defense Acquisition System, 7 January 2015 SECDEF Mission Assurance Strategy memorandum, 7 May 2012 Mission Assurance Strategy Implementation Framework, October 2013 JP 1-02, Department of Defense Dictionary of Military and Associated Terms, 8 November 2010 (As Amended Through 15 January 2016) JP 3-60, Joint Targeting, 31 January 2013 JP 5-0, Joint Operation Planning, 11 August 2011 AFI 90-1601, Air Force Lesson Learned Program, 18 December 2013 AFI 10-1301, Air Force Doctrine Development, 14 June 2013 AFI 33-360, Publications and Forms Management, 1 December 2015 AFI 36-2905, Fitness Program, 21 October 2013 (As Amended Through 27 August 2015) AFI 99-103, Capabilities-Based Test and Evaluation, 16 October 2013 AFMAN 33-363, Management of Records, 1 March 2008 AFPD 90-16, Air Force Studies and Analyses, Assessments and Lessons Learned, 31 August 2011 Air Force Operations Assessment Working Group Charter, 11 October 2011 Prescribed Forms None Adopted Forms AF Form 847, Recommendation for Change of Publication Abbreviations and Acronyms AF (or USAF)—United States Air Force AF/A9—Headquarters Air Force Studies, Analyses, and Assessments directorate AFI—Air Force Instruction AFMAN—Air Force Manual AFPAM—Air Force Pamphlet 28 AFPAM90-1604  4 NOVEMBER 2016 AFPD—Air Force Policy Directive AFRIMS—Air Force Records Information Management System AF/CC (or CSAF)—Air Force Chief of Staff AF/CV (or VCSAF)—Air Force Vice Chief of Staff AOC—Air Operations Center ATO—Air Tasking Order BDA—Battle Damage Assessment (kinetic assessment) BDI—Battle Damage Indication (non-kinetic assessment) CAP—Combat Air Patrol CFACC—Combined Force Air Component Commander CJCS—Chairman of the Joint Chiefs of Staff CJCSI—CJCS Instruction COA—Course of Action COI—Community of Interest CONOPS—Concept of Operations COP—Community of Practice C2—Command and Control DAF—Department of the Air Force DoD—Department of Defense DoDD—DoD Directive DoDI—DoD Instruction DOTMLPF-P—Doctrine,  Organization,  Training,  Materiel,  Leadership  and  education, Personnel, Facilities, and Policy DTIC—Defense Technical Information Center FY—Fiscal Year HAF—Headquarters Air Force IADS—Integrated Air Defense System IAW—In Accordance With ISR—Intelligence, Surveillance and Reconnaissance JFACC—Joint Force Air Component Commander JFACC—Joint Force Air Component Commander JFC—Joint Force Commander JLLIS—Joint Lessons Learned Information System AFPAM90-1604  4 NOVEMBER 2016 29 JP—Joint Publication MAJCOM—Major Command MEA—Munitions Effectiveness Assessment MOE—Measure of Effectiveness MOP—Measure of Performance NAF—Numbered Air Force OA—Operational Assessment OAT—Operational Assessment Team OAWG—Operations Assessment Working Group OPLAN—Operational Plan OPR—Office of Primary Responsibility POL—Petroleum, Oil and Lubricants QDR—Quadrennial Defense Review RAF—Risk Assessment Framework RDS—Records Disposition Schedule SAM—Surface-to-Air Missile SECDEF—Secretary of Defense SES—Senior Executive Service SITREP—Situation Update Report SME—Subject Matter Expert US—United States Terms Assessment—According  to  JP  1-02,  Department  of  Defense  Dictionary  of  Military  and Associated Terms, an assessment is a continuous process that measures the overall effectiveness of  employing  joint  force  capabilities  during  military  operations.  This  includes  assessments performed at the AOC, Task Force, and Numbered Air Force (NAF) levels. As stated in JP 5-0, Joint  Planning,  an  assessment  is  “an  operational  activity  that  supports  decision  making  by determining progress toward accomplishing a task, creating an effect, or achieving an objective or  end  state  for  the  purpose  of  making  operations  and  campaigns  more  effective,”  which  can include those that take place at Major Command (MAJCOM) and Headquarters Air Force (HAF) levels as well. Works in this latter category, while similar to assessments of military operations, are focused instead on staff functions or activities supporting operations and strategy. Assessment  of  operations  and  strategy—An  assessment  of  any  operation  or  higher-level process for which there is an operation and an underlying strategy. Assessor—An individual whose key responsibility is to conduct an assessment. 30 AFPAM90-1604  4 NOVEMBER 2016 Commander—In  this  pamphlet  only,  the  commander  supported  by  an  assessment,  whether  a warfighting commander, a headquarters staff directorate lead, or any principal stakeholder for the assessment. Criteria—Standards  of  judgment  or  evaluation  used  as  a  basis  for  assessment  or  testing;  also defined thresholds of measures indicating a level of success or progress. Effects—The changes (intended or not) in  the operational  environment  created by  contributing actions. End  State  (or  Ends)—JP  1-02  defines  the  end  state  as  “the  set  of  required  conditions  that defines achievement of the commander's objectives.” The end state is generally derived from the commander’s  intent  statement.  For  other  assessments,  project  planners  and  managers  should consult  with  assessment  experts  to  help  establish  reasonable  end  states  for  the  project.  Ideally, assessors will be members of bodies such as operational planning teams. Means—The resources put toward accomplishing the ways to achieve the desired end state. The Doctrine,  Organization,  Training,  Materiel,  Leadership,  Personnel,  Facilities,  and  Policy (DOTMLPF-P)  construct  is  often  a  useful  methodology  for  examining  and  developing  the means. Measures—Characteristics, dimensions, or ratings used in an assessment to quantify or qualify task performance, effectiveness, or progress toward success criteria for an objective. Objectives—Operational  and  strategic  results  sought  by  a  commander.  Objectives  are characterized  by  a  set  of  effects  that  are  created  in  part  by  a  set  of  prescribed  actions  and operations. Planner—An  individual  who  translates  the  commander’s  strategic  guidance  into  a  feasible Course of Action (COA) and Concept of Operations (CONOPS), by which the organization can achieve its assigned mission and military end state. Ways—The  tasks  or  actions  undertaken  to  help  achieve  the  ends,  as  generated  during  the detailed  planning  process.  In  a  warfighting  command,  the  strategy-to-task  process  may  be  a useful start in determining the planned tasks of an operation. For staff assessments, action plans can provide a reasonable starting point for understanding the ways.  AFPAM90-1604  4 NOVEMBER 2016 31 Attachment 2 REFERENCE SHEET STEPS TO ASSESSMENT OF OPERATIONS & STRATEGY A2.1.  Step 1 – Identify the Strategy.  Developing the appropriate end states, ways, and means required to achieve the commander’s intent at an acceptable risk level is critical to the success of an assessment. Define required conditions (ENDS) for achievement of the commander's objectives. Detail the tasks or actions (WAYS) that must be taken to help achieve the ENDS. Determine the resources (MEANS) needed to accomplish the WAYS. A2.2.  Step 2 – Develop Criteria.  Define attribute and threshold criteria for assessing progress toward the end state and accomplishing required tasks. Generate criteria that are: Relevant to the effect or action being assessed. Mutually exclusive across the range of outcomes of the particular effect or action being assessed. Collectively exhaustive across the range of outcomes of the particular effect or action being assessed. Well-defined. A2.3.  Step  3  –  Identify  Measures  and  Collect  Data.  Determine  useful  assessment  measures including Measures of Effectiveness  (MOEs)  and Measures of Performance (MOPs) that relate directly to the criteria they are supporting. Ensure measures to be collected are appropriate, useful, and acceptable to stakeholders for a given level of assessment. Construct measures that represent a scale, not a goal. Strive for measures that can be assessed with observable (or at least inferable) data. Ensure measures are clear and concise. A2.4.  Step  4  –  Analyze  and  Present  Insights.  Present  results  and  insights,  and  make recommendations  to  decision  makers  to  support  their  decision  processes.  Continually  and critically evaluate the usefulness of the collected data to the assessment. Derive assessment insights. Critically examine data and assumptions. Investigate the causes underlying assessment results.  Seek feedback from experts and decision makers, and incorporate their insights into the assessment.   