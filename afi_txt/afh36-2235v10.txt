BY ORDER OF THE  SECRETARY OF THE AIR FORCE   AIR FORCE HANDBOOK 36-2235 VOLUME 10 1 NOVEMBER 2002    Personnel  INFORMATION FOR DESIGNERS OF INSTRUCTIONAL SYSTEMS APPLICATION TO EDUCATION Certified by: HQ USAF/DPDT (Col Patricia L. C. Priest) Pages: 173/Distribution: F   RECORDS  DISPOSITION:    Ensure  that  all  records  created  by  this  handbook  are maintained and disposed of IAW AFMAN 37-139, “Records Disposition Schedule”  NOTICE:    This  publication  is  available  digitally  on  the  AFDPO  www  site  at: http://afpubs.hq.af.mil.   OPR: HQ AETC/DOZ (Gary J. Twogood)  Supersedes: AFH 36-2235, Volume 10, 1 November 1993    This  volume  provides  information  and  guidance  for  applying  the  Instructional  System Development process described in AFMAN 36-2234.  This handbook is a guide for Air Force personnel who plan, design, develop, approve, administer, or manage education in the Air Force.  Chapter 1 GENERAL INFORMATION........................................................................... 4 Section A Background of ISD...................................................................................... 6 Section B ISD and the Continuum of Education........................................................ 10 Chapter 2 ISD IN THE TOTAL INSTRUCTIONAL SYSTEM ....................................... 11 Section A Air Force ISD Model ................................................................................. 12 Figure 1 Air Force ISD Model........................................................................................ 12 Section B Total Instructional System Functions........................................................ 17 Figure 2 System Functions............................................................................................ 17 Chapter 3 PLANNING.................................................................................................. 24 Section A Determine Instructional Requirements / Needs ........................................ 26 Section B Determine Personnel Requirements......................................................... 29 Section C Evaluating Your Development Effort......................................................... 31 Chapter 4 ANALYSIS................................................................................................... 34 Section A Educational Analysis................................................................................. 37 Section B Learning Analysis ..................................................................................... 39 AFH 36-2235 Volume 10 1 November 2002 2 Section C Resource Analysis.................................................................................... 45 Section D Target Audience Analysis......................................................................... 48 Section E What and How to Evaluate in the Analysis Phase .................................... 49 Section F ISD Management Plan Update ................................................................. 50 Chapter 5 DESIGN....................................................................................................... 51 Section A Develop Objectives................................................................................... 52 Section B Develop Tests........................................................................................... 64 Section C Review and Use Existing Materials .......................................................... 72 Section D Design Instructional Plan.......................................................................... 76 Section E Develop Implementation Plan................................................................... 90 Section F Design Instructional Information Management System............................. 92 Section G What to Evaluate in the Design Phase..................................................... 94 Section H Update ISD Management Plan................................................................. 95 Chapter 6 COURSEWARE DEVELOPMENT .............................................................. 97 Section A Prepare Course Syllabus.......................................................................... 99 Section B Develop Instructional Materials............................................................... 100 Section C Install Instructional Information Management System ............................ 104 Section D Validate Instruction................................................................................. 105 Section E What to Evaluate in the Development Phase ......................................... 115 Section F Finalize Instructional Materials................................................................ 116 Chapter 7 IMPLEMENTATION .................................................................................. 118 Section A Implementation of System Functions...................................................... 119 Section B Conducting Instruction ............................................................................ 120 Chapter 8 EVALUATION ........................................................................................... 123 Section A Levels of Educational Evaluation............................................................ 124 Section B Internal and External Evaluation............................................................. 126 Attachment 1 – GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION....................................................................................................... 130 Attachment 2 - ISD and Learning Theory and Application to the Learning Situation................................................................................................................. 151 Attachment 3 - Recent Developments in Learning Theory.................................... 158 Attachment 4 - An Example of Implementing the ISD Process: The Wing Company Grade Officer Course .......................................................................... 172    AFH 36-2235 Volume 10 1 November 2002 3 Chapter 1 GENERAL INFORMATION Overview  Introduction   Introduction to ISD   Is this handbook for you?              Instructional System Development (ISD) is the process that the Air Force prescribes for planning, developing, and evaluating instructional programs.  This handbook explains how you can use this process to produce your block of Air Force educational curricula effectively and efficiently.   This chapter describes how ISD benefits curriculum developers, and how basic learning theories support the ISD decision-making process.   This handbook addresses the question "How do you apply the ISD process in education?"  It is applicable whether the curriculum is developed by a contractor or the Air Force.  But is it for you?  Are You Responsible For . . . Planning instructional systems? Determining instructional requirements/needs? Determining personnel requirements?  Developing ISD evaluation plans? Conducting educational analyses? Conducting learning analyses? Conducting resource analyses? Conducting target audience analyses? Developing objectives? Developing tests?  29 31 37 39 45 48 52 64 Page 24 26                     Yes No AFH 36-2235 Volume 10 1 November 2002 4   Yes No  Is this handbook for you? (Continued)                     Additional information         99 100 104 Page 72 76 77 84 88 90 92 Are You Responsible For . . . Reviewing and using existing materials? Designing instructional plans? Selecting instructional methods? Selecting media? Developing lesson plans? Developing implementation plans? Designing instructional information management systems? Preparing course syllabuses? Developing instructional materials? Installing instructional information management systems? Validating instruction? Developing validation plans? Conducting internal reviews? Conducting individual tryouts? Conducting small-group tryouts? Conducting operational (field) tryouts? Implementing system functions? Conducting instruction? Conducting operational evaluations? Conducting internal and external evaluations?  If you checked YES to any of these questions, this handbook will help you do your job.   For additional information on Air Force ISD, see: 105 106 107 110 112 113 119 120 123 124                                   AFMAN 36-2234, Instructional System Development. AFMAN 36-2236, Handbook for Air Force Instructors. Other volumes of AFH 36-2235. AFH 36-2235 Volume 10 1 November 2002 5 Section A Background of ISD  What is ISD?   Why use ISD?   What does ISD do?   Instructional System Development (ISD) is the official Air Force process for curriculum planning.  It is used to guide the design and development of Air Force instruction in a systematic way.  It is a tool that organizes all the activities that go into course development.  The full ISD process carries the work of curriculum development all the way from identifying a need for personnel to learn a new area of knowledge or set of skills, through building and teaching the course, finding out whether the students learned what they needed, and improving the course to get better results.   ISD helps you produce a course or educational unit effectively and efficiently.  Using ISD minimizes jumping to conclusions and making premature decisions about what to teach and how to teach it.  Even if there were no ISD process, a logical, sequential process is needed to tie planning, development, execution, and evaluation together.  This systematic process is what any good curriculum developer or instructor accomplishes, consciously or unconsciously, for course planning.  Accountability is a key element in this process.   ISD is flexible and systematic.  It can:  Show you how to identify what the students need to know. Explain how to state what knowledge or skill students will have after instruction that they did not have before. Describe how to determine whether the students are learning what they need to learn. Explain how to organize logically the content the course will cover. Help you choose the most cost-effective teaching methods and media for the content and the educational setting. Give you the steps for developing student and instructor course materials. Describe scheduling and managing instruction. Show you how to collect and use student test results to improve the quality of the course. Ensure teacher and student accountability.  AFH 36-2235 Volume 10 1 November 2002 6  How does ISD work?   Key to using ISD   ISD meets a real need   Education and training   ISD ties the whole process together with a common thread, as follows:  What the students must know, to what level (requirements) ties into What the students must learn, to what level (objectives) ties into What the standards must measure, to what level (tests) ties into What the course teaches, to what level (design, development, implementation) and How successful the program is (evaluation)   ISD is a continuous process that lets you evaluate the quality of your work as you proceed and avoid wasted effort.  Use ISD as a flexible tool to help you plan and then stay on track.   A systematic approach to course development addresses a real need.  This is especially true in gathering and analyzing educational requirements and in translating these requirements into realistic educational experiences and integrating resources, instructional techniques, and procedures into overall course planning.  Equally important is selecting the methods for measuring instructional effectiveness through verifying changes in student behavior as a result of instruction.   While ISD was designed as a generic tool, the Air Force previously applied it primarily to the development of technical training courses.  The past problem with implementing ISD in Air Force education has been that the regulations and manuals governing the implementation of ISD did not make adjustments for the very real differences between training and education.  In education, you are teaching students a body of knowledge and skills to prepare them to deal with situations and solve problems not yet defined.  AFH 36-2235 Volume 10 1 November 2002 7  ISD in education  Systems approach example            The systems approach ties instruction to what the students will be able to do after instruction that they could not do before.  Objectives in educational curricula deal largely with cognitive learning (knowledge) and affective learning (attitudes) rather than with psychomotor skills.  You measure student success in educational curricula through student performance samples that can demonstrate that learning the desired subject matter and attitudes has taken place.  This handbook illustrates how the ISD process works in the development of educational curricula.  The guidance in this handbook will help you apply a systematic approach in your curriculum development work.   The following table presents an overview of a systems approach to instructional design and development, and suggests some questions to ask at each stage.  DETERMINING INSTRUCTIONAL NEEDS Capability Analysis What knowledge and skills exist? Requirements Analysis What levels of knowledge and skills are needed? DESIGNING INSTRUCTIONAL SYSTEMS What is expected as a result of instruction? How can it be measured? What knowledge and skills are to be learned? In what order or sequence? What method is most likely to be effective? What kinds of tools, aids, and materials should be used? What options and resources are available? How can they be evaluated? DEVELOPING INSTRUCTIONAL SYSTEMS AND MEDIA How many people will be instructed? Who will they be? What should lesson plans include? What knowledge and skills are required to conduct instruction? What facilities and equipment should be used?   AFH 36-2235 Volume 10 1 November 2002 8 Systems approach example (Continued)   DEVELOPING INSTRUCTIONAL SYSTEMS AND MEDIA (Continued) Where and when will learning activities take place? What media and materials will learners and instructors need? What existing media and materials can be adapted and used? What more should be produced? How will feedback be obtained and managed? How should the system be validated? IMPLEMENTING AND MAINTAINING THE SYSTEM   How often should activities take place? Who will provide and use feedback? How will results be used?  AFH 36-2235 Volume 10 1 November 2002 9  What is the Continuum of Education  Value of the Continuum of Education  Relation to ISD Section B ISD and the Continuum of Education  The Continuum of Education (COE) is an educational framework, a way of viewing the progress of Air Force professional education throughout an Air Force member’s career.  The COE begins with the first education an Air Force member receives (accession or initial enlistment training and education) and continues through each phase of the member’s career-long professional development.  The COE identifies what professional knowledge and skills are needed by Air Force members at any point in their careers.   For instructional developers, the COE serves as a roadmap enabling the developer to see his/her course of instruction in relation to those that precede and follow it.  This provides the developer an understanding of what knowledge and skills the incoming student should have already acquired prior to his/her course.  It also gives the developer a sense of what his/her course should impart to students to advance them along the continuum.   In developing a course using ISD principles, the developer must first determine what the instructional need is and what incoming students already possess.  A developer simply has to look at the previous phases along the COE to determine what incoming students should know and be able to do.  Comparing those things against the documented instructional needs allows the developer to determine where his/her course must lead the student to fulfill that need.  This information also helps to ensure that the developer doesn’t unnecessarily re-teach knowledge and skills already acquired at an earlier stage in the continuum.  AFH 36-2235 Volume 10 1 November 2002 10 ISD IN THE TOTAL INSTRUCTIONAL SYSTEM Chapter 2 Overview  Introduction   Where to read about it      In Air Force education, the design, development and implementation of instruction takes place within the larger instructional environment, which ensures that Air Force personnel will receive effective and efficient high-quality instruction.  This environment is called the total instructional system.   This chapter contains two sections.  Section A B Title  Air Force ISD Model Total Instructional System Functions Page 12 17  AFH 36-2235 Volume 10 1 November 2002 11 Section A Air Force ISD Model  ISD models   ISD is continually evaluated    Air Force ISD model   Over the years, the ISD process has been described graphically through a wide variety of models which call out the procedures in a number of phases or steps.  Most models encompass the functions of analyzing instructional needs and designing, developing, evaluating and improving instruction.  The use of a systematic problem-solving approach is the common thread that runs through all models.   The processes and products of the phases are continuously evaluated with emphasis on how well they meet the users' needs.  Life cycle evaluation ensures continuous improvement of the instruction.   The Air Force model is shown in Figure 1.    Figure 1 Air Force ISD Model   AFH 36-2235 Volume 10 1 November 2002 12  Description of  model   The Air Force model organized the ISD functions into five steps.  These steps are described as follows.  1.  Analyze System Requirements.  This is done through occupational, job, and task analyses which result in statements of actions, conditions, and standards for task performances.  2.  Define Education and Training Requirements.  This step includes a needs analysis to determine if training or education is needed, assessment of target population characteristics, and selection of tasks for instruction through consideration of such factors as criticality, learning difficulty, frequency of performance, and contribution to future problem solving.  3.  Develop Objectives and Tests.  Here the designer writes the three-part objectives that define what the students will be able to do after instruction, the conditions under which they will perform, and the acceptable standard of performance.  The designer then writes test items to measure student performance on each objective.  4.  Plan, Develop, and Validate Instruction.  In this step, the designer designs and produces course materials.  The developer tries out these materials on students using the criterion test items to ensure that the students can achieve course objectives.  5.  Conduct and Evaluate Instruction.  Here, the course is fielded.  Evaluation of instructional effectiveness continues for the life of the course and identifies needs that may develop for improving or updating the instruction.  AFH 36-2235 Volume 10 1 November 2002 13  Feedback and constraints   Flexibility   Feedback:  Figure 1 depicts how the ISD process uses feedback and interaction among the four phases with double-headed arrows connecting each phase of the process with the ever-present evaluation function.  Evaluation is what drives feedback.  If you don't evaluate the output of each phase of the ISD process before embarking on the next phase, you may propagate errors from one phase to the next and compound problems that could have been easily fixed at an earlier phase.  Without some type of evaluation, there is no information to make decisions or to provide feedback to take corrective actions.  Constraints:  We live and work in a world bounded by a myriad of constraints.  Everything we do must be planned for with respect to the particular constraints that bound the situation we are working within.  Three of the most common constraints instructional developers and managers have to contend with are money, manpower, and time.  Constrains change from project to project.  Some times there many be a shortage of manpower to assist in the development effort.  However, there may be sufficient money to permit hiring of contractors to help support the project.  Depending on the availability of money, manpower, and time and other resources that might be needed for your development project, management decisions may have to be made to control the efficient and effective use of the resources available for the project.  Constraints, especially time and manpower, also impact the instructional developer.  The developer must intelligently plan his/her development effort to fit within the allotted time and with the available manpower.  If the effort will be impaired by the existing constraints, he/she should raise the issue to the instructional manager who has the authority to make appropriate decisions.   The process allowed instructional designers to enter or reenter the steps as necessary to develop or refine the instructional system.  The Air Force model worked well and was considered adequate.  It supported an instructional system that was focused primarily on classroom education and technical training delivered by an instructor using the lecture/demonstration method.  AFH 36-2235 Volume 10 1 November 2002 14  New concerns   Future requirements   Today, we are not only concerned with classroom instruction, but we are also concerned with instruction that is exported to the job site using new delivery methods and technologies.  We are concerned with new automated instructional development tools that can make the instructional development more efficient, building quality in our instructional system, the concept of totally integrated training systems, and how to use the ISD process in different applications such as systems acquisition, education, aircrew, and technical training programs.   Principles of ISD have themselves evolved over the past three decades from ISD as a tool for applying behavioral learning principles to classroom instruction, through models of step-by-step procedures designed to enable anyone to develop instruction, to sophisticated models concerned with complex technological as well as cognitive issues that require experienced instructional design experts to sort out.  Today, instructional development requires expertise not only in instructional design but in media (e.g., computer hardware and software, video, interactive systems), cognitive learning theory, and vastly complex content areas.  The scope of the expertise has gone beyond the capabilities of the single instructional design expert and requires the integration of several disciplines.  Attempts are being made to use expert system techniques to help both the experts and novice instructional developers cope with contemporary advancements.  If successful, these techniques will impact instructional design in fundamental ways, such as by providing ISD expert system tools.  It is clear that any new model of the ISD process should reflect the movement away from rigorously applied procedures and should emphasize adaptability to changing environments.  These concerns have become cornerstones in the revision of the Air Force ISD process.  Updating the process will allow the Air Force to meet today's need for effective and efficient instructional systems and continue to meet the future challenges in instructional systems development.  AFH 36-2235 Volume 10 1 November 2002 15  Purpose   Most descriptions of systems approaches to the development of instruction use some kind of model to illustrate the concept.  The Air Force model shown in Figure 1 has been designed to represent simplicity and flexibility and to avoid any implication that it requires following lock-step procedures.  The instructional development process, which the model summarizes, calls for instructional designers to:  Analyze and determine what instruction is needed. Design instruction to meet the need. Develop instructional materials to support system requirements. Implement the instructional system. Evaluate the results of instruction to determine needed improvements.  AFH 36-2235 Volume 10 1 November 2002 16  What are system functions?   Section B Total Instructional System Functions  The system functions are those that must be in place before an educational program can operate.  The basic instructional system functions are discussed in the following paragraphs and are depicted in Figure 2.  Management.  This is the practice of directing and controlling all aspects of the instruction process.  Support.  This provides for and maintains the system on a day-to-day and long-term basis.  Examples are resources you need to keep equipment functioning.  Administration.  This is the part of management that performs day-to-day tasks such as documentation, student assignments, and student records.  Delivery.  This is the means of giving students the instruction.  Instructors, computers, and textbooks are all examples of ways to deliver instruction.  Figure 2 System Functions   AFH 36-2235 Volume 10 1 November 2002 17  Relation to ISD   ISD is the central process of the total instructional system.  ISD products are integrated into the system, and the system functions are active throughout all phases of the process.  AFH 36-2235 Volume 10 1 November 2002 18 Management Function  Management is the practice of directing and controlling the instructional system.  Never forget, however, that ISD and the system functions require a total team effort.  No single phase or process can occur in a vacuum.   Each level within the school or responsible organization has certain management responsibilities.  For example:  Faculty manage the teaching-learning activities. Instructor supervisors manage the scheduling of courses. Instructional designers manage curriculum development.  Overview   Who is responsible?   Management activities   In addition, other support organizations such as Resource Management and Civil Engineering manage special areas that support the instructional system.  Remember, it's a team effort.   The basic activities of management are:  Plan for the design, development, support, operation, and maintenance of the instructional system. Organize the resources, which involves identifying, arranging, and bringing together resources required for the instructional system. Coordinate instructional system operation and support. Evaluate the effectiveness and efficiency of each element in the instructional system. Report status and progress of the development of instruction or operation of the instructional system.  AFH 36-2235 Volume 10 1 November 2002 19 Support Function  Overview   Who is responsible?   The support function is defined as those long-range, as well as day-to-day, tasks performed in order to implement, operate, and maintain an instructional system.  Some of the basic support activities include:  Supply equipment, parts, materials. Maintain equipment, facilities. Produce instructional materials. Construct instructional aids, facilities. Provide funding, services.   As with the other instructional system functions, managers have the responsibility of ensuring that the instructional systems are adequately supported for the life cycle of the system.  Support for the instructional system will be provided by such organizations as:  Civil Engineering. Resource Management. Contracting. Visual Services. Information Management.  These are only a few of the many support organizations that will be needed to support the implementation and operation of the instructional system.  AFH 36-2235 Volume 10 1 November 2002 20 Administration Function  Overview  Administrative activities   Who is responsible?   Administration is the part of management that performs day-to-day tasks such as:  Maintain documentation. Type reports. Keep equipment, supply, and other records. Maintain student records.   The basic administrative activities that support the instructional system are:  Provide documents, such as instructional standards, plans of instruction, and student workbooks. Maintain records, such as personnel and instructional equipment. Administer student support, such as processing students "in" and "out" of the site. Administer staff support, such as leave processing and maintenance of personnel programs. Ensure that copyright issues are resolved and permissions granted. Schedule resources, such as personnel, equipment and facilities. Monitor resources, such as equipment and funds.   Managers have the responsibility of ensuring that the administrative activities are performed in support of the instructional system.  However, various instructional organizations have specific responsibilities to support system operation.   For example, some of the organizational elements that may be engaged in administration are:  Registrar. Typing pool. Staff support elements. Student support elements. Information management. Contracting agent.  AFH 36-2235 Volume 10 1 November 2002 21 Delivery Function  The instructional system delivery function is the means by which instruction is provided to the students.  Examples of delivery methods are:  Workbooks. Instructors. Computers, which includes ICW, CAI, CMI. Training devices, including simulators and games. Satellites. Correspondence.  Overview   Who is responsible?   Ensuring readiness    The individuals responsible for the instructional delivery function are:  Managers - Ensure that adequate planning and analysis have been done prior to selecting the delivery method and, once the method is selected, see that it is supported. Instructional designers - Select the most appropriate delivery method(s). Instructional staff - Use and evaluate the selected delivery method(s) for effectiveness.   At this point, the delivery function should be fully developed and operational.  Validation will have indicated the suitability and readiness of the delivery system;  however, prior to implementing the instructional system, you should "check it out" to be sure that everything is ready.  You need answers to questions about the delivery function, such as: Are there adequate instructors to support the instructional requirements? Have the instructors been qualified, and are they certified to deliver the instructions? Are the student materials available and printed in adequate numbers? Is the necessary equipment available and operational such as computers, projectors, simulators? Has the programming of the ICW been completed? Have slides and/or transparencies been produced? Have guest speaker invitations been mailed?  Evaluation Function AFH 36-2235 Volume 10 1 November 2002 22  Overview   Evaluation is an ever-present function in the ISD process that requires the continuous identification, collection and analysis of data from an instructional program or a course to determine its value or worth as outlined by the institution's mission and goals.  Evaluation is focused around four basic questions:  (1)  Did learning take place (in terms of a change in cognitive  (2)  Were students' attitudes affected by their learning knowledge),  experience,    (3)  Did a change in behavior take place, and  (4)  Did the learning (cognitive and behavioral changes) cause the student's organization to be come more efficient and/or effective?  These questions embody four levels of evaluation, each of which characterizes a different aspect of the instructional program.  Level 1 - Student learning Level 2 - Student attitudes about the course Level 3 - Behavioral changes in the student Level 4 - Changes in graduate's job performance  The evaluation function is covered in more detail in Chapter 8 of this volume.  AFH 36-2235 Volume 10 1 November 2002 23 Chapter 3 PLANNING Overview  Introduction   Where to read about it         Planning the instructional system structure and functions includes determining ISD process management and evaluation strategies, and estimating resource requirements and constraints.    The instructional developer may be responsible for doing some of the preliminary planning activities, given input from management, or the planning decisions may have been made by another Air Force organizational level.  This chapter describes the planning activities that are an inherent part of each step and function, including the three levels or phases in designing instructional systems.  The first is the "system level."  Planning and analysis at this level focuses on needs, goals, priorities, resources, constraints, alternate delivery systems, etc.  The focus is on determining the scope and sequence of courses and delivery system design –– the "big picture." The second level is the "course level."  Planning and analysis at this level is on course objectives, course structure, and sequence. The third is "lesson level."  In most settings, curriculum developers will find themselves working to some degree at all three levels.  The key difference in the work done is the number and/or detail of decisions that have already been made when you enter a given level.   This chapter contains three sections.  Section A B C  Title  Determine Instructional Requirements / Needs Determine Personnel Requirements Evaluating Your Development Effort Page 26 29 31 AFH 36-2235 Volume 10 1 November 2002 24  Additional information   For additional information on how to plan and manage ISD, see:  Bayard-White, C. (1990). Interactive Video: The Development Process.  The Videodisc Monitor, 18-21. Bergman, R. and Moore, T. (1990).  Managing Interactive Video/Multimedia Projects.  Englewood Cliffs, New Jersey: Educational Technology Publications. Briggs, L. J. and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.).  Glenview, Illinois: Harper Collins.  Darabi, G.A. and Dempsey, J. B. (1989).  A Quality Control System for Curriculum-based CBI Projects.  Journal of Educational Technology Systems,  18(1), 15-31. Dick, W. and Carey, L. (1990).  The Systematic Design of Instruction (3rd Ed).  Glenview, Illinois: Harper Collins. Greer, M. (1992).  ID Management: Tools and Techniques for Instructional Designers and Developers.  Englewood Cliffs, New Jersey: Educational Technology Publications. Knirk, F. G. and Gustafson, K. L. (1986).  Instructional Technology:  A Systematic Approach to Evaluation.  New York: Holt, Rinehart and Winston. Litchfield, B. and Dempsey, J. (1992).  A Seven-Stage Quality Control Model for Interactive Videodisc Projects.  Journal of Educational Technology Systems, 20(2), 129-141. Rossett, A. (1987).  Training Needs Assessment.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  AFH 36-2235 Volume 10 1 November 2002 25 Determine Instructional Requirements / Needs Section A  Introduction   Definitions of need   What is an instructional need?   An important preliminary activity that takes place before entering the ISD process is to determine that there is a need for formal instruction to achieve a mission goal.  Assessing instructional need is a critical activity that should be taken before any other planning occurs or additional resources are committed to a project.   The term "need" has several definitions (Stufflebeam, 1985).  The definitions most often used are:  Discrepancy view.  A need is the difference between "what is" and "what should be."  In this definition, the difference between what "is being taught" and what "should be taught" is the discrepancy. Democratic view.  Needs are identified by a group of experts (instructional designers, project managers, and others) who, by the democratic process of majority rule, determine what changes need to be made to the instruction. Diagnostic view.  Need is determined by identifying concepts, principles, and procedures whose absence or deficiency would hamper the students in meeting job performance requirements. Analytic view.  Need is determined by accurately predicting the future instructional needs based on the current instructional situation.   Instructional need is the lack of skills, knowledge and attitudes personnel should have in order to perform an activity adequately.  Examples of types of skills, knowledge and attitudes personnel may not have are:  Behavioral - skills in using tools and test equipment Cognitive - knowledge of information Affective - commitment to a particular belief  AFH 36-2235 Volume 10 1 November 2002 26  Who determines a requirement?   What establishes instructional requirements?   Stages of assessing instructional needs   How to do it       Step 1 Determine and state purpose. 2 Identify data requirements.  Instructional developers can receive a statement of instructional need from any Air Force organizational level.  Many such statements come from Air Staff or a cognizant office in HQ USAF.  Instructional needs will be present when:  A problem surfaces in an area where there is no current instruction. Policy or doctrinal changes make existing instruction obsolete. A new force structure is planned. Instruction in a topic is mandated.   The four stages involved in assessing instructional needs are:  Plan the assessment. Define the problem. Document the deficiency. Develop the solution.   Instructional needs assessment is the means of identifying whether there is a need to develop or revise instruction to solve an identified problem.  The steps of instructional needs assessment are shown below.  Activity Purpose Determine purpose and objective of the assessment. Develop plan for conducting the needs assessment. Document the plan. Identify data that describes the actual performance and the desired performance. AFH 36-2235 Volume 10 1 November 2002 27 How to do it (Continued)  Step 3   4 Collect and analyze data. 5 Develop instruction.   Activity Purpose Select data collection method. Select appropriate method of collecting data such as: Literature review. Interview. Questionnaire/survey. Records and reports. Group discussion. Work samples. Observation. Collect sufficient data to document the performance deficiency. Analyze data to identify the performance deficiency. Develop or revise appropriate instruction to solve the performance deficiency. AFH 36-2235 Volume 10 1 November 2002 28  Purpose   What is required?   Type and level of the course   Identifying need for experts  Section B Determine Personnel Requirements  The purpose of determining personnel requirements is to ensure that you know what personnel you will need to design, develop, implement, and maintain the course.   In determining personnel requirements, you will:  Determine the level of the course. Identify the need for specialists. Define the roles of the specialists. Plan adequate education for instructors.   The type of course and level of complexity of the course will determine the personnel requirements.  For example:  Developing an exportable course using interactive courseware (ICW) will require more personnel, using different skills, than will print-based materials. Levels of complexity within computer-based training (CBT) will impact personnel requirements, since the more complex the material is, the more time it takes to design and develop. Topic integration across the course may require additional personnel or personnel with broader education and experience areas.  Note: Define the type and level of the ISD project early in the initial planning stages.   The type and level of the course will drive personnel requirements.  For example, you may need:  Content specialists Identify the kind and number of specialists. List any special skills the specialists may need. Estimate when specialists will be needed and for how long.  AFH 36-2235 Volume 10 1 November 2002 29  Identifying need for experts (Continued)   Define experts' roles  Graphic artists. Videographers. Computer programmers.   Clearly defined roles will:  Enable you to define more accurately the elements of the project, such as project definition, tasks, and milestones. Allow specialists to know what you expect of them. Allow work to be scheduled with minimum conflict. Allow specialists to be selected who have the needed skills.  AFH 36-2235 Volume 10 1 November 2002 30  Introduction   What needs to be evaluated  Evaluation plan job aid  Section C Evaluating Your Development Effort  The ISD process is a cyclic, self-correcting process.  How well the self-correction nature of the process works depends to a large extent on how well the instructional developer evaluates his/her progress through the ISD process.  The most effective curriculum development effort is one which has a well-planned and deliberate evaluation process built into the design.  This is something the instructional developer must address early in the development process and continuously throughout the development cycle.   You should evaluate your progress at each phase of the process.  So, at the end of each chapter of this handbook describing one of the ISD phases, you will find a section discussing what is important to be evaluated at that phase.  After the instructional program is developed, it needs to be evaluated as a total instructional system to ensure all the parts work together as intended.  This is known as validation or summative program evaluation.  At this point, the course is ready to go "full up."  Then, after the instructional program is in place, the evaluation phase is invoked at the completion of each course administration.  This allows for data collected during the conduct of the course to be analyzed.  This analyzed data is then fed back into the design process to correct shortcomings of the program and to generally improve the course.   Some course developers and program evaluators feel comfortable having a plan of attack laid out for them to follow during and after the course development process.  For those who prefer it, here is a job aid useful for developing an evaluation plan.  AFH 36-2235 Volume 10 1 November 2002 31  A job aid for developing an evaluation plan is provided below. Example of Evaluation Plan Job Aid ISD Process and Product Evaluations I. II.  Validation A.  Course Identification B.  Purpose of Course C.  System Reviews   1.  Identify types of reviews required. 2.  Identify who is responsible. 3.  Describe what will be reviewed. D.  Validation Process 1.  Individual Tryout  Evaluation plan job aid         a.  Identify who is responsible for tryouts. b.  Define parameters for tryouts, such as number of tryouts, materials to be validated. c.  Describe individual tryout procedures. d.  Indicate what data will be gathered and analyzed. e.  State how data will be used. f. Identify revision procedures. (Note: In PME, validation may be modified for logistics and cost reasons.  Pre-validation in small-group or large-group tryouts may not be possible due to the length of PME courses.) 2.  Small-group Tryout a.  Identify who is responsible for tryouts. b.  Define parameters for tryouts, such as number of students, number of groups. c.  Describe small-group tryout procedures. d.  Indicate what data will be gathered and analyzed. e.  State how data will be used. f. Identify revision procedures. g.  Operational Tryout h.  Identify who is responsible for tryouts. 3.  Define parameters for operational tryouts, such as number of students in each group, number of groups. a.  Describe operational tryout procedures. b.  Indicate what data will be gathered and analyzed. c.  State how data will be used. d.  Identify revision procedures. AFH 36-2235 Volume 10 1 November 2002 32 Evaluation plan job aid (Continued)     Updating the evaluation plan  Example of Evaluation Plan Job Aid  III. Summative Evaluation A.  Course Identification B.  Purpose C.  Summative Evaluation Process 1.  Internal evaluation (follow same steps as D.1 and D.2 above) 2.  External evaluation (follow same steps as D.3 above)    To be an effective tool for managing the process, the evaluation plan must be continually updated with current information.  The plan should be updated at the end of each phase of the ISD process or when significant changes warrant a revision to the plan.  Information that may require updating in the evaluation plan includes items such as:  New or revised milestones. Changes in validation procedures. Changes in data to be collected during internal evaluations. Changes in procedures for evaluating the development process. Changes in methods of collecting external evaluation data.  AFH 36-2235 Volume 10 1 November 2002 33  Introduction   Purpose  Chapter 4 ANALYSIS Overview  The analysis phase begins when the requirement for instruction has been established.  This phase covers a variety of analyses, which may include mission or content analysis, learning analysis, resource analysis, target audience analysis, etc.  You will check the results of the analyses against the preliminary evaluation plan for this ISD project and update the plan as needed.  The individual(s) charged with managing the instructional development effort must update the initial management plan to reflect any scheduling and other adjustments.   The purpose of the analysis is to identify what to teach and how much to teach.  At the "big picture" level, some analysis established the need for instruction.  For example, in the mid 1990s, a determination was made that Air Force officers required a stronger foundation in Air Force Doctrine, especially officers serving on joint and CINC staffs.  This established the educational requirement for a new course to be developed at Air University:  the Aerospace Power Course.  It also established the initial target population for the course:  all Air Force officers of the rank of major or higher assigned to positions on joint and CINC staffs.  AFH 36-2235 Volume 10 1 November 2002 34  What you do   Objective   Where to read about it           At your level, the big picture analysis has been done and the mission or major objective has been stated.  Your job is to investigate the factors that will tell you what and how much instruction you need to develop.  These factors include:  Why the students will need the information or skills (mission or goal). What skills, knowledge, and attitudes the students must master and what categories of learning this involves (learning analysis). What resources you have to work with and what your constraints are (resource analysis). Who the students will be and what they already know about the subject (target audience analysis).   The objective of this chapter is to explain the various types of analyses that may be conducted in the analysis phase.   This chapter contains six sections.  Section A B C D E F Title  Educational Analysis Learning Analysis Resource Analysis Target Audience Analysis What and How to Evaluate in the Analysis Phase ISD Management Plan Update Page 37 39 45 48 49 50 AFH 36-2235 Volume 10 1 November 2002 35  Additional information   For additional information on the analysis phase of instructional development, see:  AFMAN 36-2236, Handbook for Air Force Instructors, 1994. Briggs, L. J. and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.).  Glenview, Illinois: Harper Collins.  Carlisle, K. E. (1986). Analyzing Jobs and Tasks.  Englewood Cliffs, New Jersey: Educational Technology Publications. Cram, D. D. (1979). Professor T-Pop. Performance and Instruction, 18(6). Dick, W. and Carey, L. (1990).  The Systematic Design of Instruction (3rd Ed.).  Glenview, Illinois: Harper Collins. Gagné, R. M. (1985).  The Conditions of Learning (4th Ed.). New York: Holt, Rinehart and Winston. Johnassen, D. H. (1989). Performance Analysis. Performance and Instruction, 28(4), 15-23. Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992).  Instructional Design Strategies and Tactics.  Englewood Cliffs, New Jersey:  Educational Technology Publications. Mager, R. E., and Pipe, P.  Analyzing Performance Problems.  Belmont, California: Fearon. Rossett, A. (1987).  Training Needs Assessment.  Englewood Cliffs, New Jersey: Educational Technology Publications. Zemke, R. and Kramlinger, T. (1982).  Figuring Things Out:  A Trainer's Guide to Needs Assessment and Task Analysis.  Menlo Park, California:  Addison-Wesley.  AFH 36-2235 Volume 10 1 November 2002 36 Section A Educational Analysis  Introduction   Why analyze?   Definition   Who is responsible?   How to conduct the analysis   Normally, the analysis phase begins with an educational analysis.  The requirement to conduct this form of analysis may depend on the application, scope, and nature of the project.  This level of analysis identifies the goals and content area of an educational requirement.  A needs assessment should already have been conducted to determine if there is a problem for which instruction is the appropriate solution.  (Refer to Volume 6 of AFH 36-2235 for further guidance.)  If the assessment confirmed an instructional need, you would usually begin instructional development at the analysis phase.  However, in some cases, you may be able to enter directly into the design phase of ISD.  This section addresses the first stage of analysis.   The initial analysis of the educational requirement provides you with information needed to begin to determine instructional requirements.   Educational analysis is the process of reviewing the educational requirements, developing the educational goals or outcomes, and developing statements of these goals.   Managers are responsible for ensuring that the necessary analyses are conducted.  In most cases, other individuals such as instructional developers, subject matter experts (SMEs), or system analysts should conduct the actual analyses or at least provide technical assistance.   The process of conducting an analysis involves a number of steps.  The actual number may depend on the type and scope of the analysis being conducted.  During the analysis, you may perform some of the steps concurrently, others sequentially.  Collect data. Determine and state the learning outcomes.  AFH 36-2235 Volume 10 1 November 2002 37  How to conduct the analysis (Continued)   Sources of data   Results   Determine the learning content, e.g., skills and knowledge. Prioritize the outcomes in terms of importance to mission goals. Document the results.   Some sources of data are:  Subject matter experts. Subject matter literature. Air Force directives. Department of Defense directives. Previous course evaluation data "lessons learned" (for course revisions).   The results of the educational or mission analysis are statements of the course goals that will be the focal point for developing and organizing the instruction.  Statements of learning objectives, learning hierarchies, and measures of success in learning will all be directly traceable back to the overall course or program goals.  AFH 36-2235 Volume 10 1 November 2002 38 Section B Learning Analysis  Definition   Learning analysis steps   Conducting learning analysis   Use of learning analysis results   Example       Learning analysis is the process of analyzing what is to be taught in terms of types of learning involved and level of learning desired.   When you conduct a learning analysis, you will:  Identify the types of learning involved. Determine the level of learning needed. Build a learning hierarchy of knowledge and skills to be taught.Identify prerequisite knowledge and skills required.   In learning analysis, the curriculum developer looks directly at categorizing type and level of learning needed to satisfy the instructional goal.  Statements of outcomes are generally made in terms of cognitive and affective behaviors expected.   You will use the results of the learning analysis to design instruction.  Identifying the types and levels of learning will help define how the objectives will be stated.  The learning hierarchy will support sequencing instruction.  Establishing prerequisites will help determine what content must be included in the course.   The following example of learning analysis procedures is provided for your information.  Although not complete, it is an excellent guide for the procedures to follow.  Step Procedure 1 Identify Types of Learning Involved    Examine each educational goal and determine which type(s) of learning are expected of the student.  The categories described on page 40 may aid in your determination. Repeat this examination for all educational goals in the course. AFH 36-2235 Volume 10 1 November 2002 39  Example (Continued)  2 Determine Level(s) of Learning Needed    Examine each educational goal and determine what level(s) of learning are appropriate to satisfy the goal. Determine whether multiple levels of learning are needed for each goal, or if a single level of learning is sufficient. Multiple levels may be necessary if an educational goal refers to several content areas.  For example, if the goal is to comprehend some complex concept, and if the students are not assumed to possess the required background factual knowledge to ensure their comprehension, then you must teach the necessary factual knowledge at a fairly low level of learning (knowledge level), followed by instruction on the complex concept at the comprehension level of learning. Multiple levels may also be necessary if an educational goal refers to different learning domains.  For example, if the goal is to comprehend some concept and to value that concept, then it will be necessary to recognize two different levels of learning–one in the cognitive domain (comprehension) and one in the affective domain (value). In line with the above example, you must intentionally consider both cognitive and affective purposes for instruction.  You will probably never create a completely affective instructional program.  However, it is just as unlikely that you will completely ignore the affective component of instruction.  The combination of cognitive and affective components in the instructional process is one of the attributes that distinguishes education from training.   AFH 36-2235 Volume 10 1 November 2002 40  Types of learning  Intellectual skills  Cognitive strategies  Verbal information  Motor skills  Attitudes   Other learning categories   There are many ways to categorize types of learning.  Some of the most common are briefly discussed below.   Intellectual skills consist of discrimination, concept, rule-using and problem-solving capabilities.   Cognitive strategies are thought of as executive control mechanisms for learning.  There are different types of cognitive strategies, such as clustering items into similar groups to reduce memory loss, or reading strategies to increase comprehension.   Verbal information is the learning of names and labels that can be verbalized.  It is also called declarative knowledge.  Verbal information learning requires some basic language skills.  It is most readily learned and retained when situated within a larger context.   Motor skills are learned behaviors that involve the smooth, coordinated use of muscles.  Motor skills most often involve a sequence of activities that is first learned verbally to provide guidance for learning execution of the motor skill.  When the learner has acquired the motor skill, the verbal routine is no longer needed and the skill is performed in a smooth and continuous manner.   Attitudes are an area where affective and cognitive learning overlap.  The acquisition of particular attitudes may require prior learning of intellectual skills and particular sets of information.   Other means of categorizing learning have been in common use in Air Force curriculum development.  Some of these are listed below and on the following page.  Forming Associations.  Associate, name, or respond to a specific input (stimulus).  The person associates the response with a specific input only.  The response may be vocal, sub-vocal (say it to yourself), written or motor.  Forming Chains.  Recall sequences of actions or procedures AFH 36-2235 Volume 10 1 November 2002 41   Other learning categories (Continued)   Higher-level education   Levels of learning  which must be recalled in a specific order.  In a chain, the response to one input becomes the input to the next response.     This may involve chains of verbal responses or chains of motor responses.  Making Discriminations.  Make different responses to the various members of a particular class; it means being able to distinguish among input information sources and/or types and then to respond appropriately to each.  Making Classifications.  Respond in a single way to all members of a particular class of observable or abstract events.  This involves recognizing the essential similarity among a class of objects, people, events or abstractions, and recognizing the differences that separate those objects, people, events, or abstractions which are not members of the class.  Using Rules.  Apply rules to a given situation or condition by responding to a class of inputs with a class of actions.  A rule states the particular relationship between two or more simpler concepts.  It is helpful to think of rules as "if-then" statements.  Problem-Solving.  Compare previously learned rules to create a higher order rule.   For higher-level education, you may have to:  Evaluate conflicting information. Synthesize disparate data or perspectives.   The following categories of learning levels are in common use for cognitive and affective learning.  With cognitive learning, concern is focused on what is going on in the learner's mind.  Affective learning has to do with attitudes and motivation.  AFH 36-2235 Volume 10 1 November 2002 42  Cognitive  Affective  Knowledge:  Recall previously learned material (facts, theories, etc.) in essentially the same form as taught. Comprehension:  See relationships, concepts, and abstractions beyond the simple remembering of material.  This typically involves translating, interpreting, and estimating future trends. Application:  Use learned material in new and concrete situations, including the application of rules, methods, concepts, principles, laws and theories. Analysis:  Break down material into its component parts so that the organizational structure may be understood.  This includes identification of the parts, analysis of the relationships between parts, and recognition of the organizational principles involved. Synthesis:  Put parts together to form new patterns or structures, such as a unique communication (a theme or speech), a plan of operation (a research proposal), or a set of abstract relations (schemes for classifying information). Evaluation:  Judge the value of material for a given purpose.  Learning in this area is the highest in the cognitive hierarchy because it involves elements of all the other categories, plus conscious value judgments based on clearly defined criteria.   Receiving:  Be aware that a thing exists and pay particular attention to it. Responding:  React to a particular phenomenon in some way, such as acquiescing (reading assigned material), willingness to respond (voluntarily reading beyond assignment), or satisfaction in responding (reading for pleasure). Valuing:  Attach worth or value to any object, phenomenon, or behavior ranging from accepting a value to commitment. Organizing:  Bring together different values, including conflicts between them, and then begin to build an internally consistent value system. Characterizing:  Pervasive, consistent, and predictable behavior (life style) developing from a value system which controls behavior for a significant period of time.  AFH 36-2235 Volume 10 1 November 2002 43  Identify skills and knowledge    Identify prerequisite knowledge and skills   After categorizing the type and level of learning, you may also analyze to identify skills and knowledge content of the course.  You will determine the results and activities that applying the new skills and knowledge enables the students to achieve.  Such an analysis will help you list samples of behavior to test in order to demonstrate that students have learned.  Examples of such activities and results are:  NEGOTIATE............................to reach an .. AGREEMENT WRITE.....................................to develop a. POLICY STATEMENT ANALYZE DATA .....................to make ....... DECISIONS ANSWER QUESTIONS...........to provide .... INFORMATION DISCUSS EXPERIENCE ........to improve ... UNDERSTANDING SUPERVISE OTHERS............to ensure ..... WORK MEETS  COORDINATE RESOURCES.to develop.... PLANS   The last stage of learning analysis is the thorough analysis of goal statements.  This analysis will allow the instructional design to identify any prerequisite learning that may be necessary in order for students to learn what will be taught in a course of instruction.  STANDARDS  AFH 36-2235 Volume 10 1 November 2002 44 Section C Resource Analysis  Resources are critical factors in the instructional system from the initial planning, through instructional development, to operation and maintenance of the system. In the analysis phase, it is unlikely that the exact resource requirements can be defined, but if you consider resource requirements in early planning efforts, you should be able to estimate the requirements and have enough time to secure the resources.   Resources for the instructional system include:   Introduction   Resource categories   What you need to know   Scope of resource analysis  Conducting resource analysis  Equipment. Facilities. Funds. Personnel. Time.   Resources should be analyzed in order to identify:  Type of resources you will need to develop the course, such as instructors, equipment, facilities, designers. Quantity of resources required, such as number of instructors, number of work-hours (i.e., faculty/student ratios). When resources will be needed in order to meet scheduled delivery date. Total cost of resources for in-house or contract development. Resource constraints.   The scope of resource analysis includes both long-range and day-to-day concerns.   The resource analysis results in an estimate of resource requirements for the instructional program for both development and operation.  One of the simplest ways to conduct the initial resource analysis is to ask a series of questions to determine what resources will be needed to support the objectives, understanding that this is an estimate.  Sample resource categories and questions are suggested in the following table.  AFH 36-2235 Volume 10 1 November 2002 45  Resource Equipment Facilities Funds Personnel Time       Questions What specific equipment will you need (e.g., computers)? How will the equipment be used in the course? What quantities will be required? How much space will be required?  What type? Are facilities available? Are there special environmental requirements? What will the initial personnel, equipment, and facilities cost? What are the recurring costs associated with the system? How many instructional designers, computer programmers, videographers, are needed to develop the course?  Will instructors be needed?  If so, how many? What are the student work-year requirements? What is the scheduled delivery date? How much time will be required to develop the instruction? What is the estimated module or course length?  Finding answers to these and other questions will help you estimate the resource requirements for the instructional program.  AFH 36-2235 Volume 10 1 November 2002 46  Resource constraints   Updating resource requirements   If you are faced with a resource constraint, you will need time to select an alternative to some planned strategy or delivery approach.    Remember that any instruction, whether a topic within an existing course or a complete course, is always a sub-element of a larger system.  Also, the course has such constraints as maximum course length, manpower, budget, and student loading, all of which are driven by the agencies that use the graduates of the course.  Ignoring these constraints may lead to an instructional package that is either too large for the time allotted, or one that requires too large a share of resources compared to the relative importance of the course to the overall missions of the Air Force.   During initial planning and analysis, it is unlikely that you can completely and accurately identify all resources you will need.  As the instructional development process continues, there will be a continuing need to update the resource requirements to ensure that adequate resources are available for your instructional program.  AFH 36-2235 Volume 10 1 November 2002 47 Section D Target Audience Analysis  Conducting an analysis of the target audience allows you to design the instructional system based on the knowledge and attitudes the target audience is likely to bring to the course.  This reduces the likelihood of a mismatch between the students and the level of course content.   Target audience analysis produces various data depending on the nature and scope of the analysis.  Examples of the data produced are:   Purpose  Products of analysis  Use of the data  Range of aptitudes. Previous background and experiences. Previous education. Interests. Physical characteristics. Size of target audience. Computer literacy. Supervisory experience. Rank/grade. Command experience. Subject matter competence. Writing proficiency.   The data produced during analysis of the target audience results in a typical student profile and is used by the instructional designer to determine:  Course content. Level of course content. Media. Delivery methods. Course length. Equipment needs. Affective strategies.  AFH 36-2235 Volume 10 1 November 2002 48 What and How to Evaluate in the Analysis Phase Section E  What to consider   Take into account the following points:  Ask someone who is not involved in the course development effort, but who is knowledgeable in the content to be taught, to review the data sources used in the Educational Analysis to ensure all relevant subject matter experts and appropriate directives have been consulted.  Ask someone who is not involved in the course development effort, but who is knowledgeable in learning theory, to review your learning analysis as a "sanity check."   Review the resource analysis with one or more individuals not involved in the course development effort who have a good sense of resource requirements to validate the assumptions on which your analysis was based.  Review your target audience analysis to ensure you have given appropriate credit to your audience for previous knowledge, skills, abilities, and attitudes they bring into your instructional situation.  It may be helpful to consult the Continuum of Education architecture to help validate your assumptions.  AFH 36-2235 Volume 10 1 November 2002 49 Section F ISD Management Plan Update  Project management   Purpose   Who is responsible?   What should be updated?   During your initial planning of the ISD project, the managers developed an ISD management plan to serve as a road map for managing the instructional development process and the instructional system.  You developed the plan early in the initial planning stages to plan how you are going to get where you are going.  As you complete the analysis phase of the ISD project, you will likely have gathered additional and better information on which to base your management decisions.  If this is the case, you will need to update the management plan.   Since the management plan is a "tool" for managing the instructional development process and the instructional system, update it at the end of each phase of instructional development, or when any significant change occurs that impacts planning.  If the plan is not continually updated with the most current, accurate information, it will fail as a management tool.   The ISD project manager is responsible for seeing that the management plan is updated at the end of each phase of ISD, or after any significant change has occurred that impacts the current planning.   The ISD management plan should be updated after each phase of instructional development, as applicable.  Update the plan to include the latest information, such as:  New or revised milestones. Refinements to project definition. Changes in project resource requirements or constraints. Revisions to support requirements. Identification of new taskings. New information resulting from analysis that impacts project management.  AFH 36-2235 Volume 10 1 November 2002 50 Chapter 5 DESIGN Overview  Introduction   What is it?   Objectives   Where to read about it  At this point in the process, you have completed the required analyses and updated the evaluation and management plans.  You are now ready to enter the design phase of ISD.   The design of instruction defines: What exactly your course will teach. How you will measure what the students learn. How you will teach the material. How the school will implement the course. How you will collect and maintain student and course data.  In addition, you will check: How the design affects the evaluation plan Management plan adjustments   The objectives of this chapter are to: Explain how ISD supports cognitive and affective instructional design. Review method and media selection.   This chapter contains eight sections.  Section A B C D E F G H  Title  Develop Objectives Develop Tests Review and Use Existing Materials Design Instructional Plan Develop Implementation Plan Design Instructional Information Management System What to Evaluate in the Design Phase Update ISD Management Plan  Page 52 64 72 76 90 92 94 95           AFH 36-2235 Volume 10 1 November 2002 51 Section A Develop Objectives  Introduction   Definition   Purpose   Goals vs. objectives   Types of objectives   The first activity in the design phase is to develop objectives.  Objectives are the cornerstones of learning.  Objectives are developed at all levels of instruction where evidence of learning is required.   An objective is a statement of what students are expected to demonstrate as a result of instruction.  Objectives are usually used to identify outcomes of units, blocks, modules, instructional periods, and areas of instruction such as courses of a few days.   Some of the purposes of an objective are to:  Serve as the foundation for instructional design. Provide the basis for instructional strategy decisions. State expected student behaviors. Determine content of the instruction. Serve as a basis for developing test items and performance appraisals. Demonstrate student accountability. Validate instruction.   Educational objectives are not to be confused with educational goals, which are published outcomes of instruction in terms of broad intent, state, or condition.  When educational goals are published, there are usually several educational objectives published that serve as evidence of achievement of the broadly stated goal.   Terminal objectives are usually designed to identify required learning outcomes at the end of major divisions of a course, such as a module, unit, or block. However, terminal objectives may be written for stand-alone periods of instruction. Objectives for short courses or content involving numerous small areas not related are usually stated as terminal objectives.  Terminal objectives are also known as primary objectives.  AFH 36-2235 Volume 10 1 November 2002 52  Types of objectives (Continued)  Formats for objectives   Developmental objectives are prerequisite outcomes that form the building blocks necessary to achieve a terminal objective.  These objectives provide the opportunity to evaluate student progress toward the terminal objective, but do not serve as proof of achieving the terminal objective.  Developmental objectives are also known as enabling, secondary, supporting, and/or subordinate objectives.   Level of learning objectives contain a specific level of learning and a clearly stated subject expressed in terms of what students are expected to achieve, such as comprehend the concepts of discipline and non-judicial punishment. Level of learning objec-tives represent a large domain of behaviors at a specific level of proficiency.  Curriculum designers must carefully select representative, measurable samples of behavior (MSB) from the domain of behaviors that will serve as evidence of objective achievement.  MSBs are the basis for constructing written test items or performance appraisals.  They may be written in the format that possesses all the criteria of a criterion objective, such as behavior, conditions, and standards, or published as behavioral statements only.  Criterion objectives are statements that specify precisely what student behavior is expected.  Complete criterion objectives are made up of three parts:  Behavior(s) - What the students will be able to do (for example, list the names of the Joint Chiefs of Staff). Condition(s) - Environment in which they will do it (for example, using student manuals, list the names of the commanders of each MAJCOM and special operating activities). Standard(s) - How well they will do it in order to demonstrate that they have learned (for example, "... list the names ... within ten minutes”).  Activity statements or expressive outcomes are planned activities that have no explicit or precise objectives (Eisner, 1985).  These statements are appropriate, especially in short, standardized programs, for presentations where the content is changing so rapidly that a formalized objective, lesson plans and measurement devices are inappropriate.  An activity statement   AFH 36-2235 Volume 10 1 November 2002 53  Formats for objectives (Continued)   Stating objectives for different types of learning outcome   might read, "Participate in an interchange with company grade officers to discuss problems and perceptions in today's military environment."  The use of activity statements must be fully justified and occupy a very small percentage of the total curriculum.  These types of statements are usually used for large group presentations, but may be used in small group settings also.   The three major categories of learning outcomes–cognitive, affective, and psychomotor–require somewhat different approaches to stating objectives.  Cognitive level-of-learning objectives require measurable samples of behavior verbs that serve as observable, measurable evidence of a more general mental activity such as know, comprehend, or apply.  Affective level-of-learning objectives require measurable samples of behavior verbs that serve as observable, behavioral evidence of a more general feeling state such as receive, respond or value.  Psychomotor objectives use verbs that describe observable actions usually associated with a process or procedure involving manipulative skills, such as assembles, measures, alters and dissects.  AFH 36-2235 Volume 10 1 November 2002 54  Objectives in education   Measuring cognitive and affective behavior change  Additional information     Instructional objectives in the educational area usually fall in the cognitive or affective taxonomy of learning.  Cognitive objectives focus on basic knowledge, concepts and principles, and cognitive skills which include the application, analysis, synthesis, and evaluation of principles or generalizations.  Affective objectives focus on voluntary student attending, receiving, responding, and valuing, usually at the acceptance and preference levels. However, few educational objectives are published at the attending, receiving, or responding level for adults.  Affective objectives at the commitment level of valuing and higher levels are seldom published except for courses or schools of several years in duration, or when concepts or principles are threaded through several schools over many years across a continuum of education, such as ethical principles, leadership concepts, etc.   Since cognitive and affective objectives in education cover a large domain of behaviors, a sampling of behaviors from the larger domain is accepted practice.  Careful consideration should be given to selection of behaviors that provide significant evidence of achievement of a more general and abstract outcome.   For additional information on objectives, see:  AFMAN 36-2236, Handbook for Air Force Instructors. Berk, R. A. (1986). A Consumer's Guide to Setting Performance Standards on Criterion-Referenced Tests.  Review of Educational Research, 56(1). Bloom, B. S., Hastings, J. T. and Dadaus, G. F. (1971).  Handbook on Formative and Summative Evaluation of Student Learning.  New York:  McGraw-Hill. Davies, I. K. (1976). Objectives in Curriculum Design.  London: McGraw- Hill. Ebel, R. L. and Frisbie, D. A. (1986).  Essentials of Educational Measurement (4th Ed.). Englewood Cliffs, New Jersey: Prentice Hall. Eisner, E. W. (1985).  The Educational Imagination: On the design and Evaluation of School Programs (2nd Ed.). New York: Macmillan. AFH 36-2235 Volume 10 1 November 2002 55  Additional information (Continued)    Kibler, R. J. (1981). Objectives for Instruction.  Boston, Massachusetts: Allyn and Bacon. Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992).  Instructional Design Strategies and Tactics. Englewood Cliffs, New Jersey: Educational Technology Publications. Mager, R. (1973).  Measuring Instructional Intent, or Got a Match?  Belmont, California: Fearon. Mager, R. F. (1962).  Preparing Objectives for Instruction (2nd Ed.).  Belmont, California: Fearon. Popham, W. J. (1978).  Criterion-Referenced Measurement.  Englewood Cliffs, New Jersey: Prentice Hall. AFH 36-2235 Volume 10 1 November 2002 56  The process   Area goals or objectives   Phase (Block) objectives   Period (Unit) objectives  Guidelines for Developing Objectives  When the overall course goal or mission statement has been agreed upon, that statement becomes the focal point for all course development that follows.   Area goals or objectives divide the curriculum at the top level.  Area goals describe expected outcomes in terms of broad, general intent.  Area goals do not generally use specific levels of learning in order to avoid confusion with educational objectives.  Area objectives, when published, should exhibit the characteristics of a level-of-learning objective or criterion objective.  Area objectives published as level-of-learning outcomes should be followed by an adequate number of measurable samples of behavior.   Phase objectives, also known as block or module objectives, are major subdivisions of an area and the first level of detail beneath the area goal or objective.  Phase objectives published as level-of-learning objectives should be followed by an adequate number of measurable samples of behavior.   Each area objective and phase objective (if used) is, in turn, divided into a series of instructional periods.  Period objectives are more specific than area or phase objectives.  In the period objectives, you decide on the learning activities.  Period objectives published as level-of-learning objectives should be followed by an adequate number (three or four) of measurable samples of behavior to state specific desired student outcomes.  Selecting the behavioral performance statement appropriate for a given level of learning will enable students to demonstrate clearly the desired observable behavior.  You need to look at the total context of the behavioral statement to determine the level of the sample, not just at the verb used.  Cognitive and affective levels of learning with corresponding behavioral verbs for stating specific learning outcomes are listed in the following tables.  AFH 36-2235 Volume 10 1 November 2002 57  Level of learning objective verbs                    Cognitive  Note:  The same verbs may be used to describe multiple learning levels or learning tasks, because they identify what the learner is to do with the content.  The nature of the content to be learned helps determine the learning level or task.  For example, see the verb “Describe” identified in levels 1 and 3, or “Relate” in levels 4 and 5, or “Differentiate” in levels 3 and 4, etc.  1.  Knowledge.  The recall of previously learned material (facts or theories) in essentially the same form taught.  Acquire Identify Match Recognize  2.  Comprehension.  Seeing relationships, concepts, and Define Label Name Reproduce Describe List Outline Select Detect Mark Recall State abstractions beyond the simple remembering of the material.  Typically involves translating, interpreting, and estimating future trends.  Compare Distinguish Generalize Contrast Estimate Give Examples Paraphrase Summarize Convert Explain Illustrate Defend Extend Infer Interpret Represent  3.  Application.  The ability to use learned material in new and Predict Transform Rephrase Translate concrete situations, including the application of rules, methods, concepts, principles, laws, and theories.  Administer Develop Identify Predict Restructure Change Differentiate Manipulate Prepare Solve Compute Discover Modify Produce Transfer Demonstrate Employ Operate Relate Use AFH 36-2235 Volume 10 1 November 2002 58  Level of learning objective verbs (Continued)               4.  Analysis.  The ability to break down material into its component parts so the organizational structure may be understood, including identification of the parts, analysis of the relationships between parts, and recognition of the organizational principles involved.  Break Down Diagram Identify Point Out  5.  Synthesis.  The ability to put parts together to form new Classify Discriminate Outline Select Categorize Differentiate Illustrate Relate Deduce Distinguish Plot Separate patterns or structures, such as a unique communication (a theme or speech), a plan of operation (a research proposal), or a set of abstract relations (schemes for classifying information). Compile Design Formulate Produce Rewrite Compose Develop Generate Rearrange Tell  Combine Derive Explain Organize Relate  6.  Evaluation.  The ability to judge the value of material for a given purpose.  Learning in this area is the highest in the cognitive hierarchy because it involves elements of all the other categories, plus conscious value judgments based on clearly defined criteria. Create Devise Modify Reconstruct Write  Appraise Decide Justify  Assess Describe Relate Conclude Interpret Summarize Criticize Judge Validate AFH 36-2235 Volume 10 1 November 2002 59  Level of learning affective verbs                 Level of learning affective verbs (Continued)   Affective  Note:  The same verbs may be used to describe multiple learning levels or learning tasks, because they identify what the learner is to do with the content.  The nature of the content to be learned helps determine the learning level or task.  For example, see the verb “Develop” identified in levels 3 and 5, or “Explain” in levels 3 and 4, etc.  1.  Receiving.  The getting, holding, and directing of the student's attention, from the simple awareness that a thing exists to selective attention on the part of the learner.  Ask Give Name  2.  Responding.  The student not only attends to a particular Describe Identify Reply Choose Hold Point to Follow Locate Select phenomenon, but also reacts to it in some way such as acquiescence (reads assignment), or satisfaction in responding (reads for enjoyment).  Includes instructional objectives related to "interests." Assist Greet Practice Report  Answer Discuss Perform Recite  3.  Valuing.  The worth or value a student attaches to a particular object, phenomenon, or behavior ranging from acceptance of a value to commitment.  Includes instructional objectives related to "attitudes" and "appreciation." Conform Label Read Tell Comply Help Present Select  Complete Follow Join Select Describe Form Justify Share Differentiate Initiate Propose Study Explain Invite Report Work  4.  Organizing.  The bringing together of different values, resolving conflicts between them, and beginning to build an internally consistent value system.  Includes instructional objectives related to a "philosophy of life."  AFH 36-2235 Volume 10 1 November 2002 60            Arrange Defend Integrate Prepare Combine Explain Modify Relate Alter Complete Identify Organize Adhere Compare Generalize Order  5.  Characterizing by a Value or Value Complex.  Pervasive, consistent, and predictable behavior (lifestyle) developing from a value system which controls behavior for a significant period of time.  Instructional objectives focusing on personal, social, and emotional adjustments are in this category.  Act Listen Propose Serve  Discriminate Modify Qualify Solve Display Perform Question View Influence Practice Revise Verify AFH 36-2235 Volume 10 1 November 2002 61 Organization of Objectives  Organizing goals and objectives   Levels of objectives   Terms for objective levels   General to specific   In organizing objectives, you may work from general statements or goals to specific objectives to produce an objective hierarchy of the course structure and organization.  This produces a developmental structure that allows you to tell the students what they will achieve overall and the enabling outcomes that will contribute to this learning.   A learning outcomes hierarchy consists of two or more levels.  Terminal outcomes describe the end result of the instructional experience.  These outcomes may be published as goals and/or objectives. Developmental objectives are a series of enabling outcomes that increase the probability of achieving a complex terminal outcome.  This approach makes it possible to check learner performance in the developmental stage and provide feedback and remediation as necessary.   Learning hierarchies use various terms to describe educational outcomes.  The most common terms used to distinguish these levels are:  Terminal Outcome Goal and/or Objective Terminal Primary  Developmental Level Interim Enabling   At the course goal or mission level, the outcome is a broad, general statement.  However, as the course outcomes are analyzed and organized systematically into smaller units, the objectives become more specific.  This systematic arrangement provides a learning hierarchy for the course.  AFH 36-2235 Volume 10 1 November 2002 62  Course outline hierarchy  A learning hierarchy of objectives provides the structure for a course outline as shown below.  1.1  Course Goal (Mission Statement) or Objective (Specific Outcome) 1.1.1  Area Objective   1.1.1.1  Phase Objective  1.1.1.1.1  Period Objective    2.1  Level of Learning Hierarchy Objectives: 2.1.1  Course Goal (Mission Statement)               2.1.1.1  Area Objective  2.1.1.2  Phase Objective  2.1.1.3  Period Objectives  2.1.1.1.1  Measurable Samples of Behavior 2.1.1.2.1  Measurable Samples of Behavior 2.1.1.3.1  Measurable Samples of Behavior AFH 36-2235 Volume 10 1 November 2002 63 Section B Develop Tests  Introduction   Purpose   Testing practices   Characteristics of tests   An effective measurement program is essential for all instructional systems.  To ensure that tests adequately measure the objectives they support, the performance required in the test must match the performance required in the objective.  A good way for you to develop tests that measure the objectives is to develop tests immediately after you have developed the objectives and before starting to develop instruction.   Tests serve several purposes:  Identify problems or weaknesses in the instruction.  Indicate whether a class is performing up to standards on specific objectives. Indicate instructor proficiency. Diagnose problems in the instructional program.   Air Force educational measurement measures individual student performance with respect to standards that are specified in the objectives.  This form of testing measures individual student performance with respect to the standard of performance that are specified in the objectives.  To attain acceptable performance, the student must meet or exceed the standards specified for the test.   There are six basic characteristics that should be considered when developing tests to ensure they measure what is intended each time they are administered.  The characteristics are:   Characteristic Validity Reliability  Objectivity      Refers to Degree to which a test measures what it is intended to measure. Degree to which a test yields the same results consistently. Ability of a test to be free from variations due to  factors other than the behavior being measured. AFH 36-2235 Volume 10 1 November 2002 64  Characteristics of tests (Continued)   Characteristic Compre-hensiveness Differentiation Usability Refers to Adequacy of a test to sample what is being measured.  This characteristic relates to content validity. For a criterion-referenced test, the ability of the test to distinguish between students who have mastered the knowledge or skill being tested and those who have not mastered it.  For a norm-referenced test, the ability of the test to distinguish between level or amount of acquired knowledge or performance ability each student has attained. Test that is easy to administer, score, and interpret.   Classification of tests    Most Air Force tests can be classified into two main groups: written tests and performance tests.  There are two types of written tests:  Selection - Requires the student to choose the correct answer from several provided.  Examples are: Multiple choice. True-false. Matching. Supply – Requires that the student supply the answer from recall.  Examples are: Essay. Completion (short answer). Labeling (identification).  There are two types of performance tests:  Product tests, which produce a product that is tangible, permanent, and observable. Process tests, where verbal skills or procedures are observable but transient.   AFH 36-2235 Volume 10 1 November 2002 65  Test construction factors   Several key factors should be considered when constructing tests.  What to measure.  Determine what to measure from the objective to be covered by the test.  For selection type test items, two to four test items may be needed to adequately measure each sample of behavior.  Tests should measure application of principles, knowledge of factual information, ability to perform tasks, and transfer of knowledge and skills to solve similar problems.  The higher the level of learning, the more demanding, complex, and abstract is the performance expected.  Supply type test items may be designed such that one item measures several samples of behavior simultaneously.  Thus, the number of supply items needed to address all samples of behavior depends on the item’s complexity. Test length.  Adequate coverage of the course objective is one of the major factors in determining the length of the test.  Another important factor is the time available for testing.  If time available for testing does not allow for administering a test large enough to measure all course objectives, consider creating several smaller tests that could be administered sequentially throughout the course. Longer tests are considered more reliable than shorter tests because they cover more material, more completely.  However, the overall content validity of a series of short, related tests can be considered equivalent to the content validity of a single, large test containing items from all the smaller tests. Selection and arrangement of test items.  Select test items that cover the most important points of the material.  Test items should be clear, concise and well written to minimize misunderstandings.  Items of the same type should be grouped together in a test.  Individual test items should also be arranged in approximate order of difficulty, which allows the students to progress as far as they can without spending excessive time on difficult items at the first part of the test.    AFH 36-2235 Volume 10 1 November 2002 66  Constructing written tests   Multiple choice    Tests must be well written in order to measure what students have learned or can do.  You should be familiar with developing the various types of written tests.  Following is information on preparing the most common types of written tests.  Note:  Refer to Volume 12, Test and Measurements Handbook, Table 12 for additional Test Construction Guidelines.   Multiple-choice tests are probably the most-used written test.  They are used for testing problem-solving skills, application of facts and principles, and understanding.  A multiple-choice item consists of a stem (a question or uncompleted statement), a correct response, and distractors (incorrect responses).  They can be written at higher comprehension levels but can be very difficult to construct, especially for novice test writers.  Construction Guidelines  Do not use the articles "a" and "an" at the end of the stem; this can tend to indicate the correct answer. All responses should follow grammatically from the stem. All responses should be of approximately the same length. All responses should have a similar grammatical structure. All responses should use similar terminology. Provide as many responses as necessary, but normally no less than three. Position the correct response randomly throughout the final test.  (For purposes of internal review, you can make all correct answers "a".) Limit the use of responses such as "none of the above" or "all of the above." Distractors should be plausible, but incorrect. Responses should be arranged in order so that they increase or decrease in length.  AFH 36-2235 Volume 10 1 November 2002 67  True-False   Matching   True-false tests are often used when you desire students to identify a completely true or false statement.  True-false tests should be used sparingly, since the chance of random guessing is high.  Construction Guidelines  Include only one idea in each statement. Place the crucial element at or near the end of the statement. Avoid using negatives such as "not," as they tend to confuse students. Do not use absolutes such as "all,” "every," "none," and "never." Do not use statements containing "some," "any," and "generally."   Matching tests are used to measure the students' ability to identify and discriminate among related or similar items.  Matching items normally use two columns of related items, and students are required to match a series of items listed in one column with related items in the other column.  It provides a way to test multiple knowledge simultaneously.  Construction Guidelines  Provide clear, concise directions on how to match the items in the two columns. Indicate whether the responses may be used more than once or not at all. Limit test items to a single area and the choices to a single subject matter category. Arrange the responses in the same logical order.  AFH 36-2235 Volume 10 1 November 2002 68  Completion   Labeling or identification    The completion test item requires the students to recall and supply one or more key words that have been omitted from the statement.  The word(s), when placed in the appropriate blanks, make the statement complete, meaningful, and true.  Construction Guidelines  Leave blanks for key words only. Keep items brief. Make all blanks approximately the same size. Grammatical cues to the correct answer, such as the articles "a" and "an" right before the blank, should be avoided. Ensure that only one correct answer fits each block.   Labeling or identification tests are used to measure a student's ability to recall and label parts in pictures, schematics, diagrams, or drawings.  This form of testing is most often used to measure recognition of equipment components or other concrete objects.  Construction Guidelines  Make all sketches, drawings, and illustrations clear and of sufficient size.  If possible, use the actual parts of a unit. Provide sufficient information to indicate what the equipment is and which part is to be labeled. Parts to be labeled or identified should be clearly pointed out by using lines or arrows. Ensure that only one definite answer is possible. AFH 36-2235 Volume 10 1 November 2002 69  Essay testing should be used when students are required to think reflectively or creatively, to organize knowledge in the solution of a problem, and to express their solutions in writing.  The essay test encourages a wider range of study and learning than other test items.  Essay tests are not the same as written papers.  Although some essay tests may be rather comprehensive, they do not generally require the student to develop an extended logical or factual argument for more than a few hundred words.  However, in PME, longer essay responses are common.  Papers, on the other hand, require students to extend their arguments typically over several thousand words.  Construction Guidelines  Test several objectives in an essay. State the item clearly so the student will know exactly what is expected. The essay item should ask for comparisons, decisions, solutions, cause-effect relationships, explanations, or summaries. When possible, use more essay items and limit the discussion on each. Set limits on essay questions, such as time or number of words.  Essay   Additional information    For additional information on test development, see:  Angoff, W. H. (1971)  Scales, Norms, and Equivalent Scores.  In R. L. Thorndike (Ed.), Educational Measurement. Washington, D.C.: American Council on Education, 514. Berk, R. A. (1976). Determination of Optimal Cutting Scores in Criterion-referenced Measurement. Journal of Experimental Education, 45, 4-9. Ebel, R. L. (1972).  Essentials of Educational Measurement. Englewood Cliffs, New Jersey: Prentice Hall, 492-494. Jaeger, M. (1991). Selection of Judges for Standard-setting. Educational Measurement: Issues and Practice, 10,  3-6,10,14. Koffler, S. L. (1980)  A Comparison of Approaches for Setting Proficiency Standards.  Journal of Educational Measurement, 17, 167-178.  AFH 36-2235 Volume 10 1 November 2002 70  Additional information (Continued)   Livingston, S. A. (1980) Choosing Minimum Passing Scores by Stochastic Approximation Techniques.  Educational and Psychological Measurement, 40, 859-873. Livingston, S. A. and Zieky, M. J. (1982). Passing Scores: A Manual for Setting Standards of Performance on Educational and Occupational Tests.  Princeton, New Jersey: Educational Testing Service. Mills, C. N., Melican, G. J. and Ahluwalia, N. T. (1991).  Defining Minimal Competence.  Educational Measurement:  Issues and Practice, 10, 7-9. Nedelsky, L. (1954). Absolute Grading Standards for Objective Tests. Educational and Psychological Measurement, 14, 3-19. Plake, B. S. (1991). Factors Influencing Intrajudge Consistency During Standard-setting.  Educational Measurement: Issues and Practice, 10, 15-16, 22, 25-26. Reid, J. B. (1991). Training Judges to Generate Standard-Setting Data.  Educational Measurement:  Issues and Practice, 10, 11-14. Shepard, L. (1980).  Standard Setting Issues and Methods.  Applied Psychological Measurement, 4, 447-467. Shepard, L. (1984).  Setting Performance Standards.  In Berk, R. A. (Ed.), A Guide to Criterion-Referenced Test Construction. Baltimore, Maryland: Johns Hopkins University Press, Chapter 7. Smith, R. L. and Smith, J. K. (1988). Differential Use of Item Information by Judges Using Angoff and Nedelsky Procedures.  Journal of Educational Measurement, 25, 259-274. AFH 36-2235 Volume 10 1 November 2002 71 Section C Review and Use Existing Materials  Developing instructional materials is an expensive, time-consuming task regardless of the medium used.  After developing the objectives and tests, one of the first tasks is to determine if materials already exist that will support the objectives.  It is possible that some of the material found during the review may not totally satisfy the need.  In this case, don't hesitate to modify the materials to "fit" the need.  Even the use of some portions of existing materials will be economically advantageous.  In every case, time spent reviewing existing material will be time well spent.   Several benefits to be gained from using existing material are:   Introduction   Why review existing materials?  Where can you find materials?  Types of existing material Time - Developing instructional materials is time-consuming; therefore, using existing materials will save time. Personnel - Using existing materials saves man-hours that can be spent developing other portions of the instruction. Material - Valuable materials will be saved. Funds - Time, personnel, and materials cost money.   Materials exist that cover almost every aspect of instruction such as leadership, mathematics, weather, and management.  Several sources of existing materials are:  DoD. Other services. Other federal agencies. Industry/commercial. Colleges and universities. Defense Instructional Technology Information System (DITIS). Web-based resources.   Existing materials can be found in many different types of media.  For example, materials may exist in one or more of the following:  AFH 36-2235 Volume 10 1 November 2002 72   Types of existing material (Continued)  How to select materials Textbooks/publications/technical orders/handbooks. Slides/video.Audio cassettes. Computer-based systems such as ICW, CAI, CMI. Job aids. Training aids.   In order to select the appropriate materials, a deliberate and thorough review of existing materials must be conducted.  After it has been determined what instructional materials are needed to support the objectives and materials have been gathered for review, the review process is ready to begin.  A good way to conduct a review is to use a job aid. The job aid helps to standardize the process and allows a comparison between materials under review. Following is an example of a job aid that can be constructed to aid in the review of existing materials.      Existing Material Checklist Yes/No Does the material meet the requirements of the objective(s)? Is the content level of the material appropriate? Is the material accurate? Is the material current? Is the material copyrighted? Does the material address motivational factors? Is the material properly sequenced? Does the material provide sufficient guidance? Are sufficient practice exercises provided? Are the measurements adequate?  Other types of job aids or forms can be used to review and select materials. Keep the job aids or forms as simple as possible.   AFH 36-2235 Volume 10 1 November 2002   How to select materials (Continued)   73   Another example of a job aid that can be adapted for use is provided. Material Review Rating Form  Evaluator                                                             Date  1.  Objective  2.  3.  Content Evaluation of Material Type of Media            Poor  Good  Excellent  Structure  Suitability Accuracy Currency Level Author expert 1    2    3    4    5 1    2    3    4    5 1    2    3    4    5 1    2    3    4    5 Organization Sequence 1    2    3    4    5 1    2    3    4    5 1    2    3    4    5 Supports objective 1    2    3    4    5 User-friendly (readability)  1    2    3    4    5 Pace 1    2    3    4    5 Guidance 1    2    3    4    5 Feedback 1    2    3    4    5 Motivational5 1    2    3    4    5 Measurement Conditions of release or use  1    2    3    4    5 (normally not rated on a scale)  4.  What do I like about the material?  5.  What do I dislike about the material?  6.  Can the material be modified to improve its utility?  If so, what should be done?    AFH 36-2235 Volume 10 1 November 2002 74  Making existing material usable   Copyrighted material  Additional information   The materials you need may or may not exist.  If they do, they may require modification before they can be used in course development.  If this is the case, consider modifying or updating the material (unless it is copyrighted), since it is normally cost-effective and efficient to do so.   Modification of existing materials may include:  Adding new material. Expanding existing material. Deleting material. Updating material. Resequencing material.   If existing material is copyrighted, you should obtain permission from the publisher before using it.   For additional information on reviewing existing materials, see:  Dick, W. and Carey, L. (1990). The Systematic Design of Instruction (3rd Ed.). Glenview, Illinois: Harper Collins. Knirk, F. G. and Gustafson, K. L. (1986). Instructional Technology: A Systematic Approach to Education.  New York: Holt, Rinehart, and Winston.  AFH 36-2235 Volume 10 1 November 2002 75 Section D Design Instructional Plan  Introduction   Where to read about it       Once the objectives and tests have been developed and existing instructional materials have been reviewed for usability, you are ready to start designing instruction.  Designing quality, cost-effective instruction is the ultimate goal in this phase of ISD.   This section covers three topics.    Select Instructional Method Select Media Develop Lesson Plans  Page 77 84 88 Topic AFH 36-2235 Volume 10 1 November 2002 76 Select Instructional Method  Introduction   Definition   Continuum of instructional methods       In the design phase, one of the first and most important tasks is that of selecting the instructional method.  In order to select the most appropriate method, several key factors must be considered which have a direct impact on the effectiveness and efficiency of the instructional system.   Instructional method is the procedure or process used to attain an objective(s).  Examples of instructional methods are:  Lecture. Team learning and teaching. Demonstration. Self-study. Discussion. Cooperative Learning. Case Study. Experiential.   "Method" is the primary way a learning experience will be conducted.  One consideration in selecting a method is the degree of learner control desired.  Instructor-Centered Learning Dependence on instructors High stimulus control by instructor Information Knowledge Cognitive Intellectual Formal Passive learner role Learner-Centered Learning Dependence on each other and self Low stimulus control by instructor Behavior Attitudes Self-insight Experiential  Informal Active learner role AFH 36-2235 Volume 10 1 November 2002 77  Traditionally, most Air Force instruction has been instructor-led.  The instructor's role in the classroom placed the instructor in control.  Students had little control over what they learned, how they learned it, or how much time they spent.  It became very easy to develop instructional programs for the instructor rather than the student.  Emphasis is now being placed on students controlling the learning situation.  Emerging technologies, combined with adult learning theories, are changing the emphasis from instructor-controlled to learner-controlled instruction.  The first effort in learner-controlled instruction goes back many years to programmed text, which had limited application.  However, with the new technologies, the possibilities are almost unlimited.  The reasons for changing emphasis become apparent when you consider the following points. Interactive technologies, such as ICW, can allow the learner to control what objectives to work on, how to sequence the lessons, the pace of the instruction, and how many and what kind of examples to review. In many cases, adults learn better when they can control the learning situation according to their individual needs.  Research shows that self-pacing may hinder progress or completion.  Continuum of instructional methods (Continued)  Other factors to consider in method selection    Subject  Facilities  Target audience  Is direct information needed? Are there different points of view to be presented? Is this a controversial subject which will stimulate discussion? Is there a problem-solving dimension? Does the room lend itself to formal or informal use? How can the setting be adapted to facilitate discussion? Is equipment available? How large is the class? What is the level of education, aptitude, background, and interests of the class? AFH 36-2235 Volume 10 1 November 2002 78  Application of traditional methods   Some traditional methods of instruction are discussed below.  Method  Lecture (Formal and Informal) Definition Discourse given before a class or an audience for instructional purposes without question (Formal) or Interaction with the students Advantages Useful if time is short Many ideas can be presented Useful if number of instructors is limited Useful where subject matter changes frequently  Demonstration  Questioning   Accurate portrayal of the precise actions necessary to perform skills or processes - may be presented directly (classroom instructor) or indirectly (film, tv) Discourse by the student before an instructor in which the student relates what has been learned through previous study    Useful in teaching motor skills, simple manual skills or processes Sets standards of performance Focuses attention upon basic procedures Useful for assessment of learning by instructor Useful for providing feedback to student Useful for verbal content and concepts On-the-job instruction (informal) Formal course Correspondence course (on video or audiotape) Distance learning Knowledge building  Disadvantages  AppropriatenessLimits student participation (Example—Formal) Lecture becomes a "telling session" for instructor Checking student learning before testing is difficult Student attention and interest may wander Demonstrator must be skilled performer Since student does not perform during demonstration, student learning cannot be evaluated except through questioning Number of student observations may be limited Learning for recitation may be rote Participation of other students not reciting is limited and their attention and interest may wander On-the-job instruction Formal course Knowledge and skill building On-the-job instruction Formal course Knowledge building Motivation   AFH 36-2235 Volume 10 1 November 2002 79 Application of traditional methods (Continued)  Method  Guided Discussion Definition Instructor-controlled  interactive process of sharing information and experiences related to achieving a lesson objective  Performance Student interacting with things, data, or persons, as necessary to attain objectives – includes all forms of simulations and interaction with actual equipment or materials Advantages Useful as an extension of existing knowledge or to clarify and amplify familiar material Useful when students must learn to identify and solve problems and to frame their own decisions Useful when students need to be exposed to a variety of approaches, interpretations and personalities Useful when teamwork is needed Permits student to apply learning to actual situations Allows practice with job-similar conditions, under supervision and guidance Disadvantages  AppropriatenessTime-consuming and limited by class size Requires that participants have sufficient background so that they can talk about subject On-the-job instruction Formal course Knowledge building Motivation On-the-job instruction Formal course Skill building Time-consuming because students must be given  the opportunity to practice until they reach proficiency May require special facilities and equipment which may be expensive and difficult to obtain.  Once obtained, equipment must be constantly maintained       AFH 36-2235 Volume 10 1 November 2002 80 Application of traditional methods (Continued)  Disadvantages  AppropriatenessStudent must be motivated and have initiative Completion rates significantly lower Students object to lack of social interaction Correspon-dence course Formal course Knowledge and skill building Correspond-ence course Formal course Knowledge and skill building Development cost is comparatively high Development time and revision time are comparatively long because of validation Students using programmed instruction object to lack of social interaction Method  Self-Directed Definition Readings or document research which students undertake on their own without special guidance or instruction  Programmed Self-Instructional  Instructional materials are prepared specifically to employ techniques of programming  Classical programmed instruction variables include "small steps," carefully sequenced and cued to reduce error; immediate feedback; and freedom on the part of students to vary their own rate of learning Advantages Useful as an adjunct to other methods of instruction Useful as an improvement to individual's present job performance Useful to prepare an individual for a promotion Allows a student to pursue a special interest not shared by other students Useful in accommodating individual differences in rate of learning, background, and experience Useful if scheduling is a problem as students may work through materials when convenient Provides uniformity of instruction May be sole source of instruction or supplementary      AFH 36-2235 Volume 10 1 November 2002 81 Application of traditional methods (Continued)  Disadvantages  AppropriatenessFormal course Can become Seminar outdated quickly Development time and revision time can be relatively long Can be time-consuming in a discussion format Formal course Knowledge and skill building Motivation Students may be inhibited about participating Students may become so involved in simulation that they fail to observe processes Evaluation is difficult because behaviors affected by process are difficult to measure Formal course Seminar Not very effective for students with little or no experiential base Definition A carefully designed description of a problem situation, written specifically to provoke systematic analysis and discussion (Games) win/lose situations which dramatize certain principles  (Role-playing) active process in which learners "act out" selected situations Life experiences, (professional and personal) that provide context within which to internalize and assimilate new learning Advantages Can extend existing knowledge Promotes concept exploration and discussion Useful when teamwork is needed Students can "practice" taking the responses to various situations which are similar to the real job Active participation Expansion or compression of real time Allows focus on more subtle and less easily defined human relationships Gives student a "vested interest" in learning Virtually guarantees student will internalize new learning if it's tied to his/her previous experiences Method  Case Study   Games and Role-Playing   Experiential   AFH 36-2235 Volume 10 1 November 2002 82  Methods of instruction can determine success or failure in reaching desired learning outcomes.  One should match methods to the type of learning, as listed below.  TYPE OF LEARNING OUTCOME KNOWLEDGE (Generalizations about experience and internalization of information) UNDERSTANDING (Application of information and generalizations) SKILLS (Incorporation of new ways of performing through practice) VALUES (Adoption and priority arrangement of beliefs)    Matching methods to desired learning outcomes      APPROPRIATE METHODS Lecture, television, debate, dialogue, interview, symposium, panel, group interview, colloquy, motion picture, slide film, recording, book-based discussion, readingAudience participation, demonstration, motion picture, dramatization, Socratic discussion, problem-solving discussion, critical incident process, case method, games Role-playing, in-basket exercise, games, action mazes, participative cases, T-group, nonverbal skill practice exercises, drills, coaching Television, lecture (sermon), debate, dialogue, symposium, colloquy, motion picture, dramatization, guided discussion, experience-sharing discussion, role playing, critical incident, process, games, T-Group  AFH 36-2235 Volume 10 1 November 2002 83 Select Media  Introduction   Definition   Media guides   Instructional media     As you have seen, selection of instructional methods and media are discussed separately.  However, in many applications they can't be considered separately.  Regardless of whether they are considered together or separately, media selection is of no less importance to the design process.  No single medium is the most appropriate choice for every instructional situation.  Selecting the appropriate media ensures that the information to be learned is presented to the students by the most effective and efficient means possible.   Media are the means, instruments, or materials used to communicate information–in other words, a means of giving information to the students.   A series of media guides has been included in this section to assist you in:  Selecting media. Determining learning potential. Applying various technologies.   The following are examples of instructional media.  Instructional Medium Group Classroom instructor with classroom aids  Classroom instructor Instructional aids Representative Examples Lecturer Demonstrator Tutor/coach Overhead projector Film strip (silent) Film slides Chalkboard Computer Generated Visuals AFH 36-2235 Volume 10 1 November 2002 84  Instructional media (Continued) Instructional Medium Group Multimedia     Media selection for Distance Learning (DL) programs   Potential of instructional media   Representative Examples Pre-narrated slides Pre-narrated film strips Slide/workbook/tape recorder combinations Video cassette television Interactive courseware (ICW) Books Computers Programmed instruction booklets Microfiche Role playing Discussion groups Debates Case studies Problem or resolution scenarios Actual equipment trainers Gaming Interactive computer (simulation) Flight training simulators Print Peer (or peer group) Training devices and simulators  NOTE:  Don't forget to get permission to use copyrighted material.   Air University has developed a guide to aid the instructor in determining suitability of courses for DL conversion.  The guide gives an overview of DL media and supplies a survey to match media to learning objectives.  Copies of the "Distance Learning Curriculum Analysis and Media Selection" can be obtained from Air Force Institute for Advanced Distributed Learning (AFIADL), Maxwell AFB-Gunter Annex, AL 36114.   The following are examples of the potential of various instructional media.  AFH 36-2235 Volume 10 1 November 2002 85 Potential of instructional media (Continued)  Types of Learning   serutcPi llitSi schparGPotential of Instructional Media  VT / erutcPi noitoM denirtP txeT demmargorP txeTi sgndroceR ioduA stcebOj D-3 noitcarentI larOt sroaumSil renarTiLow  Avg Avg Avg Low  Avg Avg Avg Low  Avg  High Low Low  High Low Low  High  High Low  High Low  High Low  High Avg  High  Avg  High  Avg Avg Avg  High Avg  High  Avg Avg Avg Low  Avg  High Low  Avg Low Low  Avg Low  Avg  High Avg Avg Avg Low High High Learning Factual Information Learning to Make Identifications Learning to Make Perceptual Discriminations Learning to Follow Procedural Sequences Learning Principles and Relationships Learning to Make Decisions and Solve Problems Learning to Perform Motor Skill Actions Developing Desirable Attitudes, Opinions, or Motivations   Technologies as media     Technologies as media (Continued)  Low Low Low Low Low Low  Avg Low Low  High Low  Avg Avg Avg Low  Avg Avg Low   The following are examples of various technologies as media.  AFH 36-2235 Volume 10 1 November 2002 86 Application Technology ICW  CGI  VE X X X ITS  PSS  X X X X   X  X  Task requires complex performance-based equipment simulations Instructional approach is to use a hypertext/hypermedia format X Selection of instruction to present to learner is based on  diagnosis of what the computer thinks student needs to know Instruction to be developed for soft skills domain Instruction needed for basic skills or entry-level skills and knowledge Instructional content is stable, requires few updates Instructional content unstable, constantly changing Instruction involves dangerous environment and activities Instruction requires 3-dimensional spatial reasoning Instructional content demands a realistic environment Procedures difficult or impossible to videotape Instruction requires input from students through voice commands Learning requires repetitive cognitive practice Learner has complete control over amount of guidance and feedback Students have poor reading skills (extensive use of audio required) Instruction must be sent across phone lines or local area networks Instruction to be delivered on both PC's and Macintosh computers Existing resources (videotapes, slides, photographs) available for extensive use Instruction has no pre-set sequence Entry level skills and knowledge of learner are extremely diversified Content requires 3-D simulation Large number of students  ICW – Interactive Courseware VE –  Virtual Environment  PSS – Performance Support System    X X X   X   X X X X      X    X X  X    X    X X     X  X X X X  X  X    X  X  X X  X X       X      X   X X  X X X X   X  X X X X X X  X X  X CGI – Computer-Generated Imagery ITS –  Intelligent Tutoring System AFH 36-2235 Volume 10 1 November 2002 87 Develop Lesson Plans  Steps in lesson planning   Specify learning content   Design presentation sequence   The individual lesson plan is the plan you produce to organize what you present in a lesson, as well as when and how.  Recommended steps to use in the planning of new lessons include:  Determine the objective. Rough-out the evaluation instrument(s). Finalize the evaluation instrument(s). Research the topic defined by the objective. Select the instructional method. Identify your planning format. Decide how to organize the lesson. Choose the support material. Prepare the lesson from beginning to end. Prepare a final outline.  Your hierarchy of objectives should provide an adequate starting place for building individual lessons.  You may find it useful to attach learning planning products to your objectives as you proceed.   Identification of lesson content specifies the inputs–materials, information, instructions–the learner will use to achieve the objective.  Attaching the input specifications to the objective enables the planner to identify what resources and materials must be prepared or obtained and provided to learners in order to help them achieve behavioral objectives.   Once the objectives/content selection is fairly complete, the presentation sequence can be designed for the entire course.  The following guidelines are offered for the sequencing of content.  Place easily learned material early in the sequence. Introduce broad concepts and technical terms early in the sequence.  AFH 36-2235 Volume 10 1 November 2002 88  Design presentation sequence (Continued)    Place the practical application of concepts and principles close to the point of initial development. Place prerequisite knowledge and skills in the sequence prior to the points where they must be combined with subsequent knowledge, skills, and application. Provide for practice and review skills and knowledge, which are essential parts of later lessons. Introduce a skill concept in the lesson in which it is most frequently used. Do not overload any lesson with elements that are difficult to learn. Provide for practice of required skills and review of concepts and principles in areas where transfer of identical or related ability is not likely to occur unaided. Place complex or cumulative skills late in the sequence. AFH 36-2235 Volume 10 1 November 2002 89 Section E Develop Implementation Plan  An implementation plan documents the design of the instructional program and the plan for implementation.  Plans may include, but not be limited to, information such as:   Definition of implementation plan  Purpose of implementation plan  Identification information. Parameters of instructional system. Type of instruction. Instructional methods. Instructional content. Resource requirements. Design, development, and implementation milestones.  Implementation plans may include resource and control documents such as:  Tasking letters or messages. Equipment lists. Personnel documents. Facility requirements. Course maps. Course charts. Resource constraints. Lesson plans.   Implementation plans that are well developed have several purposes.  Some of the purposes are:  Document the instructional system. Identify resource requirements/constraints such as manpower, equipment or facilities. Set the design, development, and implementation milestones. Serve as the approval document for operation of the instructional system.  Implementation plans may serve other purposes as determined by the needs or requirements of the instructional system.  AFH 36-2235 Volume 10 1 November 2002 90  Who is responsible?   Implementation plans are the direct responsibility of instructional systems management.  Managers should develop a plan that describes or documents the instructional system.  In order to develop this plan, input will be required from other organizations such as instructional staff, resource managers, or civil engineers.  AFH 36-2235 Volume 10 1 November 2002 91 Design Instructional Information Management System Section F  The purpose of an automated instructional information management system is to enable those responsible to better manage system information in real time.  For example, the automated system can be used by:  Instructors to update student status. Registrar to track student status. Instructional designers to update instructional materials. Managers to manage resources.  Purpose   Who is responsible?   Designing a management system   Responsibility for designing an instructional information management system normally falls on a project manager.  However, course managers have the responsibility to ensure that the instructional information is adequately managed for their instructional system.   Managers will probably not have an option of using or not using an instructional information management system.  If one does exist, you will likely be required to use it to manage the information that it is capable of managing.  However, if a system doesn't already exist, and there are plans to install one, you may be called on to help design it.  Also, if a system does exist, you may be called upon at some time to help redesign it.  If you have the opportunity to help design or redesign an instructional information management system, there are several things that you should consider.    What is the cost? What are the hardware capabilities? Are there any special environmental requirements for the hardware? What are the software capabilities? Is the system documentation adequate? Who are the system users? Are the hardware and software user-friendly? Does this system have proven reliability? Is the system maintainable?  AFH 36-2235 Volume 10 1 November 2002 92  Designing a management system (Continued)  Will the system interconnect/interface to existing systems? Is the system software compatible with other software? Does the hardware have expansion capability? Can the software be upgraded or modified easily? What instructional information will the system be able to manage?  These are some of the items that should be considered when designing or redesigning an instructional information management system.  AFH 36-2235 Volume 10 1 November 2002 93  Overview Section G What to Evaluate in the Design Phase  Consider the following points:  Review the course objectives developed in this phase and ensure they are stated at appropriate levels of learning for the intent of the instruction.  Review test questions and ensure they are capable of measuring the instructional objectives at the stated levels of learning.  As this phase of the ISD process may be the most time-consuming part of the development effort, be sure to make a final check for existing course materials that may be useful in your program.  AFH 36-2235 Volume 10 1 November 2002 94 Section H Update ISD Management Plan  Project management   Purpose   Who is responsible?   What should be updated?  During your initial planning of the ISD project, the managers developed an ISD management plan to serve as a road map for managing the instructional development process and the instructional system.  You developed the plan early in the initial planning stages to plan how you are going to get where you are going.  As you complete the design phase of the ISD project, you will likely have gathered additional and better information on which to base your management decisions.  If this is the case, you will need to update the management plan.   Since the management plan is a "tool" for managing the instructional development process and the instructional system, update it at the end of each phase of instructional development, or when any significant change occurs that impacts planning.  If the plan is not continually updated with the most current, accurate information, it will fail as a management tool.   The ISD project manager is responsible for seeing that the management plan is updated at the end of each phase of ISD, or after any significant change has occurred that impacts the current planning.   The ISD management plan will be updated after each phase of instructional develop-ment, as applicable. Update the plan to include the latest information, such as:  New or revised milestones. Refinements to project definition. Changes in project resource requirements or constraints. Revisions to support requirements. Identification of new taskings. New information resulting from analysis that impacts project management.  AFH 36-2235 Volume 10 1 November 2002 95  What should be updated? (Continued)   Once you have updated the ISD management plan and ensured that previously discussed processes and procedures are in order, you are ready to enter the development phase.  But remember, ISD is flexible.  You will find yourself re-entering the decision phase as you progress through ISD.  AFH 36-2235 Volume 10 1 November 2002 96 Chapter 6 COURSEWARE DEVELOPMENT Overview  Introduction   Objectives   Where to read about it          Once you have completed your detailed course design, you are ready to enter the development phase.  Some of the tasks in this phase include writing materials, producing media, developing ICW and writing tests.   The objectives of this chapter are to:  Describe materials to be developed. Explain installation of the information management system. Discuss the plans that need to be updated. Describe the validation process.   This chapter contains six sections.  Section A B C D E F  Title  Prepare Course Syllabus Develop Instructional Materials Install Instructional Information Management System Validate Instruction What to Evaluate in the Development Phase Finalize Instructional Materials Page 99 100 104 105 115 116 AFH 36-2235 Volume 10 1 November 2002 97  Additional information   For additional information on courseware development, see:  Dick, W. and Carey, L. (1990).  The Systematic Design of Instruction (3rd Ed.). Glenview, Illinois: Harper Collins. Gery, G. (1987).  Making CBT Happen. Boston, Massachusetts: Weingarten Publications. Kearsley, G. and Halley, R. (1986).  Designing Interactive Software.  La Jolla, California: Park Row Press. Martinetz, C. (1986).  A Checklist for Course Evaluation.  Performance and Instruction Journal, 25(5), 1-8.  AFH 36-2235 Volume 10 1 November 2002 98 Section A Prepare Course Syllabus  Course syllabus contents   The course syllabus contains all details needed to implement the course.  Although a course syllabus can be in different formats, it is normally organized by units or modules, with each unit containing information such as:  Course identification such as title, number, security classification. Objectives. Preferred sequence. Hours and approximate allocations of hours to objectives. Standards that the unit supports. Instructor requirements. Methods such as lecture, discussion and case studies . Necessary support materials. Media utilization. Equipment utilization. Instructor guidance. Lesson plans. Copyright permission letters. Guest speaker invitations.  AFH 36-2235 Volume 10 1 November 2002 99 Section B Develop Instructional Materials  In the design phase, the method and medium best suited for the instructional need were selected.  At this point, you are ready to start developing the course materials.  Developing materials is a time-consuming and exacting task regardless of the medium you use.   Media are the means, instrument, or materials used to communicate information to the student.  Examples of media range from the classroom instructor using a lesson plan to study guides, CBT, satellite training, interactive video, web-based, or numerous other types of media.   Instruction can be delivered using the following media:  Instructor. Print-based material. Slide/tape presentations. Audio/video tapes. Interactive video. Training devices/aids. Satellite.   Development of materials for presenting instruction requires many activities.  Some of the most common development activities are listed in the table below.   Introduction   Description   Types of delivery   Development activities     Medium Print Transparencies    Development Activity Draft/write material. Edit material. Publish material. Copyright release or payment. Draft transparency. Generate reproducible transparency. Reproduce transparency. Development Activity    Development activities  Medium AFH 36-2235 Volume 10 1 November 2002 100 activities (Continued) Slide/Tape Videotape CBI Interactive Video Internet (Web-based Instruction)      Storyboard/script slide or tape. Shoot and edit slide/tape. Narrate audio. Print slide/tape. Storyboard/script. Shoot and edit video. Develop audio. Storyboard/script. Develop graphics. Program/code computer. Storyboard/script. Shoot and edit video. Develop graphics. Develop audio. Program/code computer. Draft/write material Edit material Develop graphics if necessary Take photographs (if necessary) and convert to digital images Shoot video (if necessary) and convert to digital video stream Develop audio if necessary Convert to web-based language (HTML, SHTML, DHTML, etc.) Develop additional control programming, if necessary (Java, Java Script, etc.) AFH 36-2235 Volume 10 1 November 2002 101  Who is responsible?   Developing materials normally involves teamwork and requires various skills.  Curriculum developers are responsible for planning, scheduling, and making sure the materials are produced and correlated with the evaluation instrument.  The following table lists some team members involved in production of different types of materials.           Medium  Print Transparencies Slide/Tape Videotape CBI Interactive Video Internet (Web-based Instruction)  Development Role Subject Matter Expert. Curriculum Developer. Editor. Graphic Artist. Curriculum Developer. Graphic Artist. Script Writer. Subject Matter Expert. Photographer. Sound Technician. Script Writer. Subject Matter Expert. Video Producer, Editor, Photographer. Sound Technician. Script Writer. Subject Matter Expert. Graphic Artist. Computer Programmer. Script Writer. Subject Matter Expert. Video Producer, Editor, Photographer. Graphic Artist. Sound Technician. Computer Programmer. Subject Matter Expert Curriculum Developer Editor Graphic Artist (if necessary) Audio Technician (if necessary) Video Producer, editor, photographer (if necessary) Computer Programmer (HTML, Java, Java Script, etc. AFH 36-2235 Volume 10 1 November 2002 102  Guidelines for developing materials   Additional information   When developing course presentation materials, make sure they:  Support the objectives. Are student-centered. Meet the design that was specified in the design phase. Use techniques that are consistent with the principles of effective learning. Are appealing to the students. Are constructed so students will be attentive.  Focus on making the material interesting and meaningful to the students. Require the students' attention.  One way to accomplish this is to require specific responses or actions. Lead the student in the direction of the behavior specified in the objective.  With the proper stimuli and reinforcement, the desired student behavior can be shaped. Are developed using experts such as programmers, photographers, graphic artists, script writers and editors in order to develop high-quality materials. Are checked for such items as technical accuracy, completeness, programming errors, and blurred slides prior to publication or production. Have the appropriate level of vocabulary for the target population. Are properly paced; not too fast or too slow. Are easy to understand. Include the appropriate safety precautions. Support the human relations concepts to which the Air Force is committed.   For additional information on development of materials, see:  AFMAN 36-2234, Instructional System Development. AFMAN 36-2236, Handbook for Air Force Instructors. Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992). Instructional Design Strategies and Tactics.  Englewood Cliffs, New Jersey: Educational Technology Publications.  AFH 36-2235 Volume 10 1 November 2002 103 Install Instructional Information Management System Section C  Introduction   What should you look for?   As you learned in Chapter 5, Section F, an automated instructional information management system will facilitate management in real time.  As a curriculum developer, you may never be involved in designing or redesigning an information management system.  However, if you are involved, you may also be involved in the installation of the system.  This section lists the basic issues to be considered when installing an information management system.   If you are involved in installing or testing an information management system, there are several things you should consider.  Is the hardware user-friendly? Is the software user-friendly? Are there adequate terminals for all system users to do their jobs in a timely manner? Is the information management in your area accurate? Is the information management in your area complete? Is the system reliable?  AFH 36-2235 Volume 10 1 November 2002 104 Section D Validate Instruction  ISD is more an art than a science.  It offers no fixed formulas that guarantee outcomes–only guidelines.  Validation is a quality improvement tool that helps identify problems in the instruction during development so that revisions can be made.   Validation is a process that assesses the effectiveness of a course as it is being developed.  Its purpose is quality improvement.  It is a process of repetitive cycles of development, tryouts, and revisions until evidence shows that the instruction is effective.   It is important that validation be conducted during, not after, development.  The purpose is to correct mistakes or problems before you spend too many resources on a flawed product.  Validation should be done as segments, units, or blocks are being developed.   This section covers five topics. Topic  Develop Validation Plan Conduct Internal Reviews Conduct Individual Tryouts Conduct Small-Group Tryouts Conduct Operational (Field) Tryouts  Page 106 107 110 112 113  Introduction   What is validation?   When should you validate?   Where to read about it        AFH 36-2235 Volume 10 1 November 2002 105 Develop Validation Plan  Purpose   What is in a validation plan?   As with any other plan, a validation plan provides curriculum developers and instructors with a "road map" for validating the course.  A validation plan adds structure and creditability to the validation process.   Validation plans contain the following information:  Description of course to be validated (objectives, method, media). Validators. Validation procedures. Validation schedules. Number of tryouts to be conducted. Number of students to be used in small-group and field tryouts. Sources and how results will be documented. How problems will be resolved.  Remember, include only necessary information in the validation plan and keep it simple.  AFH 36-2235 Volume 10 1 November 2002 106 Conduct Internal Reviews  Introduction   Who should review?   What should be reviewed?         The first step in the validation process is internal reviews.  These reviews identify content inaccuracies, instructional weaknesses, and resource shortfalls.   Individuals who perform internal reviews include:  Content experts, who make sure the content is accurate and the coverage is adequate. Curriculum developers, who ensure that the material follows sound instructional principles and that the methods and activities are appropriate to the content and specified target population.   Materials should be reviewed as they are developed.  The table below lists materials to be validated for each type of media.   Print-based materials  Media Type Materials To Be Validated List of objectives Test items Drafts of printed materials List of objectives Test items Plans of instruction Lesson plans Drafts of support materials (transparencies, etc) List of objectives Test items Storyboard/script List of objectives Instructional design/strategies Test items Storyboard/script Graphics/video produced Screens developed Classroom  Traditional audiovisual materials (videotapes, film slide, etc.) Interactive courseware (IVD, CBT)   AFH 36-2235 Volume 10 1 November 2002 107   What should be reviewed? (Continued)  Media Type  Internet (Web-based Instruction) Materials To Be Validated List of objectives Instructional design/strategies Test items Program (HTML, Java, Java Script, etc.) code to ensure it functions properly on the various web "browsers" students are likely to use If interactive, synchronous communication is desired, ensure "chat room" software functions properly and can support the number of students expected to participate If electronic bulletin boards or discussion groups (asynchronous communication protocols) are used, ensure they function properly Ensure all hyperlinks connect to their expected locations (if links are external to your site, testing of links is recommended at least every three months to ensure external sites are still "active")   Common problems identified    Some common problems that you may identify in the material during the internal reviews are:  Lack of agreement. Inaccuracies in subject matter. Incomplete material. Weaknesses. Lack of continuity. Lack of timeliness in preparation or review.  AFH 36-2235 Volume 10 1 November 2002 108  After a review   Upon completion of the internal reviews, you should:  Determine the changes needed to improve the materials. Decide the best way to make changes to the materials. Make the needed changes to the materials.  AFH 36-2235 Volume 10 1 November 2002 109 Conduct Individual Tryouts  Introduction   Purpose   Select students for tryouts   Before a tryout     During the individual tryouts, the curriculum developer tests the materials on individual students.  Use several students for comparison in order to gain valid and reliable results.  In long-term education courses this may not be practical, and other testing methods may have to be adapted.   The purpose of individual (one-on-one) tryouts is to determine the effectiveness of small segments or units of materials as they are developed.   When selecting students for individual tryouts, consider these issues.  Students selected should represent the target population in: Aptitude Attitude Prior knowledge Background experience Select students for the first tryouts from the upper percentage ranges in aptitude and background.  The reasons for selecting these students include: Above-Avg students often point out and analyze weaknesses in the materials. If better students cannot learn the material, less capable students will not be able to. If lower-level students are used in the individual test and they do well, there is no way to tell if the material is at the right level. It is easier to work down from a known point of difficulty than to work up from an unknown point of difficulty.   You should prepare the students for the tryout by explaining:  Purpose of the tryout. Their role in the tryout. That they are not being evaluated–the training is. That their feedback is necessary to determine the quality of the material. AFH 36-2235 Volume 10 1 November 2002 110  During the individual tryouts, the curriculum developer should:  Use a pretest to identify entry behavior. Use a post test to assess learning as a result of the tryout. Observe where each student seems to have problems. Give assistance only when essential. Note where errors occur, type of errors, and how many students make the same error(s). Get student views about difficulties encountered during the tryout. Ask each student for suggestions.  During a tryout   Common problems identified   After a tryout    Some common problems that you may identify during individual tryouts are:  Improper sequencing of instruction. Inadequate content. Inadequate explanation of content. Wrong assumption about target population's prerequisite knowledge. Confusing test items. Test items that do not measure objectives.   After individual tryouts, analyze the data to determine if there are problems.  Change the material as appropriate.  AFH 36-2235 Volume 10 1 November 2002 111 Conduct Small-Group Tryouts  Purpose  Before a small-group tryout   During a tryout   After a tryout     The purpose of conducting small-group tryouts is to determine if the instruction is appropriate for the Avg target students.   Before a small-group tryout, you should:  Determine the number of students and groups to be used in the tryouts. Select representative students from the target population. Ensure that instruction has been revised in accordance with results from individual tryouts. Ensure that student materials are available in adequate quantities. Establish the number of trials a student will be permitted to achieve the criterion performance.   During a small-group tryout, you should:  Record the time each student takes to complete the material.  This information will be used to validate unit time, course lengths, and course content. Record student responses.  This information will help determine the weaknesses of the materials.  Do not supplement the materials.  This will skew the results of the tryout.   After a tryout, analyze the results to:  Identify the Avg completion time for each segment or unit.  This information determines the exact time for lessons, segments, units, or modules. Revise the materials. AFH 36-2235 Volume 10 1 November 2002 112 Conduct Operational (Field) Tryouts  Purpose   Student selection  During a tryout   Data collection        The final stage of validation may be called field, or operational, tryouts.  All of the materials should be complete and ready to go.   These tryouts serve several purposes:   Determine if the instruction actually accomplishes the objectives. Provide feedback from a large sample of the target population for final revisions. Identify implementation problems, such as equipment and facilities.   For field tryouts, students are assigned to the tryout group using the normal class assignment process.   In many respects, a field tryout is like the normal day-to-day operation of a course, in that you should: Present the instruction in a normal environment. Collect validation data such as time requirements, measurement results, instructor and student comments, problem areas. Use adequate class samples to ensure that the data are both valid and reliable.   You may collect data from students before, during, and after the training.  Stage Data To Be Collected Before During After  Student entry skill/knowledge level Number of errors students make Questions raised by students Student learning gains Student views about the course Data Collection Methods Pretest Oral examination Student interviews Observations Recording student questions Student critiques Post test Student interviews Student critiques  AFH 36-2235 Volume 10 1 November 2002 113  After a tryout   Use the tryout to: Analyze the data gathered during the tryouts. Revise the materials as necessary.  AFH 36-2235 Volume 10 1 November 2002 114 What to Evaluate in the Development Phase Section E  Introduction   This phase of the ISD process includes validation of the instruction you have created.  Evaluation of this phase takes place during the internal reviews, individual, small-group, and operational try-outs, as described earlier in this chapter.  The data you collect from these validation trails should be fed back into the most appropriate phases of the development process to effect improvements.  For example, if test questions need improvement, return to the Design Phase.  Another example would be if data indicated that your initial analysis of the course's target audience was in error, you should return to the Analysis Phase.  Carefully develop an appropriate feedback mechanism between the evaluator and the instructor and course developer.  In this feedback mechanism, the program evaluator will analyze and interpret the curriculum data received through the evaluation system (tests, critiques, surveys, etc.).  He/she should provide the analysis and interpretations, along with recommendations for curriculum improvement, through a formal channel to course supervisors, instructors, and developers so they may consider the evaluator's recommendations for improvement.  Not all, or even any, of the evaluator's recommendations may be accepted by management or instructional personnel.  Whether or not the recommendations are accepted, management and instructional personnel should formally respond to the recommendations indicating how they will be incorporated or the rationale for not incorporating them.  This formal feedback mechanism from evaluator to manager/instructor and back to evaluator is to provide an audit trail of curricular decisions and their supporting data and rationales.  Such audit trails are much more that mere paperwork drills.  They constitute the "corporate" decision trail for the entire instructional program, which is something that both accreditors and auditors look for as evidence of a mature instructional program.  This evaluation feedback mechanism should be incorporated into the final course structure when it is officially implemented in the final phase of the ISD process.  AFH 36-2235 Volume 10 1 November 2002 115 Section F Finalize Instructional Materials  Purpose   Quality checklist   The purpose of finalizing materials is to ensure that they:  Have been revised to include the most current and accurate information. Are complete. Are ready for implementation.   The following list of questions may help to ensure that everything is ready for implementation.  Use it to develop your own personalized checklist as you desire.  Course Syllabus  Has the course syllabus been updated? Is the course syllabus complete? Has the course syllabus been approved? Has the course syllabus been published and distributed?  Lesson Materials  Print Materials Have the student workbooks been updated? Are the student workbooks complete? Have the student workbooks been published? Have the instructor lesson plans been updated? Are the instructor lesson plans complete? Have the instructor lesson plans been approved and published? Have copyright permissions been obtained?  Audiovisual Have the transparencies been updated? Are the transparencies complete? Are the transparencies ready for use? Have the slides been updated? Are the slides complete? Are the slides ready for use?  AFH 36-2235 Volume 10 1 November 2002 116  Quality checklist (Continued)  ICW Has the program been updated? Is the programming complete? Has the ICW been operationally tested?  AFH 36-2235 Volume 10 1 November 2002 117 Chapter 7 IMPLEMENTATION Overview  Introduction   Objectives   In this chapter      Additional information   At this point in the ISD process, the course has been validated and you are ready to implement the instruction.  Prior to actual implementation, you need to ensure that everything is ready to support it.  For example, are the system functions in place, are adequate resources available, and is the instructional system itself ready?   The objectives of this chapter are to:  Discuss final preparations for course implementation. Describe implementation activities.   This chapter contains two sections.  Section A B Title  Implementation of System Functions  Conducting Instruction Page 119 120   For additional information on implementing instruction, see:  Burkman, E. (1987).  Factors Affecting Utilization.  In Gagné (Ed.), Instructional Technology: Foundations.  Hillsdale, New Jersey: Erlbaum Associates.  AFH 36-2235 Volume 10 1 November 2002 118  What you do   What they are   Additional information  Section A Implementation of System Functions  As a curriculum developer, it is unlikely that you will ever be required to implement the system functions.  Your job will be to ensure that these functions are being performed to support, operate, and maintain courses.   The system functions of management, support, administration, and delivery are described fully in the second chapter of this handbook.  Evaluation is an ongoing function that is discussed in the next chapter.   For additional information on system management functions, see:  AFMAN 36-2234, Instructional System Development, Implementation chapter. Bills, C. G. and Butterbrodt, V. L. (1992). Total Training Systems Design Function: A Total Quality Management Application.  Wright-Patterson AFB, Ohio: Aeronautical Systems Center. Fishburne, R. P., Williams, K. R., Chatt, J. A. and Spears, W. D. (1987). Design Specification Development for the C-130 Model Aircrew Training System: Phase I Report. Williams AFB, Arizona: Air Force Human Resources Laboratory (AFHRL-TR86-44). JWK International Corp. (1990). Final Training System Baseline Analysis Report (EWOT). Dayton, Ohio: JWK International Corp. Williams, K. R., Judd, W. A., Degen, T. E. Haskell, B. C. and Schutt, S. L. (1987). Advanced Aircrew Training Systems (AATS): Functional Design Description. Irving, Texas: Seville Training Systems (TD-87-12).  AFH 36-2235 Volume 10 1 November 2002 119 Section B Conducting Instruction  Introduction   In this section       Additional information  To this point, considerable time has been spent planning the instructional system, securing resources, managing the development process, and arranging for system support.  Curriculum developers have analyzed, designed, developed and validated the instruction.  Now the course is ready to implement.  During implementation, you will continually evaluate its effectiveness.   This section covers two topics.   Preparations for Conducting Instruction Conducting Instruction   For additional information on conducting instruction, see:  Page 121 122 Topic AFMAN 36-2236, Handbook for Air Force Instructors, 15 Jan 84.   AFH 36-2235 Volume 10 1 November 2002 120 Preparations for Conducting Instruction  Introduction   What should be checked?    Although preparations for conducting instruction started in initial planning and have continued throughout the analysis, design, and development phases of ISD, final checks need to be made prior to implementing the course.  Sufficient preparation is essential to successful instruction.   The type of instruction being implemented will determine what should be done to get ready.  For example, an exportable course will not have the same items to be checked as a course taught in a classroom environment.  Some of the last-minute preparation may include checking the following:  Equipment Equipment is available in adequate numbers and in operational condition.  Equipment support is available. Facilities Appropriate facilities are available.  Facility modifications, such as electrical and air conditioning, are complete. Human Resources Personnel are ready, including instructors and students. Students have been scheduled for the classes. Funds Adequate funds are available to meet implementation costs and the costs associated with daily operation of the course. Time Curriculum developers have had adequate time to develop the materials. Materials and Supplies Materials are available in adequate quantities to support instruction.  Supplies are available in adequate quantities to support implementation. Copyright Approvals Copyright permissions have been obtained.      There is no substitute for good planning and adequate preparation.AFH 36-2235 Volume 10 1 November 2002 121 Conducting Instruction  Introduction   Conducting instruction   If adequate preparations have been made throughout the development process and a last-minute check of each system component was made, there should be few problems, if any, encountered with implementation.   Once the course is implemented and becomes operational, it will normally remain so until there is no longer a need.  The focus will be on conducting effective and efficient instruction.  Ongoing activities that ensure system integrity are described below.  Resource management is probably the single most critical issue.  Resources must be well managed to gain maximum benefit. Faculty development is an activity that goes on continually while instruction is being conducted.  Curriculum developers as well as other staff and faculty should periodically attend courses, conferences, and workshops that help them develop professionally in curriculum development. Conducting instruction is at the center of system integrity.  No matter what has been done to this point, the program can fail if the course is not properly conducted. Some helpful items to remember are: Instruction should be student-centered. Instruction should follow the course syllabus. Curriculum developers or instructors should always perform professionally in the teaching-learning environment.    Teaching staff should be qualified to perform their duties. Evaluation maintains the quality of instruction.  During the life of the course, evaluation is continually performed to ensure the quality of the program.  It is covered in detail in the next chapter of this handbook.   AFH 36-2235 Volume 10 1 November 2002 122 Chapter 8 EVALUATION Overview  Description  Objectives   In this chapter      Evaluation is a function of the ISD process that require the continuous design, collection and analysis of data from an instructional program or a course to determine its value or worth as outlined by the institution's mission and goals.  It focuses its attention in four basic areas:  (1)  Did learning take place (in terms of a change in cognitive knowledge), (2)  Were students' attitudes affected by their learning experience, (3)  Did a change in behavior take place, and (4)  Did the learning (cognitive and behavioral changes) cause the student's organization to become more efficient and/or effective?   The objectives of this chapter are to:  Describe the operational evaluation process. Explain the four steps in the evaluation process. Explain internal and external evaluation.   This chapter contains two sections.  Section A B Title  Levels of Education Evaluation Internal and External Evaluation Page 124 126  AFH 36-2235 Volume 10 1 November 2002 123 Section A Levels of Educational Evaluation  Introduction   Definition   Who is responsible?   What should you look for?    Educational program evaluation consists for four levels.  These levels provide instructional managers with an overall perspective of how well the instruction achieved the established goals and objectives of the school or organization.  Program evaluation is an ongoing process that begins when the instructional program is activated and continues through the life of the program.  The nature and scope of the program evaluation will depend, to a great degree, on the type of course being evaluated.  Courses exported to the field, or taught via distance learning methods, may not have the same evaluation requirements as courses taught in-residence, but the basic four levels of program evaluation should still be addressed.   Program evaluation is the continual process of evaluating an instructional program to identify and correct problems and to maintain and improve the quality of the program.   Most schools have personnel designated as course/program evaluators.  In lieu of such personnel, curriculum developers are typically responsible for evaluation of their courses.  Other organizations or activities, such as inspection teams or other command evaluation programs, may also provide input for evaluating your course.    When conducting a program evaluation, look for both strengths and weaknesses in the course, with special emphasis on:  Effectiveness and efficiency of the course How well the graduates are performing in their jobs If instruction is being provided that is not needed If needed instruction is not being provided  AFH 36-2235 Volume 10 1 November 2002 124  Levels of program evaluation  Additional information   Program evaluation consists of four levels, each of which characterizes a different aspect of the instructional program.  Level 1 – Student Learning – focuses on whether or not learning took place. Level 2 – Student Attitude – regarding the course, did the student see a benefit and the value in participating in the program / course. Level 3 – Behavioral Change – did the student modify his or her behavior as a result of having attended the program / course.  Is the student using the concepts and principles taught during the course on the job? Level 4 – As a result of having attended the program / course, did the student's job performance become more effective or efficient?   For additional information on evaluation, see:  AFMAN 36-2234, Instructional System Development, Implementation chapter. Kirkpatrick, D.L. (1994). Evaluating Training Programs.  Berrett-Koehler Publishers, San Francisco. Gronlund, N.E. (1998). Assessment of Student Achievement.  6th ed. Allyn and Bacon, Needham Heights, MA. Popham, W.J. (1999). Classroom Assessment:  What teacher's need to know.  2nd ed. Allyn and Bacon, Needham Heights, MA. Haladyna, T.M. (1997). Writing Test Items to Evaluate Higher Order Thinking. Allyn and Bacon, Needham Heights, MA.  AFH 36-2235 Volume 10 1 November 2002 125 Section B Internal and External Evaluation  Overview  Definition   Who is responsible?   What is required?   There are two major divisions in program evaluation:  internal and external evaluations.  The internal evaluation component is composed of the first two levels of the program evaluation process.  These levels focus attention on maintaining the quality of the course.  Sometimes internal evaluations are referred to as course reviews where the evaluation component is looking specifically at whether or not learning took place and how the students perceived their learning experience, i.e., did the student see the value in attending the course?  The external evaluation component is composed of the last two levels of the program evaluation process.  The last two levels focus attention on determining if the knowledge, skills, and attitudes acquired in the course enhanced the graduate's job proficiency.   Internal evaluation is the periodic, ongoing process of gathering and analyzing internal feedback and management data from within the school environment in order to assess the success of the instruction.  External evaluation is the periodic, ongoing process of gathering and analyzing feedback data on graduates of the course.   Curriculum developers are responsible for periodically conducting internal and external evaluations of their courses.  Other organizations or agencies will normally be involved in the evaluation by providing input data in the form of inspection reports or critiques.   Three basic stages are involved in accomplishing any type of program evaluation:  Data collection Data analysis Interpreting results and revising the curriculum  Each of these stages is discussed in further detail below.  AFH 36-2235 Volume 10 1 November 2002 126  The first stage of program evaluation is made up of several facets:  Identify who is responsible for gathering the evaluation data. Decide when and where the data will be collected. Determine data collection sources.  Some sources are: Pre and post test data. Self-inspections. Formal reports. Student critiques. Direct observations. Faculty evaluations. Surveys. Questionnaires. Field visits. Job performance evaluations.   Stage 1 –  Data Collection   Stage 2 –  Analyze data  Establish procedures for collecting the evaluation data Collect the data.   Before beginning this stage, the evaluator will need to ensure that sufficient data samples have been collected in order to validate the reliability of the finding and to ensure that there is generalizability from the sample to the population.  There is no easy or obvious way to determine how many samples are enough to ensure a representative sampling.  Realistically speaking, time and money often are limiting factors in the sampling process.  Regardless of how large or small your sample is, however, you should strive to collect enough data from a variety of sources to ensure you feel confident that the sample you collected is as representative as possible.  Data analysis consist of several activities:   Identify who is responsible for analyzing the data. Decide when the data will be analyzed. Determine validity of the data. Establish procedures for analyzing the data. Analyze the data. Publish the results of the analysis.  AFH 36-2235 Volume 10 1 November 2002 127  Stage 3 –  Interpreting results and revising the curriculum   When is it done?   The evaluator makes recommendations for curriculum improvement based on the interpretations of his/her data analysis.  This activity often brings the evaluator and the curriculum developer together to discuss the importance of the interpretations and what they mean for curriculum revision.  This is a crucial stage in the evaluation process, because it is where the ISD system "closes the loop" so to speak on the curriculum development process.  To adequately complete the cyclic ISD process, the curriculum developer must respond to all of the evaluator's recommendations for curriculum improvement.  This does not mean that all, or any, of the evaluator's recommen-dations must be adopted.  However, it does mean that the curriculum developer must respond with a justifiable rationale if a recommendation is not accepted.  This channeling of evaluation data back into the curriculum revision process not only defines the ISD model's cyclic nature, it also ensures that improvements in the instructional program can be tied directly to decisions based on evaluation of the program's success.  This stage of program evaluation consists of the following activities:    Determine, based on the data analyses, what revisions are required. Identify what components of the system are affected. Assess the impact of the revisions. Coordinate and/or get approval to revise. Revise the course materials. Update documentation.   Internal evaluations are conducted while the student is attending the course or program.  These data provide the school with immediate feedback as to how well the students met the educational objectives of the course or program.  External evaluations are conducted after the students have graduated from the course or program, usually 6 months or so.  These data provide the school with input regarding how effective and efficient the course or program is to the user's organization.  Both internal and external evaluations are ongoing processes and will continue for the life of the course or program.  AFH 36-2235 Volume 10 1 November 2002 128  Summary   The following are summary points:  Evaluation is a formal phase of the ISD process. Evaluation is conducted to ensure that the process is being executed as planned and to provide feedback to the faculty and curriculum developers on the program's success. Evaluations focus on curriculum, the students, and the instructors. The ISD process in education courses can take 6 -12 months or longer to develop. Evaluation provides a systematic and continuous process to furnish data and information for assessing the effectiveness and efficiency of educational programs throughout the cycle and beyond. RICHARD E. BROWN, Lt General, USAF DCS/Personnel         AFH 36-2235 Volume 10 1 November 2002 129 Attachment 1 GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION  AFPD 36-22 AFI 36-2201 AFI 36-2301 AFMAN 36-2234 AFMAN 36-2236 AFH 36-2235        Military Training   Developing, Managing and Conducting Military Training   Professional Military Education  Instructional System Development   Handbook for Air Force Instructors  Vol 1 Vol 2  Vol 3  Application to Acquisition Vol 4   Manager's Guide to New Education and Training Technologies Vol 5  Advanced Distributed Learning:  Instructional Technology and Information for Designers of Instructional Systems (12 Volumes) ISD Executive Summary for Commanders and Managers ISD Automated Tools/What Works Distance Learning Vol 6  Guide to Needs Assessment Vol 7  Design Guide for Device-based Aircrew Training Vol 8  Application to Aircrew Training Vol 9  Application to Technical Training Vol 10  Application to Education Vol 11  Application to Unit Training Vol 12         Walter Dick, Lou Carey, Jomes O. Carey (2000).  The Systematic Design of Instruction Test and Measurement Handbook 5th Edition.  Addison-Wesley Pub Co.  ISBN 0321037804.  Patricia Smith & Tillman Ragan (1999).  Instructional Design, 2nd Edition.  John Wiley & Sons  399 pages. ISBN 047136570X.  Ruth Clark (1999).  Developing Technical Training 2nd Edition:  A Structured Approach for Developing Classroom and Computer-based Instructional Materials.  International Society for Performance Improvement. 238 pages.  ISBN 1-890289-C7-8  M. David Merrill (1994).  Instructional Design Theory.  Educational Technology Pub.  ISBN 0-87778-275-X.    AFH 36-2235 Volume 10 1 November 2002 130 M. David Merrill, Robert D. Tennyson, and Larry O. Possey (1992).  Teaching Concepts:  An Instructional Design Guide 2nd Edition.  Educational Technology Pub.  ISBN 0-87778-247-4  Robert M. Gagné, Leslie J. Briggs, and Walter W. Wager.  (1992).  Principles of Instructional Design 4th Edition.  Wadsworth Pub. Co.  365 pages.  ISBN  00300347572  Robert M. Gagné (1985).  The Conditions of Learning and Theory of Instruction 4th Edition.  Holt, Rinehart and Winston. ISBN 0-03-063688-4  Jeroen J. G. van Merriënboer (1997).  Training Complex Cognitive Skills.  Educational Technology Publications.  Ruth Clark (1998).  Building Expertise:  Cognitive Methods for Training and Performance Improvement.  International Society for Performance Improvement.  204 pages.  ISBN  1890289043.    Bernice McCarthy (1996).  About Learning.  Excel, Inc.  452 pages.  ISBN 0-9608992-9-4  Malcolm Fleming & W. Howard Levie (Editors) (1993).  Instructional Message Design:  Principles from the Behavioral and Cognitive Sciences 2nd Edition.  Educational Technology Publications.  331 pages.  ISBN 0-87778-253-9.  Ellen D. Gagné, Frank R. Yekovich, and Carol Walker Yekovich (1993).  The Cognitive Psychology of School Learning.  2nd Edition.  Addison Wesley Longman, Inc. 512 pages.  ISBN 0673464164.  Marcy P. Driscoll (1999).  Psychology of Learning for Instruction 2nd Edition.  Allyn & Bacon.  448 pages.  ISBN 0205263216  Ann E. Barron & Gary W. Orwig (1997).  New Technologies for Education:  A Beginner's Guide 3rd Edition.  Libraries Unlimited.  ISBN 1563084775  Robert Heinich & Michael Molenda (1998).  Instructional Media and Technologies for Learning 6th Edition.  Prentice Hall.  428 pages.  ISBN 0138591598.  Diana Laurillard (1993).  Rethinking University Teaching:  A Framework for the Effective Use of Educational Technology.  Routledge.  ISBN 0415092892.  Tom Boyle and Tim Boyle (1996).  Design for Multimedia Learning.  Prentice Hall.  275 pages.  ISBN 0132422158. William W. Lee and Diana L. Owens (2000).  Multimedia-Based Instructional Design:  Computer-Based Training, Web-Based Training, and Distance Learning.   Jossey-Bass.  304 pages.  ISBN 0787951595. AFH 36-2235 Volume 10 1 November 2002 131  Margaret Driscoll & Larry Alexander (Editor) (1998).  Web-Based Training:  Using Technology to Design Adult Learning Experiences.  Jossey-Bass Inc.  288 pages.  ISBN 0787942030.  Brandon Hall (1997).  The Web-Based Training Cookbook.  John Wiley & Sons.  496 pages.  ISBN 0471180211.  Stephen M. Alessi & Stanley P. Trollip (2000).  Computer Based Instruction.  Allyn & Bacon.  432 pages.  ISBN 0205276911.  Andrew S. Gibbons & Peter G. Fairweather (1998).  Computer-Based Instruction.  Educational Technology. 570 pages.  ISBN 0877783012.  Douglas M. Towne (1995).  Learning and Instruction in Simulation Environments.  Educational Technology.  351 pages.  ISBN 0877782784.  Thomas M. Duffy & David H. Jonassen (Editors) (1992).  Constructivism and the Technology of Instruction:  A Conversation.  Lawrence Erlbaum Associates.  221 pages.  ISBN 0805812725.  Brent G. Wilson (1995).  Constructivist Learning Environments:  Case Studies in Instructional Design.  Educational Technology Publications.  ISBN 0877782903.  Roger C. Schank (Editor) (1997).  Inside Multi-Media Case Based Instruction.  Lawrence Erlbaum Associates.  451 pages.  ISBN 080582538X.  David H. Jonassen, Wallace H. Hannum & Martin Tessmer (1999).  Task Analysis Methods for Instructional Design.  Lawrence Erlbaum Associates.  275 pages.  ISBN 0805830863.  Allison Rossett (1999).  First Things Fast:  A Handbook for Performance Analysis.  Jossey-Bass. 241 pages.  ISBN  0787944386.  Charles M. Reigeluth (Editor) (1983).  Instructional-Design Theories and Models: An Overview of their Current Status.  Lawrence Erlbaum Associates.  487 pages. ISBN 0-89859-275-5.  Charles M. Reigeluth (Editor) (1999).   Instructional-Design Theories and Models:  A New Paradigm of Instructional Theory.  Vol. II.  Lawrence Erlbaum Associates.  715 pages.  ISBN0-8058-2859-1.  Tennyson, Robert D.,  Schot, Franz, Norbert, Seel & Dijkstra, Sanne. (Editors)  (1997).  Instructional Design International Perspective Vol. 1  Theory, Research, and Models.  Lawrence Erlbaum Associates. 475 pages.  ISBN 0-8058-1397-7.  AFH 36-2235 Volume 10 1 November 2002 132 Sanne Dijkstra, Norbert Seel, Franz Schott & Robert D. Tennyson (Editors) (1997).  Instructional Design:  International Perspective Vol. 2:  Solving Instructional Design Problems.  Lawrence Erlbaum Associates. 418 pages.   ISBN 0805814000.  George M. Piskurich, Peter Beckschi, and Brandon Hall (Editors) (1999). The ASTD Handbook of Training Design and Delivery:  A Comprehensive Guide to Creating and Delivering Training Programs -- Instructor-led, Computer-based.  McGraw Hill. 530 pages. ISBN 0071353105.  Charles R. Dills & A. J. Romiszowski (Editors) (1997). Instructional Development Paradigms.  Educational Technology Publications. 882 pages.  ISBN 08777882954.  Sanne Dijksra, Bernadette van Hout Wolters & Pieter C. van der Sijde (Editors) (1990).  Research on Instruction: Design and Effects.  Educational Technology Publications.  ISBN 0877782210.  David H. Jonnassen (Editor) (1996).  Handbook of Research on Educational Communications and Technology. Macmillan.   1267 pages.  ISBN 0028646630.  Byron Reeves & Clifford Nass (1996).  The Media Equation:  How People Treat Computers, Television, and New Media Like Real People and Places.  Cambridge University Press.  305 pages.  ISBN 1-57586-053-8.   Donald A. Norman (1990).  The Design of Everyday Things.  Doubleday & Company.  256 pages.  ISBN 0385267746,  Anderson, J. R., (1976) Language, Memory and Thought.  Hillsdale, New Jersey:  Angoff, W. H. (1971). Scales, Norms, and Equivalent Scores.  In R. L. Thorndike (Ed.), Educational Measurement. Washington, D.C.: American Council on Education, 514.  Ausubel, D. P. (1968). Educational Psychology: A Cognitive View.  New York: Holt, Rinehart and Winston.  Bayard-White, C. (1990). Interactive Video: The Development Process.  The Videodisc Monitor, 18-21.  Anderson, J. R., Boyle, C. F., and Reiser, B. J. (1985).  Intelligent Tutoring Systems.  Anderson, R. H. (1976).  Selecting and Developing Media for Instruction.  New York: Erlbaum Associates. Science, 228, 456-468. Van Nostrand Reinhold. AFH 36-2235 Volume 10 1 November 2002 133  Bergman, R. and Moore, T. (1990).  Managing Interactive Video/Multimedia Projects.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  Berk, R. A. (1976). Determination of Optimal Cutting Scores in Criterion-referenced Measurement. Journal of Experimental Education, 45, 4-9.  Berk, R. A. (1986).  A Consumer's Guide to Setting Performance Standards on Criterion-Referenced Tests.  Review of Educational Research, 56(1).  Bills, C. G. and Butterbrodt, V. L. (1992). Total Training Systems Design Function: A Total Quality Management Application. Wright-Patterson AFB, Ohio: Aeronautical Systems Center.  Bloom, B. S., Hastings, J. T. and Dadaus, G. F. (1971).  Handbook on Formative and Summative Evaluation of Student Learning.  New York:  McGraw-Hill.  Bloom, B. S. (Ed.), Englehart, M. D., Furst, E. J., Hill, W. K. and Krathwohl, D. R. (1956).  Taxonomy of Educational Objectives, Handbook I: Cognitive Domain.  New York: Longman.  Bobrow, D. G. (1968).  Natural Language Input for a Computer-Solving System.  In M. Minski (Ed.), Semantic Information Processing.  Cambridge, Massachusetts: MIT Press.  Breuer, K. and Kummer, R. (1990).  Cognitive Effects from Process Learning with Computer-Based Simulations. Computers in Human Behavior, 6, 69-81.  Briggs, L. J.  and Wager, W. W. (1981).  Handbook of Procedures for the Design of Instruction (2nd Ed.).  Glenview, Illinois: Harper Collins.  Brown, J. S., Collins, A. and Duguid, P. (1989). Situated Cognitions and the Culture of Learning. Educational Researcher, 18, 32-42.  Bruner, J. S., Goodnow, J. J. and Austin, G. A. (1956).  A Study of Thinking. New York: Wiley.  Bruner, J. S. The Act of Discovery.  Harvard Educational Review, 31, 21-32. AFH 36-2235 Volume 10 1 November 2002 134  Burkman, E. (1987).  Factors Affecting Utilization.  In Gagné (Ed.), Instructional Technology: Foundations.  Hillsdale, New Jersey: Erlbaum Associates.  Carlisle, K. E. (1986).  Analyzing Jobs and Tasks.  Englewood Cliffs, New Jersey: Educational Technology Publications.   Cram, D. D. (1979).  Professor T-Pop. Performance and Instruction, 18(6).  Darabi, G. A. and Dempsey, J. B. (1989).  A Quality Control System for Curriculum-based CBI Projects.  Journal of Educational Technology Systems, 18(1), 15-31.  Davies, I. K., (1976). Objectives in Curriculum Design.  London: McGraw-Hill.  Dick, W. and Carey, L. (1990). The Systematic Design of Instruction (3rd Ed.). Glenview, Illinois: Harper Collins.  Ebel, R. L. (1972). Essentials of Educational Measurement. Englewood Cliffs, New Jersey: Prentice Hall, 492-494.  Ebel, R. L. and Frisbie, D. A. (1986).  Essentials of Educational Measurement (4th Ed.). Englewood Cliffs, New Jersey: Prentice Hall.  Eisner, E. W. (1985).  The Educational Imagination:  On the Design and Evaluation of School Programs (2nd Ed.). New York: Macmillan.  Feltovich, P. J., Spiro, R. J. and Coulson, R. L. (1988).  The Nature of Conceptual Understanding in Biomedicine: The Deep Structure of Complex Ideas and the Development of Misconceptions.  In D. Evans and V. Patel (Eds.), The Cognitive Science in Medicine. Cambridge, Massachussets: MIT Press, 113-172.  Fishburne, R. P., Williams, K. R., Chatt, J. A. and Spears, W. D. (1987). Design Specification Development for the C-130 Model Aircrew Training System: Phase I Report. Williams AFB, Arizona: Air Force Human Resources Laboratory (AFHRL-TR86-44).  Gagné. R. M. (1977).  The Conditions of Learning (3rd Ed.). New York: Holt,  Gagné, R. M. (1985).  The Conditions of Learning (4th Ed.).  New York: Holt, Rinehart and Winston. Rinehart and Winston.  Gagné, R. M. and Merrill, M. D. (1990).  Integrative Goals for Instructional Design.  Educational Technology, 38(1), 22-30. AFH 36-2235 Volume 10 1 November 2002 135  Gagné, R. M. and Briggs, L. J., (1974). Principles of Instructional Design. New York: Holt, Rinehart, and Winston.  Gardner, H. and Hatch, T. (1989).  Multiple Intelligences Go to School: Educational Implications of the Theory of Multiple Intelligence.  Educational Researcher, 18 (8), 4-10.  Gery, G. (1987).  Making CBT Happen. Boston, Massachusetts:  Weingarten Publications.  Glaser, R. (1990).  The Reemergence of Learning Theory Within Instructional Research.  American Psychologist, 45(1), 29-39.  Greer, M. (1992).  ID Management:  Tools and Techniques for Instructional Designers and Developers.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  Hannum, W. (1990). The Application of Emerging Training Technology.  Alexandria, Virginia: American Society for Training and Development.  Heinich, R., Molenda, M. and Russell, J. D. (1989).  Instructional Media and the New Technologies of Instruction (3rd Ed.). New York: Macmillan.  Jaeger, M. (1991). Selection of Judges for Standard-setting.  Educational Measurement: Issues and Practice, 10, 3-6, 10,14.  Jehng, J. C. (1990).  The Nature of Epistemological Beliefs About Learning.  Doctoral dissertation.  Champaign, Illinois: University of Illinois.  Jehng, J. C. and Johnson, S. D. (1991).  The Situated Nature of Students' Beliefs about Learning.  Paper presented at 1991 Annual Meeting of American Educational Research Association, Chicago.  Jenkins, J. J. (1973).  Language and Memory.  In G. A. Miller (Ed.),  Communication, Language, and Meaning: Psychological Perspectives.  New York: Basic Books.  Johnassen, D. H. (1989).  Performance Analysis.  Performance and Instruction, 28(4), 15-23.  JWK International Corp. (1990). Final Training System Baseline Analysis Report (EWOT).  Dayton, Ohio:  JWK International Corp.  Kearsley, G. and Halley, R. (1986). Designing Interactive Software. La Jolla, California: Park Row Press. AFH 36-2235 Volume 10 1 November 2002 136  Kellog, R. T. (1982).  When Can We Introspect Accurately About Mental Processes?  Memory and Cognition, 10, 141-144.  Kibler, R. J. (1981). Objectives for Instruction.  Boston, Massachusetts: Allyn and Bacon.  Knirk, F. G. and Gustafson, K. L. (1986). Instructional Technology: A Systematic Approach to Education. New York: Holt, Rinehart, and Winston.  Knowles, M. S. (1987).  Adult Learning: A Neglected Species (2nd Ed.). Houston: Gulf Publishing.  Knowles, M. S. (1987).  Adult Learning.  In R. L. Craig (Ed.), Training and Development Handbook: A Guide to Human Resource Development.  New York:  McGraw-Hill.  Koffler, S. L. (1980). A Comparison of Approaches for Setting Proficiency Standards. Journal of Educational Measurement, 17, 167-178.  Krathwohl, D. R., Bloom, B. S. and Masia, B. (1964).  Taxonomy of Educational Objectives, Handbook II: Affective Domain.  New York:  David Mckay.  Kule, J. (1985).  Volitional Mediators of Cognition-behavior Consistency: Self-regulator Processes and Action Versus State Orientation.  In J. Kule and J. Beckmann (Eds.), Action Control: From Cognition to Behavior.  New York: Springer.  Lave, J. (1988). Cognition in Practice.  Boston, Massachusetts: Cambridge University Press. AFH 36-2235 Volume 10 1 November 2002 137 Lefebvre-Pinard, M. and Pinard, A. (1985).  Taking Charge of One's Cognitive Activity: A Moderator of Competence.  In E. D. Neimark, R. De Lisi, and J. L. Newman (Eds.), Moderator of Competence (43-76).  Hillsdale, New Jersey: Erlbaum Associates.  Leshin, C. B., Pollock, J. and Reigeluth, C. M. (1992).  Instructional Design Strategies and Tactics.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  Litchfield, B. and Dempsey, J. (1992).  A Seven-Stage Quality Control Model for Interactive Videodisc Projects.  Journal of Educational Technology Systems, 20(2), 129-141.  Livingston, S. A. (1980). Choosing Minimum Passing Scores by Stochastic Approximation Techniques. Educational and Psychological Measurement, 40, 859-873.  Livingston, S. A. and Zieky, M. J. (1982). Passing Scores: A Manual for Setting Standards of Performance on Educational and Occupational Tests. Princeton, New Jersey: Educational Testing Service.  Mager, R. E. and Pipe, P.  Analyzing Performance Problems.  Belmont, California:  Fearon.Mager, R. (1973).  Measuring Instructional Intent, or Got a Match?  Belmont, California:  Fearon.   Mager, R. F. (1962).  Preparing Objectives for Instruction (2nd Ed.).  Belmont, California: Fearon.  Martinetz, C.  (1986).  A Checklist for Course Evaluation.   Performance and Instruction Journal,  25(5), 1-8.  Mayer, R. E. (1977). The Sequencing of Instruction and the Concept of Assimilation-to-Schema. Santa Barbara, California: Department of Psychology, University of California.  Merrill, M. (1983).  Component Display Theory.  In C. M. Reigeluth, (Ed.), Instructional Design Theories and Models: An Overview of Their Current Status.  Hillsdale, New Jersey: Erlbaum Associates.  Mills, C. N., Melican, G. J. and Ahluwalia, N. T. (1991).  Defining Minimal Competence.  Educational Measurement: Issues and Practice, 10, 7-9.  Nedelsky, L. (1954).  Absolute Grading Standards for Objective Tests. Educational and Psychological Measurement, 14, 3-19.  AFH 36-2235 Volume 10 1 November 2002 138 O'Neil, H. F., Jr. and Baker, E. L. (1991).  Issues in Intelligent Computer-assisted Instruction: Evaluation and Measurement.  In T. Gutkin and S. Wise (Eds.), The Computer and the Decision Making Process.  Hillsdale, New Jersey:  Erlbaum Associates.  Plake, B. S. (1991). Factors Influencing Intrajudge Consistency During Standard-setting.  Educational Measurement: Issues and Practice, 10, 15-16, 22, 25-26.  Popham, W. J. (1978).  Criterion-Referenced Measurement.  Englewood Cliffs, New Jersey: Prentice Hall.  Reid, J. B. (1991). Training Judges to Generate Standard-setting Data.  Educational Measurement: Issues and Practice, 10, 11-14. AFH 36-2235 Volume 10 1 November 2002 139  Reigeluth, C. M. (1983).  Instructional Design:  What Is It and Why Is It?  In C. M. Reigeluth (Ed.), Instructional Design Theories and Models: An Overview of Their Current Status.  Hillsdale, New Jersey:  Erlbaum Associates.  Reiser, H. and Gagné, R. M. (1983).  Selecting Media for Instruction.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  Rogoff, B. and Lave, J. (1984) Everyday Cognition: Its Development in Social Context.  Cambridge, Massachusetts: Harvard University Press.  Rossett, A. (1987).  Training Needs Assessment.  Englewood Cliffs, New Jersey:  Educational Technology Publications.  Rumelhart, D. E. and Norman, D. A. (1978). Accretion, Tuning, and Restructuring: Three Modes of Learning.  In J. W. Cotton and R. Klatzky (Eds.).  Salomon, G. and Globerson, T. (1987).  Skill May Not Be Enough: The Role of Mindfulness in Learning and Transfer.  International Journal of Educational Research, 11, 601-712.  Schneider, W. and Fisk, D. (1984).  Automatic Category Search and Its Transfer.  Journal of Experimental Psychology:  Learning, Memory, and Cognition, 190, 1-15.  Shepard, L. (1980). Standard Setting Issues and Methods. Applied Psychological Measurement, 4, 447-467.  Shepard, L. (1984). Setting Performance Standards.  In R. A. Berk (Ed.), A Guide to Criterion-referenced Test Construction. Baltimore, Maryland: Johns Hopkins University Press, Chapter 7.  Shiffrin, R. M. and Schneider, W. (1977).  Controlled and Automatic Human Information Processing: II. Perceptual Learning, Automatic Attention, and General Theory.  Psychological Review, 84, 127-190.  Simon, I. A. (1979).  Information Processing Models of Cognition. Annual Review of Psychology,  30, 383-396.  Smith, R. L. and Smith, J. K. (1988).  Differential Use of Item Information by Judges Using Angoff and Nedelsky Procedures. Journal of Educational Measurement, 25, 259-274.  Spiro, R. J., Coulson, R. L., Feltovich, P. L. and Anderson, D. K. (1988).  Cognitive Flexibility Theory:  Advanced Knowledge Acquisition in Ill-structured Domains.  AFH 36-2235 Volume 10 1 November 2002 140 In Proceedings of the Tenth Annual Conference of the Cognitive Science Society.   Hillsdale, New Jersey: Erlbaum Associates, 375-383.  Stufflebeam, D. (1985).  Conducting Educational Needs Assessment. Boston: Kluwer-Nijhoff.  Tennyson, R. D. and Breuer, K. (1991).  Problem-oriented Simulations to Develop and Improve Higher-order Thinking Strategies.  Computers in Human Behavior, 3, 151-165.  Tennyson, R. D. and Michaels, M. (1991).  Foundations of Educational Technology: Past, Present and Future.  Englewood Cliffs, New Jersey: Educational Technology Publications.  Tobias, S. (1982).  When Do Instructional Methods Make a Difference?  Educational Researcher,  11(4), 4-9. AFH 36-2235 Volume 10 1 November 2002 141 Wager, W. W., Briggs, L. J. and Gagné, R. M.  Principles of Instructional Design (4th Ed.).  Orlando, Florida: Harcourt Brace Jovanovich.  West, C. K., Farmer, J. A. and Wolff, P. M. (1991).  Instructional Design:  Implications from Cognitive Science.  Englewood Cliffs, New Jersey:  Prentice Hall.  Williams, K. R., Judd, W. A., Degen, T. E. Haskell, B. C. and Schutt, S. L. (1987). Advanced Aircrew Training Systems (AATS): Functional Design Description. Irving, Texas: Seville Training Systems (TD-87-12).  Zemke, R. and Kramlinger, T. (1982).  Figuring Things Out:  A Trainer's Guide to Needs Assessment and Task Analysis.  Menlo Park, California: Addison-Wesley.  AFH 36-2235 Volume 10 1 November 2002 142 Abbreviations and Acronyms AFH AFI AFMAN AFPAM AFPD CAI CBI CBT CGI CMI DoD IC ISD ITS PME POI PSS VE  Air Force Handbook Air Force Instruction Air Force Manual Air Force Pamphlet Air Force Policy Directive Computer-assisted Instruction Computer-Based Instruction Computer-Based Training Computer-generated Imagery Computer-managed Instruction Department of Defense Interactive Courseware Instructional System Development Intelligent Tutoring System Professional Military Education Plan of Instruction Performance Support System Virtual Environment AFH 36-2235 Volume 10 1 November 2002 143 TERMS The following list of definitions includes those terms commonly used in education as it relates to instructional system development and as used in this handbook.  The list is not to be considered all-inclusive.  Association.  The connection made between an input (stimulus) and an action (response).  Attitude.  (a) The emotions or feelings that influence a learner's desire or choice to perform a particular task.  (b) A positive alteration in personal and professional beliefs, values, and feelings that will enable the learner to use skills and knowledge to implement positive change in the work environment.  Also see Knowledge and Skill.  Behavior.  Any activity, overt or covert, capable of being measured.   Cognitive Strategies.  The capability of individuals to govern their own learning, remembering, and thinking behavior.   Computer-Assisted Instruction (CAI).  The use of computers to aid in the delivery of instruction. A variety of interactive instructional modes are used including tutorial, drill, practice, gaming, simulations, or combinations.  CAI is an integral part of computer-based instruction (CBI) and computer-based training (CBT).  Computer-Based Instruction (CBI) and Computer-Based Training (CBT).  The use of computers to aid in the delivery and management of instruction.  CBI and CBT are synonymous and are used interchangeably.  CAI (the delivery of instruction) and CMI (computer-managed instruction) are both elements of CBI and CBT.  Computer-Managed Instruction (CMI).  The use of computers to manage the instructional process in CBI.  Management normally includes functions such as registration, pretesting, diagnostic counseling, progress testing, and posttesting.  CMI is also used to schedule and manage educational resources and actual equipment.   Constraints.  Limiting or constraining conditions or factors, such as policy considerations, time limitations, equipment, environmental factors, personnel, budgetary, or other resource limitations.  Course.  (a) Logically grouped instruction on a subject, designed to achieve predefined learning objectives. It usually concerns a single job or task (job-skills-type training) or a section of organized knowledge (information-type training).  (b) A complete series of instructional units identified by a common title or number. (c) An ordered arrangement of subject matter designed to instruct personnel in the AFH 36-2235 Volume 10 1 November 2002 144 knowledge, skills, or attitudes required for the performance of tasks in a designated area of specialization.  A course consists of one or more modules.    Course Chart. A qualitative course control document that states the course identity, length, and security classification, lists major items of equipment, and summarizes the subject matter covered.  Course Control Documents.  Specialized publications used to control the quality of the instructional system. Examples are standards, plans of instruction, syllabi, and course charts.   Courseware.  Training materials such as technical data, textual materials, audiovisual instructional materials, and computer-based instructional materials.    Criterion.  (a) The standard by which something is measured.  (b) In test validation, the standard against which test instruments are correlated to indicate that accuracy with which they predict human performance in some specified area. (c) In evaluation, the measure used to determine the adequacy of a product, process, behavior, and other conditions.  Criterion-Referenced Test (CRT).  A test to determine, as objectively as possible, a student's achievement in relation to a standard based on criterion objectives.  During instructional development, the CRT can be used to measure the effectiveness of the instructional system.  The test may involve multiple-choice items, fill-in items, essays, or actual performance of a task.  If given immediately after the learning sequence, it is an acquisition test; if given considerably later, it is a retention test; if it requires performance not specifically learned during instruction, it is a transfer test.   Discrimination.  The process of making different responses to a stimulus. A discrimination requires a person to determine the differences among inputs and to respond differently to each.  Distance Learning.  Training that is exported, such as from a resident course to a field location.  It is instruction that takes place without the physical presence of the instructor.   Duty.  A large segment of the work done by an individual; major divisions of work in a job.   Education.  The formal academic instruction offered by institutions of higher learning that focuses on the study of the nature and principles of a given discipline.  Evaluation.  A judgment expressed as a measure or ranking of trainee achievement, instructor performance, process, application, instructional material, and other factors (see DoD Inst. 29612).  It includes  Formative Evaluation; Operational Evaluation; and Summative Evaluation.  AFH 36-2235 Volume 10 1 November 2002 145  Exportable Instruction.   See Distance Learning.   External Evaluation.  The acquisition and analysis of feedback data from outside the formal training environment to evaluate the graduate of the instructional system in an operational environment.  Also called Field Evaluation.  Also see Operational Evaluation.  Feedback.  Information that results from or is contingent upon an action. The feedback does not necessarily indicate the rightness of an action; rather, it relates the results of the action from which inferences about correctness can be drawn.  Feedback may be immediate, as when a fuse blows because a lamp is incorrectly wired; or delayed, as when an instructor provides a discussion pertaining to an exam taken the previous week, or when completed graduate evaluation questionnaires are reviewed.  Fidelity.  The degree to which a task or a training device represents the actual system performance, characteristics, and environment.  Field Evaluation.  See External Evaluation.  Formative Evaluation.  An activity that provides information about the effectiveness of educational materials to meet learning objectives and the student acceptance of instructional materials as they are being developed.  It is conducted while the instructional system or course is still under development, to gather data on lessons, units, or modules of instruction as they are completed. The purpose is to make improvements to the system or course while development is still in progress.  Also see Evaluation.  Generalization.  Learning to respond to a new stimulus that is similar, but not identical, to one that was present during original learning.  For example, during learning a child calls a beagle and spaniel by the term "dog"; a child who has generalized would respond "dog" when presented with a hound.   Instructional Objective.  See Objective.  Instructional Strategy.  An overall plan of activities to achieve an instructional goal.  Instructional System.  An integrated combination of resources (students, instructors, materials, equipment, and facilities), techniques, and procedures performing effectively and efficiently the functions required to achieve specified learning objectives.  Instructional System Developer.  A person who is knowledgeable of the instructional system development (ISD) process and is involved in the analysis, design, development, implementation, and evaluation of instructional systems.  Also AFH 36-2235 Volume 10 1 November 2002 146 called Instructional Designer, Instructional Developer, Curriculum Developer, Curriculum Development Manager, and other terms.  Instructional System Development (ISD).  A deliberate and orderly, but flexible, process for planning,  developing, implementing, and managing instructional systems.  ISD ensures that personnel are taught in a cost-efficient way the knowledge, skills, and attitudes essential for successful job performance.  Interactive Courseware (ICW).  Computer-controlled instruction designed to allow the student to interact with the learning environment through input devices such as keyboards and light pens. The student's decisions and inputs to the computer determine the level, order, and pace of instructional delivery, and forms of visual and aural outputs. (May include CAI, CMI, CBI.)  Interactive Videodisc (IVD).  A form of ICW instruction that specifically makes use of videodisc technology. Video and audio signals are pressed onto the laser videodisc; programming codes may or may not be pressed onto the disc depending on the IVD level. As a result, motion sequence, still-frame shots, computer-generated graphics, and audio may be displayed and heard through a monitor under computer and user control.    Internal Evaluation.  The acquisition and analysis of feedback and management data from within the formal training environment to assess the effectiveness of the instructional system.  Also see Operational Evaluation.  Job.  The duties, tasks, and task elements performed by an individual.  The job is the basic unit used in carrying out the personnel actions of selection, training, classification, and assignment.  Job Aid.  A checklist, procedural guide, decision table, worksheet, algorithm, or other device used by a job incumbent to aid in task performance.  Job aids reduce the amount of information that personnel must recall or retain.  Job Analysis.  The basic method used to obtain salient facts about a job, involving observation of workers, conversations with those who know the job, analysis questionnaires completed by job incumbents, or study of documents involved in performance of the job.  Job Performance Requirements (JPR).  The tasks required of the human component of the system, the conditions under which these tasks must be performed, and the quality standards for acceptable performance. JPRs describe what people must do to perform their jobs.  Job Task Analysis.  A process of examining a specific job to identify all the duties and tasks that are performed by the job incumbent at a given skill level.  Also called Task Analysis. AFH 36-2235 Volume 10 1 November 2002 147  Knowledge.  Use of the mental processes which enable a person to recall facts, identify concepts, apply rules or principles, solve problems, and think creatively.  Knowledge is not directly observable.  A person manifests knowledge through performing associated overt activities.  Also see Attitude and Skill.  Learning.  A change in the behavior of the learner as a result of experience. The behavior can be physical and overt, or it can be intellectual or attitudinal.  Media.  The delivery vehicle for presenting instructional material or basic communication stimuli to a student to induce learning.  Examples are instructors, textbooks, slides, interactive courseware (ICW), and TV.  Metrics.  Measurement tools used for assessing the qualitative and quantitative progress of instructional development with respect to the development standards specified.  Mission Analysis.  A process of reviewing mission requirements, developing collective task statements, and arranging the collective tasks in a hierarchical relationship.  Motor Skill.  Physical actions required to perform a specific task. All skills require some type of action.  Norm-Referenced Test.  The process of determining a student's achievement in relation to other students.  Grading "on the curve" involves norm-referenced measurement since an individual's position on the curve (grade) depends on the performance of other students.  Generally, norm-referenced measurement is not appropriate in the Air Force ISD process.  Objective.  A statement that specifies precisely what behavior is to be exhibited, the conditions under which behavior will be accomplished, and the minimum standard of performance.  Objectives describe only the behaviors that directly lead to or specifically satisfy a job performance requirement.  An objective is a statement of instructional intent.    Operational Evaluation.  The process of internal and external review of system elements, system requirements, instructional methods, courseware, tests, and process guide revision as needed to enhance the continued training effectiveness and efficiency of the training system during full-scale operations.  The process begins at the training system readiness review and continues throughout the life of the training system.  It includes Internal Evaluation and External Evaluation.  Also see Evaluation. Perceptual Skill.  The process of information extraction; the process by which an individual receives or extracts information from the environment through experiences and assimilates this data as facts (sight, sound, feel, taste, smell). AFH 36-2235 Volume 10 1 November 2002 148  Performance.  Part of a criterion objective that describes the observable student behavior (or the product of that behavior) that is acceptable to the instructor as proof that learning has occurred.  Plan of Instruction (POI).  A qualitative course control document designed for use primarily within a school for course planning, organization, and operation.  Generally, criterion objectives, duration of instruction, support materials, and guidance factors are listed for every block of instruction within a course.  Also called Syllabus.    Posttest.  A criterion-referenced test designed to measure performance on objectives taught during a unit of instruction; given after the instruction.  Also see Criterion-Referenced Test.  Pretest.  A criterion-referenced test designed to measure performance on objectives to be taught during a unit of instruction and performance on entry behavior; given before instruction begins.  Also see Criterion-Referenced Test.  Psychomotor.  A major area of learning that deals with acquiring physical skills requiring dexterity, coordination, and muscular activity.  Reliability.  (a) A characteristic of evaluation which requires that testing instruments yield consistent results.  (b) The degree to which a test instrument can be expected to yield the same result upon repeated administration to the same population.  (c) The capability of a device, equipment, or system to operate effectively for a period of time without a failure or breakdown.  Skill.  The ability to perform a job-related activity that contributes to the effective performance of a task.  Skills involve physical or manipulative activities which often require knowledge for their execution.  All skills are actions having specific requirements for speed, accuracy, or coordination.  Also see Attitude and Knowledge.  Subject Matter Expert (SME).  (a) An individual who has thorough knowledge of a job, duties/tasks, or a particular topic, which qualifies him/her to assist in the training development process (for example, to consult, review, analyze, advise, or critique). (b) A person who has high-level knowledge and skill in the performance of a job.   Summative Evaluation.  The overall assessment of a program at the completion of the developmental process. It is designed and used after the instructional system has become operational.  Also see Evaluation.  Syllabus.  See Plan of Instruction.  System Approach to Training (SAT).  Procedures used to develop instruction. Each phase requires input from the prior phase and provides input to the next AFH 36-2235 Volume 10 1 November 2002 149 phase. Evaluation provides feedback which is used to revise instruction.  Also see Instructional System Development.   Target Audience.  The total collection of possible users of a given instructional system; the persons for whom the instructional system is designed.  Task.  A unit of work activity or operation which forms a significant part of a duty.  A task usually has clear beginning and ending points and directly observable or otherwise measurable processes, frequently but not always resulting in a product that can be evaluated for quantity, quality, accuracy, or fitness in the work environment.  A task is performed for its own sake; that is, it is not dependent upon other tasks, although it may fall in a sequence with other tasks in a duty or job array.  Task Analysis.  See Job Task Analysis.   Terminal Objective.  An objective the learner is expected to accomplish upon completion of the instruction. It is composed of enabling (support or subordinate) objectives.  Training.  A set of events or activities presented in a structured or planned manner, through one or more media, for the attainment and retention of skills, knowledge, and attitudes required to meet job performance requirements.  Training Needs Assessment (TNA).  The study of performance and the environment that influences it in order to make recommendations and decisions on how to close the gap between the desired performance and the actual performance.  Validation.  The process by which the curriculum materials and instructional media materials are reviewed by the contractor for instructional accuracy and adequacy, suitability for presentation, and effectiveness in providing for the trainees' accomplishment of the learning objectives.   Validity.  The degree to which a criterion test actually measures what it was designed to measure.   AFH 36-2235 Volume 10 1 November 2002 150 Attachment 2 – ISD and Learning Theory and Application to the Learning Situation  Definition   Purpose in instruction   Relation to ISD   Learning theory is the body of principles proposed by psychologists and educators to explain how people learn skills, knowledge, and attitudes.   You use these principles in formal instruction to help make learning and retention faster, easier, more efficient and more effective.  Learning principles can guide developers in selecting and sequencing learning activities within a course to improve its effectiveness and efficiency.   Learning theory impacts instructional systems development in several ways.  Many of the products specifically called for in the ISD phases come from behavioral learning theory.  In the 1950s, behaviorists developed procedures for designing "programmed" instruction that included:  Analyze and break down content into specific behavioral objectives. Determine procedures needed to achieve the objectives. Try out and revise the steps. Validate the program against the objectives.  This approach was incorporated into early ISD.  Instructional designers apply learning theory to select instructional strategies for the type of learning required.  To the extent that there are real differences in the types of learning, (e.g., intellectual skills, motor skills, or attitudes), different instructional techniques may be needed.  Instructional designers look to learning theory to explain how individuals differ in the way they learn.  Understanding different learning styles in order to target methods and materials to individual students may become the most important theoretical area for enhancing learning in the future.  AFH 36-2235 Volume 10 1 November 2002 151  Two basic theories   New directions   Additional information  Instructional theorists and instructional technologists have been shifting their interest over the last few decades away from the behavioral learning model of stimulus-response-reinforcement and toward the cognitive learning model of acquisition, storage, and retrieval of knowledge.  Behavioral learning, which teaches students how to recognize task cues and respond with actions, and reinforces learning through feedback of successful task outcomes, remains an effective means of training personnel to perform procedural tasks.  It is still appropriate for most Air Force training today.   Cognitive learning uses categories such as declarative, procedural, and contextual, for organizing knowledge acquisition, and categories such as cognitive complexity, recall, problem solving, and creative processes, for knowledge retrieval and use.  Cognitive concepts play an important role in the development of new technologies for delivery of instruction, such as expert systems, in automated development tools, and in theories related to adult learning, such as individual cognitive styles.   Researchers are investigating new approaches, such as including a structural network analysis of the domain of information in the analysis part of ISD, in order to improve the process of building learning on previous learning.  Such mapping would require the availability of automated tools such as expert systems.  Other learning analyses predicted for future ISD applications, such as matching instruction to learning style, are far beyond current knowledge of learner differences, but they illustrate the directions in which new technologies are driving instructional design.   There have been many recent developments in learning theories, some of which have been summarized for use in Attachment C under Applications to the Learning Situation.  AFH 36-2235 Volume 10 1 November 2002 152  Introduction   Context variables   Cooperative learning   Learning activities  Applications to the Learning Situation  Within real-world constraints, instructional designers can try to design instructional programs and select learning contexts and materials to implement learning principles.   The context in which something is learned determines not only what is learned but how it can be used.  Learning is situated in the context and gains meaning from that context.  For example, problem solving should occur in an authentic context.  The fact that information and principles learned are a part of solving real problems will be better recalled when it is used again.   Social cognition psychologists feel that a learning context that promotes cooperative learning is more helpful to students than one that promotes individual work, especially with regard to learning attitudes.  Other psychologists see cooperative learning situations as effective when the interaction between students can supply learning support through tutoring and feedback.  For example, peer tutoring is an example of a technique for improving learning in a particular type of learning context.   Students learn as they confront the response demands built into activities.  Good activities are built around the attainment of multiple goals.  They engage students in active forms of learning, they help develop values and critical thinking skills, they are built around important content, and they are well matched to the learner's abilities and interests.  Different types of instructional delivery systems have been associated with particular instructional contexts.  For example, computer-assisted instruction was considered most appropriate for individualized instruction because it could provide for personalized instruction, test for misunderstandings, and provide feedback.  However, it was also found that the computer can be an important part of cooperative learning environments.  AFH 36-2235 Volume 10 1 November 2002 153  Technology changes context   Materials   Emerging technologies like CD-ROM, videodisc, Internet, and satellite conferencing change the context of instruction.  This is because technology not only delivers information, but it is also a tool that can be used to enable information retrieval from knowledge databases in ways other than printed text.  The encyclopedia on a CD-ROM or the Internet is not likely to be used by a learner in the same way as the same encyclopedia in the form of printed text.   The nature of the materials affects the stimuli that the learner interacts with during the process of learning.  Instructional materials are often used in a broader context of an instructional delivery system.  A textbook, for example, is meant to be used with a teacher.  In most cases, the textbook does not provide enough learning support; most students could not learn from it without additional information.  Broader instructional delivery systems often provide a variety of instructional materials in what are called multimedia environments.  These environments might include a live instructor or they might be totally mediated, as in a computer-interactive video.  Emerging technologies make distinguishing between different types of materials difficult, but the features of the delivery systems can be described in terms of their capabilities for providing different stimuli, and different "events" of instruction.  Events of instruction are a set of communications to the learner embedded in instructional activities.  They serve the function of activating internal events of information processing.  The events of instruction are:  Gain attention. Create goal expectancies. Bring into working memory the prerequisites for learning the new skill. Present the stimulus for the new skill. Provide learning support and guidance. Check for understanding. Remediate misunderstanding. Verify learning. Provide practice for retention and transfer.  AFH 36-2235 Volume 10 1 November 2002 154  One-on-One   Classroom instruction   Mediated materials   Physical objects   In most cases, the most capable delivery system would be the expert (instructor) working one-on-one with the learner on real problems in a real situation.  For example, if the skill to be learned was leading a seminar, the most ideal situation would be to have an expert observe the novice's performance and provide a one-on-one critique. In this case, the instructor can provide for all the events of instruction.  But in other cases, group learning may fulfill the educational requirement and some research has shown that mediated learning may be more effective in achieving certain affective learning objectives.   Because of costs, the one-on-one delivery system is not often used.  The classroom delivery system with groups of students and one instructor is more economical and can achieve the desired objectives of learning.  Using many variances and combinations of approaches, the instructor can provide for all the events of instruction.   Different mediated learning systems have different capacities for providing these events.  For example, a videotape can elicit learner responses to a question but is only capable of giving rhetorical feedback.  It is incapable of correcting learner misunderstanding because it is incapable of judging whether the response to a question was correct.  However, by looking at yourself on videotape you will be able to improve your delivery and organizational skills in presenting a lesson to a class.   Situations that contain physical objects are the most concrete and easy to understand because they don't require the translation of symbols.  When instruction is done with real objects, all the cues for later performance are available.  However, real situations may be very complex, and it may be simpler to gain information from a simplification of the real situation in the form of a mock-up or simulation.  Learning situations utilizing physical objects call for the learner to manipulate the objects.  Through handling them, whether real materials or simulations, the learners gain much information about size, sturdiness, complexity, and other features.  When the  AFH 36-2235 Volume 10 1 November 2002 155  Physical objects (Continued)   Film and video   Pictures and text   Printed text   learners are handling the object, they are building schemas of the experience that may be important and may speed future learning.  Sometimes real materials are used in demonstrations by instructors, but not used hands-on by students.  A demonstration makes the learner an observer of the expert handling the equipment.  This instructional technique usually involves the transmission of a great deal of declarative knowledge about the object being discussed, which the learner encodes and stores with greater rapidity than in the handling environment.   More abstract than physical objects are mediated materials, such as film and video.  These media generally have a linear format, and information is paced by the delivery system.  They usually present a visual from an objective rather than a subjective point of view, and they are edited to compress time.  This removes cues that are available from the real equipment and manipulation activities.  Most video instruction of procedures mediates demonstrations.   Most abstracts are still pictures, diagrams and printed text.  In order to understand text, you very often need to look at the real object; and to understand the real object, you have to read the text.  The text is providing an instructional overlay that provides the learner with the events of instruction that cannot be provided by the object itself.  On the other hand, the real object provides the context for using the information from the text.  Both are more efficient when used together.   The most abstract medium is printed text.  To gain meaning from printed text and illustrations, the learner should be able to read–that is, decode–words.  Furthermore, the learner must be able to comprehend what is being said.  Learning is more difficult because temporal and physical cues are absent.  The learner cannot question the author when there is ambiguity in the message.  This is very noticeable in education since many educational subject areas deal heavily with abstract ideas.  On the other hand, it is not perishable, can be restudied, and is more precise.  AFH 36-2235 Volume 10 1 November 2002 156  Printed text (Continued)   Additional information   Printed text is generally "formal" and follows different rules for communication than demonstrations or video instruction.  This formality makes printed text more difficult to understand.  Authors also vary in their ability to write clearly and use illustrations to provide learning guidance.   For additional information on learning theory, see:  Ausubel, D. P. (1968). Educational Psychology: A Cognitive View. New York: Holt, Rinehart and Winston. Bloom, B. S. (Ed.), Engelhart, M. D., Furst, E. J., Hill, W. K. and Krathwohl, D. R. (1956).  Taxonomy of Educational Objectives, Handbook I: Cognitive Domain.  New York: Longman. Bruner, J. S., Goodnow, J. J. and Austin, G. A. (1956).  A Study of Thinking. New York: Wiley. Bruner, J. S. The Act of Discovery. Harvard Educational Review, 31, 21-32. Gagné, R. M. (1977).  The Conditions of Learning (3rd Ed.). New York:  Holt, Rinehart and Winston. Krathwohl, D. R., Bloom, B. S. and Masia, B. (1964). Taxonomy of Educational Objectives, Handbook II: Affective Domain. New York: David McKay. Reigeluth, C. M. (1983). Instructional Design: What Is It and Why Is It?  In C. M. Reigeluth (Ed.), Instructional Design Theories and Models: An Overview of Their Current Status.  Hillsdale, New Jersey: Erlbaum Associates. West, C. K., Farmer, J. A. and Wolff, P. M. (1991).  Instructional Design:  Implications from Cognitive Science.  Englewood Cliffs, New Jersey:  Prentice Hall.   AFH 36-2235 Volume 10 1 November 2002 157 Attachment 3 RECENT DEVELOPMENTS IN LEARNING THEORY  Introduction   Adult learning principles   The following recent developments in learning theories have marked importance for education and provide guidelines for the instructional design process.   The literature on teaching adult learners has grown substantially in recent years and can be used to guide the specification of an instructional system for adults.  Knowles, in his 1987 publication on Adult Learning, summarized the information about adult learning and identified characteristics for effective instruction of adults.  Each of these characteristics has an implication for how Air Force personnel should be educated and trained.  An underlying characteristic is that adults differ from children in how they learn and, therefore, the process used for teaching adults should differ from the process used for teaching children.  When compared to children, adult learners: Want to know why they are being taught something. Want to participate and control the learning experience. Prefer active rather than passive learning experiences. Have a greater repository of experience to which the new learning can be related and bring greater diversity to learning situations. Prefer to set their own pace for learning. Have a greater need for feedback on their learning. Are more task-oriented. Prefer a variety of instructional methods. Like to determine the place and time of learning. Knowles (1987) developed principles for teaching adults based on characteristics of adult learners.  Frequently cited principles include: Selection of objectives to meet the learner's needs Variety of options built into the instruction Frequent feedback on learners' progress Task-oriented content Ability to adapt to learner's self-paced learning Frequent interaction   AFH 36-2235 Volume 10 1 November 2002 158  Schema theory / mental models   The human mind requires that data have meaning.  It rejects nonsense and works actively to find or create meaning for the perceptions it takes from the environment, using the contexts of stored meanings that individuals build up during their lifetime.  Researchers in instruction have concentrated on this area of cognitive activity, presumably because appropriate organization and sequencing of content would facilitate the learner's knowledge acquisition and processing activities.  Thus, how a person organizes and stores information coming from the environment continues to be of importance to the instructional design field.  Instructional theorists such as Bruner (1956), Ausubel (1968), Anderson (1976), Norman (1975), Bobrow (1968), Reigeluth (1983), and Merrill (1983) discuss the effects of cognitive structures or schemas on acquisition and retention of knowledge.  They assume a hierarchical organization of knowledge in memory from abstract and general to concrete and specific, with linkages by which the information is related within a knowledge structure.  Networks or schemas refer to the units of knowledge created by means of these linkages. Norman and others have analyzed schemas to show how they can be elaborated indefinitely in a hierarchical fashion to attach new knowledge that alters the meaning of the original conceptual unit.  Other researchers such as Mayer (1977) suggest that schemas are simply sets of related cognitive knowledge organized in some way, without necessarily presupposing a hierarchical structure.  This view is more consistent with the variable ways the same material may be stored in memory, e.g., loops, circles, linear strings, etc.  Simon (1979) also discusses organizational strategies in problem-solving such as means-ends analyses or cognitive structures, structures of goals and subgoals. Jenkins (1973) suggests that elements of the material being learned must be organized to constitute a strong personal experience, which introduces the affective component of learning.  Anderson (1985) described a schema as an organization of memory elements (propositions, images, and attitudes) representing a large set of meaningful information pertaining to a general concept.  The concept may be that of an object, like a fighter-jet, a weapon, or an officer.  Or it may be an   AFH 36-2235 Volume 10 1 November 2002 159  Schema theory / mental models (Continued)     Expert / novice distinctions  event like a preflight check, or preventive maintenance procedure.  Regardless of type, schemas contain information on certain well-understood features of the object or event.  These features, called slots, are filled in by the learner when encountering new information that relates to the schema.  Schemas are acquired through experience and may be the foremost benefit of apprenticeships. Spears (1983) stressed that schemas in general have the characteristics of learning sets (also referred to as discrimination sets and learning to learn).  He says, "The phenomenon was recognized quite early in associationistic psychology (Brown 1820), but its systematic experimental study is usually dated from Harlow's (1949) review of experiments by him and his coworkers that demonstrated the empirical reality of learning sets.  Their significance for understanding complex behavior and learning was recognized quite quickly in conceptions of cognitive processing."  The significance of schemas in learning theory comes from the principle of building learning on learning.  This is related both to providing increasingly complex mental models in proficiency development and adapting or accommodating learning styles.   Researchers agree that there are strong differences between the performance of experts and novices on any given task or problem, and an approach for looking at the personality variables that affect learning involves analyzing the differences.  There are definite instructional implications for moving an individual from novice to expert.  The study of what experts know and how they function compared to what novices know and how they function can guide the development of instruction. Information about what experts know and how they work has been made more important by new technology.  For example, human expertise can be captured and represented in a computer program, as is the case with expert systems.  Building expert systems requires careful analysis of the knowledge experts possess in a particular domain (Hannum 1990).  AFH 36-2235 Volume 10 1 November 2002 160  Expert / novice distinctions (Continued)     In general, experts are distinguished by the amount of information they possess, how they organize their knowledge, and how they select and use knowledge to solve problems.  When presented with problems, experts look for patterns, underlying structures or principles, and similarities to known problems.  They try to reconfigure the problem or represent it in a different way that allows them to solve it more easily (Hannum 1990).  Identifying the characteristics of expert performance is important in many areas of Air Force instruction, particularly in the flying/aircrew training and technical training applications. Shriver and Trexler (1984) discussed the performance requirements of expert maintainers compared to those of flyers in determining maintenance simulator design.    Maintenance experts have less need for coordinated complex psychomotor skills.  They require system concept understanding and problem-solving ability.  The expert maintenance troubleshooter:  Uses system logic on generic functional knowledge. Has comprehensive knowledge of specific systems. Knows how to formulate hypotheses. Knows where and how to test hypotheses. Knows how to use test equipment. Knows how to interpret test results. Knows the physical location of parts of the system. Is able to navigate the physical system to measure inputs and outputs. Knows the physical appearance of objects in the system. Knows the symbolic representation of objects in schematic diagrams and other maps. Knows how to access components.  Knows failure probabilities. Understands cost factors. Knows safety factors. Spears (1985) enumerated an extensive list of processes which distinguish the mature professional aircrew from the merely technically competent.  He describes skill mastery and emphasizes specific demands on aircrew members for maintaining intense concentration, for performing under pressure, and for resisting fatigue with its effects of distractibility and degradation of skill coordination.  He breaks  AFH 36-2235 Volume 10 1 November 2002 161  Expert / novice distinctions (Continued)   down the information-processing aspects of the aircrew's jobs into attention factors, skill knowledge, mediational processes, and adaptability.  In later work Spears adapted the processes so that they are broadly applicable to expert performers, for example, to pilots and maintainers of complex equipment.  He first lists characteristics that experts possess and then indicates the kinds of information-processing capacity needed for these characteristics.  Expert performers:  Stay ahead of the system. Know where they are going, how to get there, and what they will do when they arrive. Can forecast outcomes of their actions, assess likely outcomes ahead of time, and readily trade off alternatives according to immediate and/or long-range performance objectives. Suffer minimum skill degradation over time. Recover proficiency rapidly when degradation occurs. Are aware of lack of "polish" in their performance during or immediately following performance; they do not have to be told. Use minimum effort in performing skills; skills are automatized, leaving attention for other purposes. Suffer minimum intra- and interskill interference; time-share readily. Can manage heavy workloads. Maintain performance under pressure; even manipulate self-imposed pressure by "lying back" or "psyching" themselves up as top performance requires. Resist critical fatigue effects such as distractibility, lack of sensitivity to less important cues, ill-timed actions, and degradation of skill coordination. Have abundance of relevant information immediately available. Rapidly discriminate peculiarities of situations and adjust to them. Rapidly call forth and assess alternative interpretations of situations and courses of action. Learn new skills and adaptations of old ones rapidly; have good first-trial performance.  AFH 36-2235 Volume 10 1 November 2002 162   Expert / novice distinctions (Continued)   Are creative in performance; are not restricted by personal rigidity in  alternatives and outlooks when assessing situations nor by stereotype when reacting to them. Quickly recognize need to restructure assessments of situations and actions required. Can recall key factors in a situation and their performance several hours after an occurrence; thus can profit from delayed cognitive feedback. Do not persist in inadequate actions. Know useful shortcuts and how to use them. In brief, are among the group about whom one can say with considerable truth: "They just don't do anything wrong." Have skill knowledge that is patterned and "indexed" for rapid access, jointly with respect to cuing patterns, possible objectives, situational contingencies, and means-end relations. Have effective hierarchical organizations for patterns as above.  Have integral, "organic" knowledge of skills and related situational requirements, as opposed to a serial knowledge of sequences of skill components. Can achieve a one-to-one correspondence between structure of conditions and job requirements, and the structure of their recognition of requirements and resulting performance (nothing significant is omitted in understanding of given task-situational requirements, and except when allowing for contingencies, nothing superfluous is included). Use hierarchical searches when matching patterns of situations with memory (serial searches in this respect are clear indicators of lack of expertise). Can abstract pattern features rather than focus on literal features of situations and skill requirements. Rapidly discriminate among variants of feature patterns and adjust to them. Are sensitive to failure to achieve an appropriate pattern match between situation and memory, and can rapidly develop an appropriate restructuring of patterns. Organize strategies for action hierarchically, with critical contingencies clearly allowed for as situations may require. AFH 36-2235 Volume 10 1 November 2002 163  Expert / novice distinctions (Continued)     Knowledge transfer  Have a variety of tools readily available to help accomplish intermediate steps in a process. Use parallel processing as required (e.g., for time-sharing). Have clearly conceived (and perceived) checkpoints, contingencies, expectancies, etc., to be used in monitoring, adjusting, and altering actions. Have hierarchical schemas for "chunking" cuing information during performance and feedback during monitoring; do not need to assess all feedback, but reduce workload by keying on feedback pattern characteristics and generalized indicators. Can retain a lot of information in short-term memory through organizational processes that "chunk" information.Spears notes that techniques are available for assessing all of these characteristics.  Indeed, this list is simply an adaptation of characteristics of expertness as they have been identified in laboratory studies.  Paul Johnson (Campbell 1983) described how expert cardiologists organized sets of information to facilitate the heuristic approach they used in diagnosis.  The experts were able to consider and discard hypotheses rapidly because of the special way they stored information in memory.  Novices, on the other hand, had to work methodically through the procedural steps of the diagnosis.  The strategies and, therefore, the organization of the same material vary with the personality and style of the specialists.  Johnson says that individuals store information in memory in accordance with the use they intend to make of the material (that is, its purpose), and the organization is designed to support performance of the tasks they are familiar with.   Knowledge transfer is not only a cognitive but also an affective or motivational issue.   Research on motivation suggests that mindfulness could be a problem when transfer does not occur during problem-solving (Salomon and Globerson, 1987).  According to them, this underlying construct of mindfulness is a state of mind that is defined as the volitional, metacognitively guided employment of non-automatic, usually effort-demanding processes.  The   AFH 36-2235 Volume 10 1 November 2002 164   Knowledge transfer (Continued)   distinction between mindfulness and mindlessness is based on the distinction between controlled and automatic processes (Schneider and Fisk 1984; Shiffrin and Schneider 1977).  Automatic processes are fast, effortless, smoothly carried out in large chunks, not limited by the constraints of the central processing capacity or working memory.  They are usually controlled by external cues to which they have been associated through practice, or by internally overlearned response.  Thus, automaticity is associated with passive processing.  Controlled processes, on the other hand, are slow, effort-demanding, relatively discrete, thus available to analytic reflection (Kellog 1982; Langer and Imber 1980; Lefebvre-Pinard and Pinard 1985), and volitionally controlled by the individual (Kule 1985).  Introductory learning emphasizes rote memory, practice, and automatic processing during the stage of advanced knowledge acquisition.  In contrast, mindful processes need to be activated in order to enable individuals to: Withhold or inhibit the evocation of a first response. Examine and elaborate situational cues and underlying meanings that are relevant to the task to be accomplished. Generate or define alternative strategies. Gather information necessary for the choices to be made. Examine the outcomes. Draw new connections and construct new structures and new abstractions by reflective mental processes.  Evidently, such behavior is typically controlled and volitional.  Therefore, the learning behavior required for knowledge transfer in advanced knowledge acquisition is different from that of introductory learning.  Studies on medical cognition (Feltovich, Spiro and Coulson 1988) and mathematical problem-solving (Schoenfeld 1985) indicate that students often carry over the attitudes they have in the introductory learning while learning complex and advanced knowledge.  According to Schoenfeld, in traditional mathematical instruction teachers always hand down problems which should be solved in an Avg of two minutes each.  Students who have been exposed to such instructional approaches for an extensive period tend to believe that all the mathematical problems must be solvable within a few minutes.  If they cannot solve problems immediately, it means  AFH 36-2235 Volume 10 1 November 2002 165  Knowledge transfer (Continued)  the problems can never be solved.  This attitude is often found in students who fail to flexibly use their prior learned knowledge while engaged in advanced knowledge acquisition.  Recently, there has been a great deal of research on cultural influences on learning, including human intelligence (Gardner and Hatch 1989; Sternberg 1986), cognitive skills (Brown, Collins and Duguid 1989; Lave 1988; Rogoff and Lave 1984), belief systems (Jehng 1990; Jehng and Johnson 1991), and knowledge application (Feltovich, Spiro, and Coulson 1988; Spiro, Coulson, Feltovich and Anderson 1988).  Researchers in this area suggest that human learning takes place in a specific context, and knowledge is contextually welded.  In the school situation, learners develop an implicit sense of what is suitable diction, what makes a relevant question, and what are legitimate or illegitimate behaviors in a particular activity.  The ease and success with which learners behave obscure the fact that what they get is a product of surrounding culture rather than explicit teaching (Brown, Collins and Duguid 1989).  In this sense, knowledge transfer could be regarded as a process of decision-making in which individuals decide whether learned knowledge ought to be applied in a certain problem context.  Learner characteristics have received considerable attention as a critical aspect of the skill and knowledge transfer process.  Understanding individual learning styles, for  example, could enable instruction to be tailored to present the same content through a variety of methods.  The intent is to be able to present students methods and materials which are optimal for them.  When the intention is to build knowledge upon knowledge, the students' schemas of previously learned information on the subject can be used effectively as a retrieval hook for the new learning.  Attitudes, another learner characteristic, are also learned and held in schemas.  Attitudes can be available for change and adaptation to enhance instructional effectiveness (Wager 1991).  Spears (1985) discusses some issues related to learning styles of aircrew trainees.  He suggests that accommodating different styles in training is important, but it is also possible to change individual styles.  "Hence, there is the issue of whether to accommodate all learning, to plan training that will alter them, or simply to reject students with styles that are not AFH 36-2235 Volume 10 1 November 2002 166  Metacognition   Metaskill   Mastery approach vs. discovery learning  compatible with the techniques [selected]."  Instructional efficiency requirements may well preclude overemphasis on learner differences.  Metacognition refers to one's knowledge or thought processes about one's own cognition.  It is the awareness and regulation of our own thought processes.  Learners often use their megacognitive knowledge when they approach a new learning task.   Metaskill, according to Spears, is a concept that organizes the discrimination systems involved in complex performance and points the way to effective and efficient training.  It is defined for present purposes as a high order generalized discrimination system.  Metaskills govern which specific skills will be drawn on for particular situations.  They determine how cue complexes will be sifted for pertinent information, how they will be interpreted, how individual skills will be adapted to each situation, and how self-monitoring of performance will occur so as to continue skill adaptations to factors as they arise during performance.  They also include mechanisms for error detection and avoidance.  In brief, metaskill refers to skill in using separate specific skills.  One hears of the difference between a person who has 10 years' experience and a person who had one year's experience 10 times.  The difference is in metaskill development.  This fundamental concept can be directly targeted during training.  Metaskills develop separately from individual skills, but they will not develop unless individual skills are practiced.  Further, whereas proficiency in individual skills relates to the amount of practice on each skill, metaskills develop as a function of the number of skills practiced, or of the number of variations of them as required in many different situations.   Researchers in the early 1990s investigated the processes and structures of competent performance in specific domains.  To develop instructional programs to produce such competence, Glaser (1990) reports that two dichotomous stances toward instruction are reflected in such programs.    One stance is that of the master approach, which emphasizes AFH 36-2235 Volume 10 1 November 2002 167    Mastery approach vs. discovery learning (Continued)   Enterprise theory  learning proceduralized knowledge through extensive practice with problem-solving.  The instructor controls the direction of  learning and the learners follow a specific path of carefully structured subgoals leading toward the efficient performance of a well-defined skill.  Practice with successful performance is thought to lead to subsequent metacognitive abilities.  Another stance toward instruction emphasizes self-regulated control of learning strategies by the learner in accomplishing a complete, non-decomposed task (Breuer and Kummer 1990).  With the discovery learning approach, the teacher provides modeling of the metacognitive strategies necessary for beginning the task and, when problems are encountered, provides assistance.  One learning procedure reflecting this stance, called Reciprocal Teaching, structures collaborative group work in sharing a complex problem-solving task.  This approach is based on theories about the social genesis of learning in which the learner is characterized as being motivated to seek explanations through explorations (Tennyson and Breuer 1991).   Gagné and Merrill (1990) proposed a method to identify learning goals that requires an integration of multiple objectives.  They proposed that such an integration of multiple objectives be conceived in terms of the pursuit of a comprehensive purpose in which the learner is engaged, called an enterprise.  When instruction is considered in the more comprehensive sense of a module, section, or course, it becomes apparent that multiple objectives commonly occur.  In seeking a way of dealing with multiple objectives other than serially, there is a need for treating human performance at a somewhat higher level of abstraction than is usual in most instructional design models.  People may learn facts, but what for?  They may learn new concepts, but how are these to function in the context of the larger task that they as human individuals do?  Learners can acquire procedures, but in the context of what larger scale activity?  Performances may be described not simply as steps in a sequence but also in terms of their function and purpose in meeting the goal of an activity as a whole.  An enterprise is a purposive activity that may depend for its AFH 36-2235 Volume 10 1 November 2002 168   Enterprise theory (Continued)  execution on some combination of verbal information, intellectual skills, and cognitive strategies, all related by their involvement in the common goal.  A task for the instructional   designer is to identify the goal of a targeted enterprise along with its component skills and knowledge, and then to design instruction that enables the student to acquire the capability of achieving this integrated outcome.  Thus, learners may acquire a fact or a concept that enables them to distinguish a given object or set of objects.  Or this fact or concept may be part of a goal that enables them to communicate the stages of some process.  Or this fact or concept may be part of a goal that enables them to predict the next in a process, to invent a new device, or discover a new process.  Each of these is a different enterprise, and each is accordingly represented by a different integrated goal.  Each such enterprise requires a different kind of integration of the multiple objectives that support it.  Different integrated goals of various enterprises are represented in memory as different kinds of cognitive structures or schemas (Rumelhart and Norman 1978).  Integrated goals are the aims of human enterprises that embody and integrate multiple objectives.  The enterprise schema is expected to contain a number of knowledge and skill constituents which become associated in the service of the integrated goal.  These include verbal labels, connected-discourse forms of verbal information, intellectual skills, and cognitive strategies.  Depending upon the enterprise, motor skills and attitudes may also be involved.  The integrative goal itself is incorporated in the schema as verbal knowledge.  An important feature associated with the goal is the enterprise scenario that relates component activities (identifying concepts, carrying out procedures, etc.)  to the goal.  It is the scenario that provides a basis for the application of the constituent knowledge and skill in the enterprise performance.  This entire complex is what is meant by the enterprise schema.  The instructional designer makes provision for the enterprise goal in instruction by communicating the schema as verbal information.  This includes, on the one hand, identification of the intellectual skills and verbal knowledge that relate to the goal, and on the other, the scenario that must be played out in AFH 36-2235 Volume 10 1 November 2002 169   Enterprise theory (Continued)   conducting the enterprise.  For example, instruction for the enterprise of troubleshooting a piece of complex electrical equipment would normally require conveying information about the current flow in the system as a whole, the identification of functioning (or malfunctioning) parts, and the   procedure for checking each of those parts.  Problem-solving strategies relevant to troubleshooting represent another type of objective to be included.  These individual knowledge and skills would become a part of the scenario that expresses the sequence and purpose of troubleshooting as checking the fault symptom, finding a malfunctioning part, and replacing it.  Considering the great variety of goals that may be aimed for in instruction, it does not appear feasible to try to identify them in a comprehensive sense.  However, there do appear to be some common types (Gagné and Merrill 1990).  One commonly occurring type of purposive activity (enterprise) consists of a kind of elaborated identifying, called denoting.  The integrative goal for denoting goes beyond pointing or labeling.  It may include demonstration of the rules incorporated in the definition of some entity.  Thus, in denoting a class of objects such as "typewriter," the performer would communicate not only its name and the appearance of the object and its parts, but also the rules involved in its functioning.  A second variety of enterprise involves teaching or "showing" others (students, co-workers) the execution of steps in a procedure or in a process.  This is called manifesting.  For example, it may be the goal of the learner to show to others the stages in the metamorphosis of a butterfly.  Manifesting this process may require a verbal narrative as well as the demonstration of some rules; thus, the integration of multiple objectives is involved.  Still a third type of enterprise is discovering.  Problem solving or the finding of a novel process may be taking place.  Typically, both intellectual skills and verbal information must be integrated in this kind of enterprise.  It may be that a number of other enterprises are of common occurrence.  Whatever particular purpose they serve, all of them are instances of the integration of multiple objectives.  It is best to make provision for the activation of a goal well toward the beginning of instruction.  The second event of instruction pertains to informing the learner of the objective.  AFH 36-2235 Volume 10 1 November 2002 170    Enterprise theory (Continued)  When instruction consists of multiple objectives involved in an integrated goal, this second event is carried out as follows: (1) providing a verbal statement of a goal and its relation to prerequisite skills, knowledge, and attitudes; and (2) giving a verbal description of a scenario that relates whatever is to be newly learned to the goal. These verbal communications about the goal appear relatively simple and yet adequate to establish a goal schema.  Of course, they should be related insofar as possible to the individual learner's prior experience and motivation.  The establishment of a goal schema may be expected to make a significant addition to the metacognitive resources available to the learner.  As such it has the function of monitoring the new learning–keeping it "on track," free of distractions, and rejecting attention to irrelevant events.  When learning has progressed to a point at which performance is to be exhibited, the schema aids in the retrieval of prerequisite knowledge and skills.  Furthermore, throughout its scenario, the schema supports mindful abstraction and, thus, enhances transfer of learning to the performances involved in the enterprise.   The result of integrating enterprises for the learner is the metaskill.  The metaskill is based on learning hierarchies which the individual can recall to meet the requirements of the novel situation being presented.  AFH 36-2235 Volume 10 1 November 2002 171 Attachment 4 – An Example of Implementing the ISD Process: The Wing Company Grade Officer Course  NOTE:  The following example is designed to illustrate how the ISD process was used to develop a completely new course of instruction.  The descriptions given for each of the first three phases of the ISD process are relatively concise.  In reality, the amount of effort expended in each phase of the process was considerably greater than these descriptions suggest.  but, for the sake of brevity and space, what is described is essentially a synopsis of all the work that occurred in each phase.  The Instructional System Development (ISD) process is the basic tool utilized in planning, developing, and evaluating the Wing Company Grade Officer Course (WCGOC).  The following topics are key to the WCGOC development and execution efforts:  Phase I  --  Analyze and determine what instruction is needed.  Review 1996 Fall CORONA identification of several concerns within the officer corps.  Analysis determined a need to improve and broaden the professional competence of company grade officers so they may better perform their duties in support of Air Force Global Engagement and associated mission requirements.  Additional analysis indicated a clear need for lieutenants to understand how operational, logistics, support, and medical elements function at wing level.     Gather information to determine instructional requirements of lieutenants to meet junior officer needs.  Survey conducted concluded that 44 of 61 (72%) of CONUS bases contacted had courses of different varieties.  Contact with junior and senior officers identified a need for nonresident instruction for lieutenants.  The identified lieutenants needs were to better understand how operational and support elements at the wing level function and the junior officers’ roles in this process. Based on data, discussions and course developer experience, an overall planned course of action was developed to educationally increase the lieutenants value to the local unit by understanding and applying interoperability between wing units. A typical student profile, further established a need for a distance learning (vice resident instruction) professional military education course.  An added need was to develop a broad perspective of current wing issues and associated skills aimed at enhancing professional competence beyond one’s own specialty. The difference between what lieutenants already knew and could do and what wing requirements required, also helped to determine instruction needed.  Formative evaluation of process and concept assessment was accomplished through discussions with commanders’ seminar, Squadron Officer School (SOS), major commands, Air War College (AWC) students, Aerospace Basic Course (ABC), Board of Visitors (BOV), Command Board of Advisors (CBOA), and installation AFH 36-2235 Volume 10 1 November 2002 172          leadership.  The major focus in Phase I is to clearly determine the desired learning outcomes, e.g., skills and knowledge required of a lieutenant within the first 6 – 12 months of assignment at a wing or equivalent unit. The bulk of resources required to support instruction were to be provided by the installations concerned. Phase II  –  Design instruction to meet the need. Determine what essential educational requirements should influence the WCGOC curriculum action.  Two factors of high priority impacted this decision: New officers graduating from a largely academic precommissioning environment require a real world practical experience to standardize officership training .  Provide a standardized nonresident course for all Company Grade Officers (CGOs) that is responsive to the challenging needs of today’s lieutenants. Develop a WCGOC curriculum plan in the AU format approved for College for Professional Development (CPD) courses.  This detailed plan of instruction should include Part I:  course description and Part II:  course of study – overview and Part III:  detailed course of study.  This format included areas of instruction, educational goals, periods of instruction, and evaluation. Three factors were instrumental in driving the design phase: WCGOC is part of the AU Continuum of Education.  WCGOC is a key element of a Wing mentoring program.  Forty hours of classroom instructional material should be provided. The basic implementation plan for the WCGOC also considered the following essential design factors: Target audience is line and non-line lieutenants and direct-appointment captains with 6 – 12 months at their first duty stations.  Primary teaching tool is to facilitate learning:  Socratic method and guided discussion.  Other methods:  group exercises, case studies, tours, and briefings.  Facilitator is a guide and advisor.  AFH 36-2235 Volume 10 1 November 2002 173 WCGOC management process.  Establish a nonresident management and accountability system.  This should include but not be limited to: Role of Wing Commander and Company Grade Officer Council.  WCGOC Course Director, facilitators, and students.  Enrollment process at unit of first assignment.  Course completion record keeping process. Formative evaluation consisted of senior officer review, AU/CC assessment, CBOA briefings/feedback, BOV, and commander’s seminar evaluation.  In Phase II, evaluation should be directed toward process and product evaluation and the achievement of required outcomes.  This control element is aimed at reaching the predefined outcomes addressed in this phase of the ISD process.   Phase III  –  Develop instructional materials to support system requirements.     Develop WCGOC support materials.  Example:  Course Directors’ Guide, Facilitator’s Guide, Student’s Guide, Reading List, Lesson Plans, PowerPoint Slides, Course Schedule, Wing Commander’s Introduction, Reading Material / Cases / Exercises.  Update implementation plan to include topics aligned to each area of instruction.  Prepare lesson plan for each instructional lesson.  Conduct course field validations at about thirteen bases.  CPD/CC to submit interim reports of field validations to AU/CC.  Validation should consist of review of course instruction and materials (formative evaluation).  Develop instrument to accomplish this critical validation.  This should be accomplished by individual and WCGOC student tryouts.  Summative evaluation should consist of macro assessment to include discussion with wing commanders, senior leadership, and supervisors. At the time of this writing, WCGOC validation is still in progress.  Course development remains in Phase III.  Based on all validation feedback, the course continues to be revised, including all materials and processes as appropriate. 