{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topycal\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "INSTR_PATH = os.path.join(os.getcwd(),\"afi_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait, as_completed\n",
    "def do_fn_on_iter(fn, iterator, num_threads=6):\n",
    "    futures = []\n",
    "    if isinstance(num_threads, str):\n",
    "        num_threads = int(num_threads)\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for elem in iterator:\n",
    "            futures.append(executor.submit(fn, elem))\n",
    "    results = []\n",
    "    for x in as_completed(futures):\n",
    "        results.append(x.result())\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(limit=500, shuffle=True):\n",
    "    files = glob.glob(\"{}/afi*.txt\".format(INSTR_PATH))\n",
    "    if shuffle:\n",
    "        random.shuffle(files)\n",
    "    if limit:\n",
    "        return files[0:limit]\n",
    "    else:\n",
    "        return files\n",
    "    #data = myfile.read()\n",
    "    \n",
    "def read_file(fname):\n",
    "    with open(fname, errors='replace') as fd:\n",
    "        return fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = get_file_list(limit=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def load_file(fname):       \n",
    "    with open(fname, 'r') as myfile:\n",
    "        return (os.path.basename(fname),re.sub(r'[\\t\\n\\r\\x0b\\x0c]',' ',myfile.read()))\n",
    "\n",
    "def load_corpus(file_list):\n",
    "    return {f[0]:f[1] for f in do_fn_on_iter(load_file, file_list)}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = load_corpus(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we sentence segment then create a basket based on the 64bit murmurhash (after whitespace trimming and checking for a minimum sentence threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "MIN_SENT_LEN = 15\n",
    "\n",
    "def do_market_basket_analysis(doc_dict):\n",
    "    baskets = defaultdict(set)\n",
    "    for k,v in tqdm(doc_dict.items()):\n",
    "        for sent in nlp(v).sents:\n",
    "            if len(sent) > MIN_SENT_LEN:\n",
    "                baskets[mmh3.hash64(sent.text.strip())].add(k)\n",
    "    return baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1141/1141 [21:49<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "baskets = do_market_basket_analysis(corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each basket is trimmed based on being a certain minimum size.. logic here is to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim baskets\n",
    "\n",
    "def trim_baskets(baskets, min_size=3):\n",
    "    return {k:v for k,v in tqdm(baskets.items()) if len(v) > min_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490156/490156 [00:00<00:00, 991812.79it/s]\n"
     ]
    }
   ],
   "source": [
    "trimmed_baskets = trim_baskets(baskets, min_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490156"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(baskets.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1785"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(trimmed_baskets.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a graph from the baskets, each combination of documents in each bucket is related to each other.. and the edge weight between documents is based on how many buckets they share. We also determine series here and enrich the node with that, so we can compare how links within or across series work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "# Build a weighted graph from the baskets\n",
    "\n",
    "def traverse_baskets(baskets):   \n",
    "    graph = nx.Graph()\n",
    "    for k,v in baskets.items():\n",
    "        combs = combinations(v, 2)\n",
    "        for comb in combs:\n",
    "            # ensure nodes exist\n",
    "            if comb[0] not in graph:\n",
    "                graph.add_node(comb[0], series=comb[0].split('-')[0][-2:]) # split on '-' and take the last 2 chars from first part of split result\n",
    "            if comb[1] not in graph:\n",
    "                graph.add_node(comb[1], series=comb[1].split('-')[0][-2:]) # e.g. afi_12-21 -> 12; afi12-22 -> 12\n",
    "            \n",
    "            # if already exists, increment\n",
    "            if comb[0] in graph[comb[1]]:\n",
    "                curr_wt = graph.edges[comb[0], comb[1]]['weight']\n",
    "                series_same = comb[0][3:5] == comb[1][3:5]\n",
    "                graph.add_edge(comb[0],comb[1], weight=curr_wt+1, label=\"{}-{}\".format(comb[0],comb[1]),sameseries=\"{}\".format(series_same))\n",
    "            # create new link\n",
    "            else:\n",
    "                series_same = comb[0][3:5] == comb[1][3:5]\n",
    "                graph.add_edge(comb[0],comb[1], weight=1, label=\"{}-{}\".format(comb[0],comb[1]), sameseries=\"{}\".format(series_same))\n",
    "    return graph\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = traverse_baskets(trimmed_baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14948"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More noise reduction.. prune nodes/edges that only share 1 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim graphs with only 1 link\n",
    "def prune_graph(graph):\n",
    "    removal_candidates = []\n",
    "    for edge in graph.edges():\n",
    "        if graph.edges()[edge]['weight'] < 2:\n",
    "            removal_candidates.append((edge[0],edge[1]))\n",
    "    # remove low weight edges\n",
    "    graph.remove_edges_from(removal_candidates)\n",
    "    # remove isolate nodes\n",
    "    graph.remove_nodes_from(list(nx.isolates(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_graph = prune_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4124"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruned_graph.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as GML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(pruned_graph,'afi_mba_pruned.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the trimmed basket keys as a starting point for sentences that are very common in the corpus.. now if we remove those and then perform a conventional cosine similarity or other affinity analysis; we are effectively excluding the facsimile sentences. Again we exclude shorter sentences to reduce noise from outlines, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_sent_hashes = list(trimmed_baskets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "MIN_SENT_LEN = 15\n",
    "\n",
    "def prune_corpus_of_reused_sentences(doc_dict, bad_hashes):\n",
    "    pruned_dict = {}\n",
    "    for docname,doctxt in tqdm(doc_dict.items()):\n",
    "        reduced_sents = []\n",
    "        for sent in nlp(doctxt).sents:\n",
    "            if len(sent) > MIN_SENT_LEN:\n",
    "                # check if hash is okay\n",
    "                this_sent = sent.text.strip()\n",
    "                if mmh3.hash64(this_sent) not in bad_hashes:\n",
    "                    reduced_sents.append(this_sent)\n",
    "        pruned_dict[docname] = \" \".join(reduced_sents)\n",
    "    return pruned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1141/1141 [22:25<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "pruned_dict = prune_corpus_of_reused_sentences(corpus_dict,redundant_sent_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afi51-302.txt'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pruned_dict.keys())[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_dict[list(pruned_dict.keys())[5]]) != len(pruned_dict[list(pruned_dict.keys())[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple code to compute dot product on 1 docs using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compare_docs(doc1,doc2):\n",
    "    tfidf = TfidfVectorizer().fit_transform([doc1, doc2])\n",
    "    diffs = list((tfidf * tfidf.T).A)\n",
    "    return diffs[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build similarity graph .. link each doc with similarity as attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "# Build a weighted graph from the pruned docs based on cosine similarity\n",
    "\n",
    "# O(n^2)!!\n",
    "# should do this more efficiently.. fingerprint method or self-organizing map\n",
    "def make_similarity_graph(doc_dict):   \n",
    "    graph = nx.Graph()\n",
    "    pairwise_docs = list(combinations([k for k in doc_dict.keys()],2))\n",
    "    for doc_key in doc_dict.keys():\n",
    "        graph.add_node(doc_key, series=doc_key.split('-')[0][-2:])\n",
    "    for pair in tqdm(pairwise_docs):\n",
    "        graph.add_edge(pair[0],pair[1],weight=10*compare_docs(pair[0],pair[1])) # multiply by 10.. better for gephi\n",
    "    return graph\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 650370/650370 [23:00<00:00, 471.03it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_graph = make_similarity_graph(pruned_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 5.0310261241513139}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_graph.edges()[list(similarity_graph.edges())[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim graphs lower similarity\n",
    "def prune_similarity_graph(graph):\n",
    "    removal_candidates = []\n",
    "    for edge in graph.edges():\n",
    "        if graph.edges()[edge]['weight'] < 4:\n",
    "            removal_candidates.append((edge[0],edge[1]))\n",
    "    # remove low weight edges\n",
    "    graph.remove_edges_from(removal_candidates)\n",
    "    # remove isolate nodes\n",
    "    graph.remove_nodes_from(list(nx.isolates(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_similarity = prune_similarity_graph(similarity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(pruned_similarity,'afi_cosine_pruned.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
